<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>17 Simple Linear Regression | (Re)Doing Bayesain Data Analysis</title>
  <meta name="description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="17 Simple Linear Regression | (Re)Doing Bayesain Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="17 Simple Linear Regression | (Re)Doing Bayesain Data Analysis" />
  
  <meta name="twitter:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

<meta name="author" content="R Pruim">


<meta name="date" content="2019-03-27">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="estimating-one-and-two-means.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">(Re)Doing Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What’s in These Notes</a></li>
<li class="part"><span><b>I The Basics: Models, Probability, Bayes, and R</b></span></li>
<li class="chapter" data-level="2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html"><i class="fa fa-check"></i><b>2</b> Credibility, Models, and Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#the-steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.1</b> The Steps of Bayesian Data Analysis</a><ul>
<li class="chapter" data-level="2.1.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-code"><i class="fa fa-check"></i><b>2.1.1</b> R code</a></li>
<li class="chapter" data-level="2.1.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-packages"><i class="fa fa-check"></i><b>2.1.2</b> R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-1-which-coin-is-it"><i class="fa fa-check"></i><b>2.2</b> Example 1: Which coin is it?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#freedom-of-choice"><i class="fa fa-check"></i><b>2.2.1</b> Freedom of choice</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#distributions"><i class="fa fa-check"></i><b>2.3</b> Distributions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#beta-distributions"><i class="fa fa-check"></i><b>2.3.1</b> Beta distributions</a></li>
<li class="chapter" data-level="2.3.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#normal-distributions"><i class="fa fa-check"></i><b>2.3.2</b> Normal distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-2-height-vs-weight"><i class="fa fa-check"></i><b>2.4</b> Example 2: Height vs Weight</a><ul>
<li class="chapter" data-level="2.4.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#data"><i class="fa fa-check"></i><b>2.4.1</b> Data</a></li>
<li class="chapter" data-level="2.4.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#describing-a-model-for-the-relationship-between-height-and-weight"><i class="fa fa-check"></i><b>2.4.2</b> Describing a model for the relationship between height and weight</a></li>
<li class="chapter" data-level="2.4.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#prior"><i class="fa fa-check"></i><b>2.4.3</b> Prior</a></li>
<li class="chapter" data-level="2.4.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior"><i class="fa fa-check"></i><b>2.4.4</b> Posterior</a></li>
<li class="chapter" data-level="2.4.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior-predictive-check"><i class="fa fa-check"></i><b>2.4.5</b> Posterior Predictive Check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#where-do-we-go-from-here"><i class="fa fa-check"></i><b>2.5</b> Where do we go from here?</a></li>
<li class="chapter" data-level="2.6" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#ch02-exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#footnotes"><i class="fa fa-check"></i><b>2.7</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html"><i class="fa fa-check"></i><b>3</b> Some Useful Bits of R</a><ul>
<li class="chapter" data-level="3.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#style-guide"><i class="fa fa-check"></i><b>3.1</b> You Gotta Have Style</a><ul>
<li class="chapter" data-level="3.1.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#an-additional-note-about-homework"><i class="fa fa-check"></i><b>3.1.1</b> An additional note about homework</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors-lists-and-data-frames"><i class="fa fa-check"></i><b>3.2</b> Vectors, Lists, and Data Frames</a><ul>
<li class="chapter" data-level="3.2.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors"><i class="fa fa-check"></i><b>3.2.1</b> Vectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#lists"><i class="fa fa-check"></i><b>3.2.2</b> Lists</a></li>
<li class="chapter" data-level="3.2.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#data-frames-for-rectangular-data"><i class="fa fa-check"></i><b>3.2.3</b> Data frames for rectangular data</a></li>
<li class="chapter" data-level="3.2.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#other-types-of-data"><i class="fa fa-check"></i><b>3.2.4</b> Other types of data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#plotting-with-ggformula"><i class="fa fa-check"></i><b>3.3</b> Plotting with ggformula</a></li>
<li class="chapter" data-level="3.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#creating-data-with-expand.grid"><i class="fa fa-check"></i><b>3.4</b> Creating data with expand.grid()</a></li>
<li class="chapter" data-level="3.5" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#transforming-and-summarizing-data-dplyr-and-tidyr"><i class="fa fa-check"></i><b>3.5</b> Transforming and summarizing data dplyr and tidyr</a></li>
<li class="chapter" data-level="3.6" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#writing-functions"><i class="fa fa-check"></i><b>3.6</b> Writing Functions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#why-write-functions"><i class="fa fa-check"></i><b>3.6.1</b> Why write functions?</a></li>
<li class="chapter" data-level="3.6.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#function-parts"><i class="fa fa-check"></i><b>3.6.2</b> Function parts</a></li>
<li class="chapter" data-level="3.6.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#the-function-function-has-its-function"><i class="fa fa-check"></i><b>3.6.3</b> The function() function has its function</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#some-common-error-messages"><i class="fa fa-check"></i><b>3.7</b> Some common error messages</a><ul>
<li class="chapter" data-level="3.7.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#object-not-found"><i class="fa fa-check"></i><b>3.7.1</b> object not found</a></li>
<li class="chapter" data-level="3.7.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#package-inputenc-error-unicode-char-not-set-up-for-use-with-latex."><i class="fa fa-check"></i><b>3.7.2</b> Package inputenc Error: Unicode char not set up for use with LaTeX.</a></li>
<li class="chapter" data-level="3.7.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#any-message-mentioning-yaml"><i class="fa fa-check"></i><b>3.7.3</b> Any message mentioning yaml</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#ch03-exercises"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#footnotes-1"><i class="fa fa-check"></i><b>3.9</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#some-terminology"><i class="fa fa-check"></i><b>4.1</b> Some terminology</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>4.2</b> Distributions in R</a><ul>
<li class="chapter" data-level="4.2.1" data-path="probability.html"><a href="probability.html#example-normal-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Example: Normal distributions</a></li>
<li class="chapter" data-level="4.2.2" data-path="probability.html"><a href="probability.html#simulating-running-proportions"><i class="fa fa-check"></i><b>4.2.2</b> Simulating running proportions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#joint-marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>4.3</b> Joint, marginal, and conditional distributions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="probability.html"><a href="probability.html#example-hair-and-eye-color"><i class="fa fa-check"></i><b>4.3.1</b> Example: Hair and eye color</a></li>
<li class="chapter" data-level="4.3.2" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>4.3.2</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#ch04-exercises"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#footnotes-2"><i class="fa fa-check"></i><b>4.5</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html"><i class="fa fa-check"></i><b>5</b> Bayes’ Rule and the Grid Method</a><ul>
<li class="chapter" data-level="5.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#the-big-bayesian-idea"><i class="fa fa-check"></i><b>5.1</b> The Big Bayesian Idea</a><ul>
<li class="chapter" data-level="5.1.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#likelihood"><i class="fa fa-check"></i><b>5.1.1</b> Likelihood</a></li>
<li class="chapter" data-level="5.1.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#when-bayes-is-easy"><i class="fa fa-check"></i><b>5.1.2</b> When Bayes is easy</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#estimating-the-bias-in-a-coin-using-the-grid-method"><i class="fa fa-check"></i><b>5.2</b> Estimating the bias in a coin using the Grid Method</a><ul>
<li class="chapter" data-level="5.2.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#creating-a-grid"><i class="fa fa-check"></i><b>5.2.1</b> Creating a Grid</a></li>
<li class="chapter" data-level="5.2.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#hdi-from-the-grid"><i class="fa fa-check"></i><b>5.2.2</b> HDI from the grid</a></li>
<li class="chapter" data-level="5.2.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#automating-the-grid"><i class="fa fa-check"></i><b>5.2.3</b> Automating the grid</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#working-on-the-log-scale"><i class="fa fa-check"></i><b>5.3</b> Working on the log scale</a></li>
<li class="chapter" data-level="5.4" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#discrete-params"><i class="fa fa-check"></i><b>5.4</b> Discrete Parameters</a></li>
<li class="chapter" data-level="5.5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#ch05-exercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
<li class="chapter" data-level="5.6" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#footnotes-3"><i class="fa fa-check"></i><b>5.6</b> Footnotes</a></li>
</ul></li>
<li class="part"><span><b>II Inferring a Binomial Probability</b></span></li>
<li class="chapter" data-level="6" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><i class="fa fa-check"></i><b>6</b> Inferring a Binomial Probability via Exact Mathematical Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-distributions-1"><i class="fa fa-check"></i><b>6.1</b> Beta distributions</a></li>
<li class="chapter" data-level="6.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-and-bayes"><i class="fa fa-check"></i><b>6.2</b> Beta and Bayes</a><ul>
<li class="chapter" data-level="6.2.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#the-bernoulli-likelihood-function"><i class="fa fa-check"></i><b>6.2.1</b> The Bernoulli likelihood function</a></li>
<li class="chapter" data-level="6.2.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#a-convenient-prior"><i class="fa fa-check"></i><b>6.2.2</b> A convenient prior</a></li>
<li class="chapter" data-level="6.2.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#pros-and-cons-of-conjugate-priors"><i class="fa fa-check"></i><b>6.2.3</b> Pros and Cons of conjugate priors</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#getting-to-know-the-beta-distributions"><i class="fa fa-check"></i><b>6.3</b> Getting to know the Beta distributions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#important-facts"><i class="fa fa-check"></i><b>6.3.1</b> Important facts</a></li>
<li class="chapter" data-level="6.3.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#alternative-parameterizations-of-beta-distributions"><i class="fa fa-check"></i><b>6.3.2</b> Alternative parameterizations of Beta distributions</a></li>
<li class="chapter" data-level="6.3.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta_params"><i class="fa fa-check"></i><b>6.3.3</b> beta_params()</a></li>
<li class="chapter" data-level="6.3.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#automating-bayesian-updates-for-a-proportion-beta-prior"><i class="fa fa-check"></i><b>6.3.4</b> Automating Bayesian updates for a proportion (beta prior)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#what-if-the-prior-isnt-a-beta-distribution"><i class="fa fa-check"></i><b>6.4</b> What if the prior isn’t a beta distribution?</a></li>
<li class="chapter" data-level="6.5" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#ch06-exercises"><i class="fa fa-check"></i><b>6.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html"><i class="fa fa-check"></i><b>7</b> Markov Chain Monte Carlo (MCMC)</a><ul>
<li class="chapter" data-level="7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#king-markov-and-adviser-metropolis"><i class="fa fa-check"></i><b>7.1</b> King Markov and Adviser Metropolis</a></li>
<li class="chapter" data-level="7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#quick-intro-to-markov-chains"><i class="fa fa-check"></i><b>7.2</b> Quick Intro to Markov Chains</a><ul>
<li class="chapter" data-level="7.2.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#more-info-please"><i class="fa fa-check"></i><b>7.2.1</b> More info, please</a></li>
<li class="chapter" data-level="7.2.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#definition"><i class="fa fa-check"></i><b>7.2.2</b> Definition</a></li>
<li class="chapter" data-level="7.2.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#time-homogeneous-markov-chains"><i class="fa fa-check"></i><b>7.2.3</b> Time-Homogeneous Markov Chains</a></li>
<li class="chapter" data-level="7.2.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#matrix-representation"><i class="fa fa-check"></i><b>7.2.4</b> Matrix representation</a></li>
<li class="chapter" data-level="7.2.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#regular-markov-chains"><i class="fa fa-check"></i><b>7.2.5</b> Regular Markov Chains</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#back-to-king-markov"><i class="fa fa-check"></i><b>7.3</b> Back to King Markov</a></li>
<li class="chapter" data-level="7.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#how-well-does-the-metropolis-algorithm-work"><i class="fa fa-check"></i><b>7.4</b> How well does the Metropolis Algorithm work?</a><ul>
<li class="chapter" data-level="7.4.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-to-any-island"><i class="fa fa-check"></i><b>7.4.1</b> Jumping to any island</a></li>
<li class="chapter" data-level="7.4.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-only-to-neighbor-islands"><i class="fa fa-check"></i><b>7.4.2</b> Jumping only to neighbor islands</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#markov-chains-and-posterior-sampling"><i class="fa fa-check"></i><b>7.5</b> Markov Chains and Posterior Sampling</a><ul>
<li class="chapter" data-level="7.5.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#example-1-estimating-a-proportion"><i class="fa fa-check"></i><b>7.5.1</b> Example 1: Estimating a proportion</a></li>
<li class="chapter" data-level="7.5.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#example-2-estimating-mean-and-variance"><i class="fa fa-check"></i><b>7.5.2</b> Example 2: Estimating mean and variance</a></li>
<li class="chapter" data-level="7.5.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#issues-with-metropolis-algorithm"><i class="fa fa-check"></i><b>7.5.3</b> Issues with Metropolis Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#two-coins"><i class="fa fa-check"></i><b>7.6</b> Two coins</a><ul>
<li class="chapter" data-level="7.6.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#the-model"><i class="fa fa-check"></i><b>7.6.1</b> The model</a></li>
<li class="chapter" data-level="7.6.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#exact-analysis"><i class="fa fa-check"></i><b>7.6.2</b> Exact analysis</a></li>
<li class="chapter" data-level="7.6.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#metropolis"><i class="fa fa-check"></i><b>7.6.3</b> Metropolis</a></li>
<li class="chapter" data-level="7.6.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#gibbs-sampling"><i class="fa fa-check"></i><b>7.6.4</b> Gibbs sampling</a></li>
<li class="chapter" data-level="7.6.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#advantages-and-disadvantages-of-gibbs-vs-metropolis"><i class="fa fa-check"></i><b>7.6.5</b> Advantages and Disadvantages of Gibbs vs Metropolis</a></li>
<li class="chapter" data-level="7.6.6" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#so-what-do-we-learn-about-the-coins"><i class="fa fa-check"></i><b>7.6.6</b> So what do we learn about the coins?</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#mcmc-posterior-sampling-big-picture"><i class="fa fa-check"></i><b>7.7</b> MCMC posterior sampling: Big picture</a><ul>
<li class="chapter" data-level="7.7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#mcmc-markov-chain-monte-carlo"><i class="fa fa-check"></i><b>7.7.1</b> MCMC = Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="7.7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#posterior-sampling-random-walk-through-the-posterior"><i class="fa fa-check"></i><b>7.7.2</b> Posterior sampling: Random walk through the posterior</a></li>
<li class="chapter" data-level="7.7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#where-do-we-go-from-here-1"><i class="fa fa-check"></i><b>7.7.3</b> Where do we go from here?</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#ch07-exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html"><i class="fa fa-check"></i><b>8</b> JAGS – Just Another Gibbs Sampler</a><ul>
<li class="chapter" data-level="8.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#what-jags-is"><i class="fa fa-check"></i><b>8.1</b> What JAGS is</a><ul>
<li class="chapter" data-level="8.1.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#jags-documentation"><i class="fa fa-check"></i><b>8.1.1</b> JAGS documentation</a></li>
<li class="chapter" data-level="8.1.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#updating-c-and-clang"><i class="fa fa-check"></i><b>8.1.2</b> Updating C and CLANG</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-1-estimating-a-proportion-1"><i class="fa fa-check"></i><b>8.2</b> Example 1: estimating a proportion</a><ul>
<li class="chapter" data-level="8.2.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-1"><i class="fa fa-check"></i><b>8.2.1</b> The Model</a></li>
<li class="chapter" data-level="8.2.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#load-data"><i class="fa fa-check"></i><b>8.2.2</b> Load Data</a></li>
<li class="chapter" data-level="8.2.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#specify-the-model"><i class="fa fa-check"></i><b>8.2.3</b> Specify the model</a></li>
<li class="chapter" data-level="8.2.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#run-the-model"><i class="fa fa-check"></i><b>8.2.4</b> Run the model</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#extracting-information-from-a-jags-run"><i class="fa fa-check"></i><b>8.3</b> Extracting information from a JAGS run</a><ul>
<li class="chapter" data-level="8.3.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#posterior-1"><i class="fa fa-check"></i><b>8.3.1</b> posterior()</a></li>
<li class="chapter" data-level="8.3.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#side-note-posterior-sampling-and-the-grid-method"><i class="fa fa-check"></i><b>8.3.2</b> Side note: posterior sampling and the grid method</a></li>
<li class="chapter" data-level="8.3.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-coda"><i class="fa fa-check"></i><b>8.3.3</b> Using coda</a></li>
<li class="chapter" data-level="8.3.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-bayesplot"><i class="fa fa-check"></i><b>8.3.4</b> Using bayesplot</a></li>
<li class="chapter" data-level="8.3.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-kruschkes-functions"><i class="fa fa-check"></i><b>8.3.5</b> Using Kruschke’s functions</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#optional-arguments-to-jags"><i class="fa fa-check"></i><b>8.4</b> Optional arguments to jags()</a><ul>
<li class="chapter" data-level="8.4.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#number-and-size-of-chains"><i class="fa fa-check"></i><b>8.4.1</b> Number and size of chains</a></li>
<li class="chapter" data-level="8.4.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#starting-point-for-chains"><i class="fa fa-check"></i><b>8.4.2</b> Starting point for chains</a></li>
<li class="chapter" data-level="8.4.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#running-chains-in-parallel"><i class="fa fa-check"></i><b>8.4.3</b> Running chains in parallel</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-2-comparing-two-proportions"><i class="fa fa-check"></i><b>8.5</b> Example 2: comparing two proportions</a><ul>
<li class="chapter" data-level="8.5.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-data"><i class="fa fa-check"></i><b>8.5.1</b> The data</a></li>
<li class="chapter" data-level="8.5.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-2"><i class="fa fa-check"></i><b>8.5.2</b> The model</a></li>
<li class="chapter" data-level="8.5.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#describing-the-model-to-jags"><i class="fa fa-check"></i><b>8.5.3</b> Describing the model to JAGS</a></li>
<li class="chapter" data-level="8.5.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#fitting-the-model"><i class="fa fa-check"></i><b>8.5.4</b> Fitting the model</a></li>
<li class="chapter" data-level="8.5.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#inspecting-the-results"><i class="fa fa-check"></i><b>8.5.5</b> Inspecting the results</a></li>
<li class="chapter" data-level="8.5.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#difference-in-proportions"><i class="fa fa-check"></i><b>8.5.6</b> Difference in proportions</a></li>
<li class="chapter" data-level="8.5.7" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#sampling-from-the-prior"><i class="fa fa-check"></i><b>8.5.7</b> Sampling from the prior</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#ch08-exercises"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="heierarchical-models.html"><a href="heierarchical-models.html"><i class="fa fa-check"></i><b>9</b> Heierarchical Models</a><ul>
<li class="chapter" data-level="9.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#gamma-distributions"><i class="fa fa-check"></i><b>9.1</b> Gamma Distributions</a></li>
<li class="chapter" data-level="9.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#one-coin-from-one-mint"><i class="fa fa-check"></i><b>9.2</b> One coin from one mint</a></li>
<li class="chapter" data-level="9.3" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-one-mint"><i class="fa fa-check"></i><b>9.3</b> Multiple coins from one mint</a></li>
<li class="chapter" data-level="9.4" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-multiple-mints"><i class="fa fa-check"></i><b>9.4</b> Multiple coins from multiple mints</a></li>
<li class="chapter" data-level="9.5" data-path="heierarchical-models.html"><a href="heierarchical-models.html#therapeutic-touch"><i class="fa fa-check"></i><b>9.5</b> Therapeutic Touch</a><ul>
<li class="chapter" data-level="9.5.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#abstract"><i class="fa fa-check"></i><b>9.5.1</b> Abstract</a></li>
<li class="chapter" data-level="9.5.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#data-1"><i class="fa fa-check"></i><b>9.5.2</b> Data</a></li>
<li class="chapter" data-level="9.5.3" data-path="heierarchical-models.html"><a href="heierarchical-models.html#a-heierarchical-model"><i class="fa fa-check"></i><b>9.5.3</b> A heierarchical model</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="heierarchical-models.html"><a href="heierarchical-models.html#other-parameterizations-we-might-have-tried"><i class="fa fa-check"></i><b>9.6</b> Other parameterizations we might have tried</a><ul>
<li class="chapter" data-level="9.6.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#shape-parameters-for-beta"><i class="fa fa-check"></i><b>9.6.1</b> Shape parameters for Beta</a></li>
<li class="chapter" data-level="9.6.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#mean-instead-of-mode"><i class="fa fa-check"></i><b>9.6.2</b> Mean instead of mode</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="heierarchical-models.html"><a href="heierarchical-models.html#shrinkage"><i class="fa fa-check"></i><b>9.7</b> Shrinkage</a></li>
<li class="chapter" data-level="9.8" data-path="heierarchical-models.html"><a href="heierarchical-models.html#example-baseball-batting-average"><i class="fa fa-check"></i><b>9.8</b> Example: Baseball Batting Average</a></li>
<li class="chapter" data-level="9.9" data-path="heierarchical-models.html"><a href="heierarchical-models.html#ch09-exercises"><i class="fa fa-check"></i><b>9.9</b> Exerciess</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>10</b> (Model Comparison)</a></li>
<li class="chapter" data-level="11" data-path="nhst.html"><a href="nhst.html"><i class="fa fa-check"></i><b>11</b> (NHST)</a></li>
<li class="chapter" data-level="12" data-path="point-null-hypotheses.html"><a href="point-null-hypotheses.html"><i class="fa fa-check"></i><b>12</b> (Point Null Hypotheses)</a></li>
<li class="chapter" data-level="13" data-path="goals-power-sample-size.html"><a href="goals-power-sample-size.html"><i class="fa fa-check"></i><b>13</b> (Goals, Power, Sample Size)</a></li>
<li class="chapter" data-level="14" data-path="stan.html"><a href="stan.html"><i class="fa fa-check"></i><b>14</b> Stan</a><ul>
<li class="chapter" data-level="14.1" data-path="stan.html"><a href="stan.html#why-stan-might-work-better"><i class="fa fa-check"></i><b>14.1</b> Why Stan might work better</a></li>
<li class="chapter" data-level="14.2" data-path="stan.html"><a href="stan.html#describing-a-model-to-stan"><i class="fa fa-check"></i><b>14.2</b> Describing a model to Stan</a></li>
<li class="chapter" data-level="14.3" data-path="stan.html"><a href="stan.html#samping-from-the-prior"><i class="fa fa-check"></i><b>14.3</b> Samping from the prior</a></li>
<li class="chapter" data-level="14.4" data-path="stan.html"><a href="stan.html#exercises"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="glm-overview.html"><a href="glm-overview.html"><i class="fa fa-check"></i><b>15</b> GLM Overview</a><ul>
<li class="chapter" data-level="15.1" data-path="glm-overview.html"><a href="glm-overview.html#data-consists-of-observations-of-variables"><i class="fa fa-check"></i><b>15.1</b> Data consists of observations of variables</a><ul>
<li class="chapter" data-level="15.1.1" data-path="glm-overview.html"><a href="glm-overview.html#variable-roles"><i class="fa fa-check"></i><b>15.1.1</b> Variable Roles</a></li>
<li class="chapter" data-level="15.1.2" data-path="glm-overview.html"><a href="glm-overview.html#types-of-variables"><i class="fa fa-check"></i><b>15.1.2</b> Types of Variables</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="glm-overview.html"><a href="glm-overview.html#glm-framework"><i class="fa fa-check"></i><b>15.2</b> GLM Framework</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html"><i class="fa fa-check"></i><b>16</b> Estimating One and Two Means</a><ul>
<li class="chapter" data-level="16.1" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#basic-model-for-two-means"><i class="fa fa-check"></i><b>16.1</b> Basic Model for Two Means</a><ul>
<li class="chapter" data-level="16.1.1" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#data-2"><i class="fa fa-check"></i><b>16.1.1</b> Data</a></li>
<li class="chapter" data-level="16.1.2" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#model"><i class="fa fa-check"></i><b>16.1.2</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#an-old-sleep-study"><i class="fa fa-check"></i><b>16.2</b> An Old Sleep Study</a><ul>
<li class="chapter" data-level="16.2.1" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#data-3"><i class="fa fa-check"></i><b>16.2.1</b> Data</a></li>
<li class="chapter" data-level="16.2.2" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#model-1"><i class="fa fa-check"></i><b>16.2.2</b> Model</a></li>
<li class="chapter" data-level="16.2.3" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#separate-standard-deviations-for-each-group"><i class="fa fa-check"></i><b>16.2.3</b> Separate standard deviations for each group</a></li>
<li class="chapter" data-level="16.2.4" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#comparison-to-t-test"><i class="fa fa-check"></i><b>16.2.4</b> Comparison to t-test</a></li>
<li class="chapter" data-level="16.2.5" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#rope-region-of-practical-equivalence"><i class="fa fa-check"></i><b>16.2.5</b> ROPE (Region of Practical Equivalence)</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#variations-on-the-theme"><i class="fa fa-check"></i><b>16.3</b> Variations on the theme</a><ul>
<li class="chapter" data-level="16.3.1" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#other-distributions-for-the-response"><i class="fa fa-check"></i><b>16.3.1</b> Other distributions for the response</a></li>
<li class="chapter" data-level="16.3.2" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#other-priors-for-sigma-or-tau"><i class="fa fa-check"></i><b>16.3.2</b> Other Priors for <span class="math inline">\(\sigma\)</span> (or <span class="math inline">\(\tau\)</span>)</a></li>
<li class="chapter" data-level="16.3.3" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#paired-comparisons"><i class="fa fa-check"></i><b>16.3.3</b> Paired Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#how-many-chains-how-long"><i class="fa fa-check"></i><b>16.4</b> How many chains? How long?</a><ul>
<li class="chapter" data-level="16.4.1" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#why-multiple-chains"><i class="fa fa-check"></i><b>16.4.1</b> Why multiple chains?</a></li>
<li class="chapter" data-level="16.4.2" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#what-large-n.eff-does-and-doesnt-do-for-us"><i class="fa fa-check"></i><b>16.4.2</b> What large n.eff does and doesn’t do for us</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#looking-at-likelihood"><i class="fa fa-check"></i><b>16.5</b> Looking at Likelihood</a></li>
<li class="chapter" data-level="16.6" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#exercises-1"><i class="fa fa-check"></i><b>16.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>17</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-deluxe-basic-model"><i class="fa fa-check"></i><b>17.1</b> The deluxe basic model</a><ul>
<li class="chapter" data-level="17.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#likelihood-1"><i class="fa fa-check"></i><b>17.1.1</b> Likelihood</a></li>
<li class="chapter" data-level="17.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#priors"><i class="fa fa-check"></i><b>17.1.2</b> Priors</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#example-galtons-data"><i class="fa fa-check"></i><b>17.2</b> Example: Galton’s Data</a><ul>
<li class="chapter" data-level="17.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#describing-the-model-to-jags-1"><i class="fa fa-check"></i><b>17.2.1</b> Describing the model to JAGS</a></li>
<li class="chapter" data-level="17.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#problems-and-how-to-fix-them"><i class="fa fa-check"></i><b>17.2.2</b> Problems and how to fix them</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standardizing"><i class="fa fa-check"></i><b>17.3</b> Centering and Standardizing</a><ul>
<li class="chapter" data-level="17.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#beta_0-and-beta_1-are-still-correlated"><i class="fa fa-check"></i><b>17.3.1</b> <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are still correlated</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weve-fit-a-model-now-what"><i class="fa fa-check"></i><b>17.4</b> We’ve fit a model, now what?</a><ul>
<li class="chapter" data-level="17.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#estimate-parameters"><i class="fa fa-check"></i><b>17.4.1</b> Estimate parameters</a></li>
<li class="chapter" data-level="17.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#make-predictions"><i class="fa fa-check"></i><b>17.4.2</b> Make predictions</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fitting-models-with-stan"><i class="fa fa-check"></i><b>17.5</b> Fitting models with Stan</a></li>
<li class="chapter" data-level="17.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>17.6</b> Exercises</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">(Re)Doing Bayesain Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-linear-regression" class="section level1">
<h1><span class="header-section-number">17</span> Simple Linear Regression</h1>
<p>Situation:</p>
<ul>
<li>Metric response</li>
<li>Matric predictor</li>
</ul>
<div id="the-deluxe-basic-model" class="section level2">
<h2><span class="header-section-number">17.1</span> The deluxe basic model</h2>
<div id="likelihood-1" class="section level3">
<h3><span class="header-section-number">17.1.1</span> Likelihood</h3>
<p><span class="math display">\[\begin{align*}
y_{i} &amp;\sim {\sf Norm}(\mu_i, \sigma) \\
\mu_i &amp;\sim \beta_0 + \beta_1 x_i
\end{align*}\]</span></p>
<p>Some variations:</p>
<ul>
<li>Replace normal distribution with something else (t is common).</li>
<li>Allow standard deviations to vary with <span class="math inline">\(x\)</span> as well as the mean.</li>
<li>Use a different functional relationship between explanatory and response (non-linear regression)</li>
</ul>
<p>Each of these is relatively easy to do.
The first variation is sometimes called <strong>robust regression</strong> becuase it
is more robust to unusual observations. Since it is no harder to work
with t distributions than with normal distributions, that will become our
go-to simple linear regression model.</p>
<p><span class="math display">\[\begin{align*}
y_{i} &amp;\sim {\sf T}(\mu_i, \sigma, \nu) \\
\mu_i &amp;\sim \beta_0 + \beta_1 x_i
\end{align*}\]</span></p>
</div>
<div id="priors" class="section level3">
<h3><span class="header-section-number">17.1.2</span> Priors</h3>
<p>We need priors for <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\sigma\)</span>, and <span class="math inline">\(\nu\)</span>.</p>
<ul>
<li><p><span class="math inline">\(\nu\)</span>: We’ve already seend that a <strong>shifted Gamma</strong> with mean around
30 works well as a generic prior giving the data room to stear us
away from normality if warranted.</p></li>
<li><p><span class="math inline">\(\beta_1\)</span>: The MLE for <span class="math inline">\(\beta_1\)</span> is</p>
<p><span class="math display">\[ \hat\beta_1 = r \frac{SD_y}{SD_x}\]</span>
so it makes sense to have a prior broadly covers
the interval <span class="math inline">\((- \frac{SD_y}{SD_x}, \frac{SD_y}{SD_x})\)</span>.</p></li>
<li><p><span class="math inline">\(\beta_0\)</span>: The MLE for <span class="math inline">\(\beta_0\)</span> is</p>
<p><span class="math display">\[ \hat\beta_0 \; = \;  \overline{y} - \hat \beta_1 \overline{x}  \; = \; \overline{y} - r \frac{SD_y}{SD_x} \cdot \overline{x}\]</span></p>
<p>so we can pick a prior that broadly covers the interval
<span class="math inline">\((\overline{y} - \frac{SD_y}{SD_x} \cdot \overline{x}, \overline{y} - \frac{SD_y}{SD_x} \cdot \overline{x})\)</span></p></li>
<li><p><span class="math inline">\(\sigma\)</span> measures the amount of variability in responses for a
<em>fixed value</em> of <span class="math inline">\(x\)</span> (and is assumed to be the same for each <span class="math inline">\(x\)</span> in
the simple version of the model). A weakly informative prior
should cover the range of reasonable values of <span class="math inline">\(\sigma\)</span> with plenty of
room to spare.
(Our 2-or-3-orders-of-magnititude-either-way uniform distribution
might be a reasonable starting point.)</p></li>
</ul>
<p>Here’s the big picture:</p>
<p><img src="images/Fig17-9.png" width="615" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="example-galtons-data" class="section level2">
<h2><span class="header-section-number">17.2</span> Example: Galton’s Data</h2>
<p>Since we are looking at regression, let’s use an historical data set that
was part of the origins of the regression story: Galton’s data on height.
Galton collected data on the heights of adults and their parents.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(mosaicData<span class="op">::</span>Galton)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">family</th>
<th align="right">father</th>
<th align="right">mother</th>
<th align="left">sex</th>
<th align="right">height</th>
<th align="right">nkids</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="right">78.5</td>
<td align="right">67.0</td>
<td align="left">M</td>
<td align="right">73.2</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="right">78.5</td>
<td align="right">67.0</td>
<td align="left">F</td>
<td align="right">69.2</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="left">1</td>
<td align="right">78.5</td>
<td align="right">67.0</td>
<td align="left">F</td>
<td align="right">69.0</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="right">78.5</td>
<td align="right">67.0</td>
<td align="left">F</td>
<td align="right">69.0</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="right">75.5</td>
<td align="right">66.5</td>
<td align="left">M</td>
<td align="right">73.5</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="right">75.5</td>
<td align="right">66.5</td>
<td align="left">M</td>
<td align="right">72.5</td>
<td align="right">4</td>
</tr>
</tbody>
</table>
<p>To keep things simpler for the moment, let’s consider only women, and only one
sibling per family.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">54321</span>)
<span class="kw">library</span>(dplyr)
GaltonW &lt;-
<span class="st">  </span>mosaicData<span class="op">::</span>Galton <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(sex <span class="op">==</span><span class="st"> &quot;F&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(family) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">1</span>)</code></pre>
<p>Galton was interested in how people’s heights are related to their parents’
heights. He compbined the parents’ heights into the “mid-parent height”, which
was the average of the two.</p>
<pre class="sourceCode r"><code class="sourceCode r">GaltonW &lt;-<span class="st"> </span>
<span class="st">  </span>GaltonW <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">midparent =</span> (father <span class="op">+</span><span class="st"> </span>mother) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)
<span class="kw">gf_point</span>(height <span class="op">~</span><span class="st"> </span>midparent, <span class="dt">data =</span> GaltonW, <span class="dt">alpha =</span> <span class="fl">0.5</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch17-midparent-1.png" width="576" /></p>
<div id="describing-the-model-to-jags-1" class="section level3">
<h3><span class="header-section-number">17.2.1</span> Describing the model to JAGS</h3>
<pre class="sourceCode r"><code class="sourceCode r">galton_model &lt;-<span class="st"> </span><span class="cf">function</span>() {
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(y)) {
    y[i]   <span class="op">~</span><span class="st"> </span><span class="kw">dt</span>(mu[i], <span class="dv">1</span><span class="op">/</span>sigma<span class="op">^</span><span class="dv">2</span>, nu)
    mu[i] &lt;-<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>x[i]
  }
  sigma <span class="op">~</span><span class="st"> </span><span class="kw">dunif</span>(<span class="dv">6</span><span class="op">/</span><span class="dv">100</span>, <span class="dv">6</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>)
  nuMinusOne <span class="op">~</span><span class="st"> </span><span class="kw">dexp</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">29</span>)
  nu &lt;-<span class="st"> </span>nuMinusOne <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
  beta0 <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">100</span><span class="op">^</span><span class="dv">2</span>)   <span class="co"># 100 is order of magnitude of data</span>
  beta1 <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">^</span><span class="dv">2</span>)     <span class="co"># expect roughly 1-1 slope</span>
}</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(R2jags)
<span class="kw">library</span>(mosaic)
galton_jags &lt;-
<span class="st">  </span><span class="kw">jags</span>(
    <span class="dt">model =</span> galton_model,
    <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">y =</span> GaltonW<span class="op">$</span>height, <span class="dt">x =</span> GaltonW<span class="op">$</span>midparent),
    <span class="dt">parameters.to.save =</span> <span class="kw">c</span>(<span class="st">&quot;beta0&quot;</span>, <span class="st">&quot;beta1&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;nu&quot;</span>),
    <span class="dt">n.iter =</span> <span class="dv">5000</span>,
    <span class="dt">n.burnin =</span> <span class="dv">2000</span>,
    <span class="dt">n.chains =</span> <span class="dv">4</span>,
    <span class="dt">n.thin =</span> <span class="dv">1</span>
  )</code></pre>
<pre><code>## module glm loaded</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(bayesplot)
<span class="kw">library</span>(CalvinBayes)
<span class="kw">summary</span>(galton_jags)</code></pre>
<pre><code>## fit using jags
##  4 chains, each with 5000 iterations (first 2000 discarded)
##  n.sims = 12000 iterations saved
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## beta0     10.695   8.008  -4.072   5.664  11.532  17.189  24.534 4.376     4
## beta1      0.800   0.120   0.593   0.702   0.787   0.877   1.020 4.071     4
## nu        33.016  27.311   6.134  14.178  24.807  42.854 106.351 1.002  1600
## sigma      1.849   0.130   1.594   1.761   1.849   1.935   2.102 1.020   140
## deviance 699.315   4.276 694.765 696.069 697.727 701.525 709.475 2.312     6
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 3.3 and DIC = 702.6</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_combo</span>(<span class="kw">as.mcmc</span>(galton_jags))</code></pre>
<p><img src="Redoing_files/figure-html/ch17-galton-summary-1.png" width="576" /></p>
</div>
<div id="problems-and-how-to-fix-them" class="section level3">
<h3><span class="header-section-number">17.2.2</span> Problems and how to fix them</h3>
<p>Clearly something is not working the way we would like with this model!
Here’s a clue as to the problem:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">posterior</span>(galton_jags) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gf_point</span>(beta0 <span class="op">~</span><span class="st"> </span>beta1, <span class="dt">color =</span> <span class="op">~</span><span class="st"> </span>chain, <span class="dt">alpha =</span> <span class="fl">0.2</span>, <span class="dt">size =</span> <span class="fl">0.4</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_density2d</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch17-galton-problems-1.png" width="576" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">posterior</span>(galton_jags) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(iter <span class="op">&lt;=</span><span class="st"> </span><span class="dv">250</span>, chain <span class="op">==</span><span class="st"> &quot;chain:1&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_step</span>(beta0 <span class="op">~</span><span class="st"> </span>beta1, <span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">color =</span> <span class="op">~</span>iter) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_density2d</span>(<span class="dt">alpha =</span> <span class="fl">0.2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_color_viridis_c</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_facet_wrap</span>(<span class="op">~</span>chain) <span class="co">#, scales = &quot;free&quot;)</span></code></pre>
<p><img src="Redoing_files/figure-html/ch17-galton-problems-2.png" width="576" /></p>
<p>The correlation of the parameters in the posterior distribution produces a
long, narrow, diagonal ridge that the Gibbs sampler samples only very slowly
because it keeps bumping into edge of the cliff. (Remember, the Gibbs sampler
only moves in “primary” directions.)</p>
<p>So how do we fix this?
This is supposed to be the <em>simple</em> linear model after all.
There are two ways we could hope to fix our problem.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Reparameterize the model</strong> so that the correlation between parameters
(in the posterior distribution) is reduced or eliminated.</p></li>
<li><p><strong>Use a different algorithm</strong> for posterior sampling. The problem is
not with our model <em>per se</em>, rather it is with the method we are using (Gibbs)
to sample from the posterior. Perhaps another algorithm will work
better.</p></li>
</ol>
</div>
</div>
<div id="centering-and-standardizing" class="section level2">
<h2><span class="header-section-number">17.3</span> Centering and Standardizing</h2>
<ul>
<li><p>Reparameterization 1: <strong>centering</strong></p>
<p>We can express this model as</p>
<p><span class="math display">\[\begin{align*}
  y_{i} &amp;\sim {\sf T}(\mu_i, \sigma, \nu) \\
  \mu_i &amp;= \alpha_0 + \alpha_1 (x_i - \overline{x})
  \end{align*}\]</span></p>
<p>Since</p>
<p><span class="math display">\[\begin{align*}
  \alpha_0 + \alpha_1 (x_i - \overline{x}) 
  &amp;= (\alpha_0 - \alpha_1 \overline{x}) + \alpha_1 x_i 
  \end{align*}\]</span></p>
<p>We see that <span class="math inline">\(\beta_0 = \alpha_0 - \alpha_1 \overline{x}\)</span> and
<span class="math inline">\(\beta_1 = \alpha_1\)</span>. So we can easily recover the original
parameters if we like. (And if we are primarily interested in <span class="math inline">\(\beta_1\)</span>,
no translation is required.)</p>
<p>This reparameterization maintains the natural scale of the data, and both
<span class="math inline">\(\alpha_0\)</span> and <span class="math inline">\(\alpha_1\)</span> are easily interpreted: <span class="math inline">\(\alpha_0\)</span> is the mean
response when the predictor is the average of the predictor values <em>in the data</em>.</p></li>
<li><p>Reparameterization 2: <strong>standardization</strong></p>
<p>We can also express our model as</p>
<p><span class="math display">\[\begin{align*}
  z_{y_{i}} &amp;\sim {\sf T}(\mu_i, \sigma, \nu) \\[3mm]
  \mu_i     &amp;= \alpha_0 + \alpha_1 z_{x_i} \\[5mm]
  z_{x_i}   &amp;=    \frac{x_i - \overline{x}}{SD_x} \\[3mm]
  z_{y_i}   &amp;=    \frac{y_i - \overline{y}}{SD_y} \\[3mm]
  \end{align*}\]</span></p>
<p>Here the change in the model is due to a transformation of the data. Subtracting the mean
and dividing by the standard deviation is called <strong>standardization</strong>, and the values
produced are sometimes called <strong>z-scores</strong>. The resulting distributions of <span class="math inline">\(zy\)</span> and <span class="math inline">\(zx\)</span>
will have mean 0 and standard deviation 1. So in addition to breaking the correlation
pattern, we have now put things on a standard scale, regardless of what the original
units were. This can be useful for picking constants in priors (we won’t have to estimate
the scale of the data involved). In addition, some algorithms work better if all the
variables involved have roughly the same scale.</p>
<p>The downside is that we usually need to convert back to the original scales of
<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in order to interpret the results. But this is only a matter
of a little easy algebra:</p>
<p><span class="math display">\[\begin{align*} 
  \hat{z}_{y_i} &amp;= \alpha_0 + \alpha_1 z{x_i}
  \\
  \frac{\hat{y}_i - \overline{y}}{SD_y} &amp;= \alpha_0 + \alpha_1 \frac{x_i - \overline{x}}{SD_x}
  \\
  \hat{y}_i &amp;= \overline{y} + \alpha_0 SD_y + \alpha_1 SD_y \frac{x_i - \overline{x}}{SD_x}
  \\
  \hat{y}_i &amp;= 
  \underbrace{\left[\overline{y} + \alpha_0 SD_y - \alpha_1\frac{SD_y}{SD_x} \overline{x} \right]}_{\beta_0} + 
  \underbrace{\left[\alpha_1 \frac{SD_y}{SD_x}\right]}_{\beta_1} x_i
  \end{align*}\]</span></p></li>
</ul>
<p>Since Kruscske demonstrates standardization, we’ll do centering here.</p>
<pre class="sourceCode r"><code class="sourceCode r">galtonC_model &lt;-<span class="st"> </span><span class="cf">function</span>() {
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(y)) {
    y[i]   <span class="op">~</span><span class="st"> </span><span class="kw">dt</span>(mu[i], <span class="dv">1</span><span class="op">/</span>sigma<span class="op">^</span><span class="dv">2</span>, nu)
    mu[i] &lt;-<span class="st"> </span>alpha0 <span class="op">+</span><span class="st"> </span>alpha1 <span class="op">*</span><span class="st"> </span>(x[i] <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x))
  }
  sigma <span class="op">~</span><span class="st"> </span><span class="kw">dunif</span>(<span class="dv">6</span><span class="op">/</span><span class="dv">100</span>, <span class="dv">6</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>)
  nuMinusOne <span class="op">~</span><span class="st"> </span><span class="kw">dexp</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">29</span>)
  nu &lt;-<span class="st"> </span>nuMinusOne <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
  alpha0 <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">100</span><span class="op">^</span><span class="dv">2</span>)   <span class="co"># 100 is order of magnitude of data</span>
  alpha1 <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">^</span><span class="dv">2</span>)     <span class="co"># expect roughly 1-1 slope</span>
  beta0 =<span class="st"> </span>alpha0 <span class="op">-</span><span class="st"> </span>alpha1 <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(x)
  beta1 =<span class="st"> </span>alpha1               <span class="co"># not necessary, but gives us both names</span>
}
galtonC_jags &lt;-
<span class="st">  </span><span class="kw">jags</span>(
    <span class="dt">model =</span> galtonC_model,
    <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">y =</span> GaltonW<span class="op">$</span>height, <span class="dt">x =</span> GaltonW<span class="op">$</span>midparent),
    <span class="dt">parameters.to.save =</span> <span class="kw">c</span>(<span class="st">&quot;beta0&quot;</span>, <span class="st">&quot;beta1&quot;</span>, <span class="st">&quot;alpha0&quot;</span>, <span class="st">&quot;alpha1&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;nu&quot;</span>),
    <span class="dt">n.iter =</span> <span class="dv">5000</span>,
    <span class="dt">n.burnin =</span> <span class="dv">2000</span>,
    <span class="dt">n.chains =</span> <span class="dv">4</span>,
    <span class="dt">n.thin =</span> <span class="dv">1</span>
  )</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(galtonC_jags)</code></pre>
<pre><code>## fit using jags
##  4 chains, each with 5000 iterations (first 2000 discarded)
##  n.sims = 12000 iterations saved
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## alpha0    64.105   0.149  63.812  64.004  64.106  64.208  64.390 1.001 12000
## alpha1     0.740   0.081   0.577   0.687   0.740   0.795   0.900 1.001 12000
## beta0     14.686   5.428   4.008  11.050  14.655  18.253  25.540 1.001 12000
## beta1      0.740   0.081   0.577   0.687   0.740   0.795   0.900 1.001 12000
## nu        32.250  24.769   6.281  14.368  24.592  42.701  98.596 1.001  8600
## sigma      1.841   0.128   1.585   1.757   1.842   1.926   2.091 1.001  5300
## deviance 697.587   2.496 694.651 695.740 696.961 698.787 704.017 1.001 12000
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 3.1 and DIC = 700.7</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_combo</span>(<span class="kw">as.mcmc</span>(galtonC_jags))</code></pre>
<p><img src="Redoing_files/figure-html/ch17-galton-centering-look-1.png" width="576" /></p>
<p>Ah! That looks much better than before.</p>
<div id="beta_0-and-beta_1-are-still-correlated" class="section level3">
<h3><span class="header-section-number">17.3.1</span> <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are still correlated</h3>
<p>Reparameterization has not changed our model, only the way it is described.
In particular, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> remain correlated in the
posterior. But <span class="math inline">\(\alpha_0\)</span> and <span class="math inline">\(\alpha_1\)</span> are not correlated, and these
are the parameters JAGS is using to sample.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_point</span>(beta1 <span class="op">~</span><span class="st"> </span>beta0, <span class="dt">data =</span> <span class="kw">posterior</span>(galtonC_jags), <span class="dt">alpha =</span> <span class="fl">0.1</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch17-still-correlated-1.png" width="576" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_point</span>(alpha1 <span class="op">~</span><span class="st"> </span>alpha0, <span class="dt">data =</span> <span class="kw">posterior</span>(galtonC_jags), <span class="dt">alpha =</span> <span class="fl">0.1</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch17-still-correlated-2.png" width="576" /></p>
</div>
</div>
<div id="weve-fit-a-model-now-what" class="section level2">
<h2><span class="header-section-number">17.4</span> We’ve fit a model, now what?</h2>
<p>After centering or standardizing, JAGS works much better. We can now sample from
our posterior distribution. But what do we do with our posterior samples?</p>
<div id="estimate-parameters" class="section level3">
<h3><span class="header-section-number">17.4.1</span> Estimate parameters</h3>
<p>If we are primarily interested in a regression parameter
(usually the slope parameter is much more interesting than the intercept
parameter), we can use an HDI to express our estimate.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hdi</span>(<span class="kw">posterior</span>(galtonC_jags), <span class="dt">pars =</span> <span class="st">&quot;beta1&quot;</span>)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">par</th>
<th align="right">lo</th>
<th align="right">hi</th>
<th align="right">prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">beta1</td>
<td align="right">0.5825</td>
<td align="right">0.9044</td>
<td align="right">0.95</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_areas</span>(<span class="kw">as.mcmc</span>(galtonC_jags), <span class="dt">pars =</span> <span class="st">&quot;beta1&quot;</span>, <span class="dt">prob =</span> <span class="fl">0.95</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch17-galtonC-estimate-params-1.png" width="576" /></p>
<p>Galton noticed what we see here: that the slope is less than 1. This means that
children of taller than average parents tend to be shorter than their parents
and children of below average parents tend to be taller than their parents.
He referred to this in his paper as
<a href="http://www.stat.ucla.edu/~nchristo/statistics100C/history_regression.pdf">“regression towards mediocrity”</a>. As it turns out, this was not a special feature
of the heridity of heights but a general feature of linear models.
Find out more in this <a href="https://en.wikipedia.org/wiki/Regression_toward_the_mean">Wikipedia artilce</a>.</p>
</div>
<div id="make-predictions" class="section level3">
<h3><span class="header-section-number">17.4.2</span> Make predictions</h3>
<p>Suppse we know the heights of a father and mother, from which we compute
ther mid-parent height <span class="math inline">\(x\)</span>.
How tall would we predict their daughters will be as adults?
Each posterior sample provides an answer
by describing a t distribution with <code>nu</code> degrees of freedom,
mean <span class="math inline">\(\beta_0 + \beta_1 x\)</span>, and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>The posterior distribution of the average hieght of daughters born
to parents with midparent height <span class="math inline">\(x = 70\)</span> is shown below, along
with an HDI.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">posterior</span>(galtonC_jags) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mean_daughter =</span> beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span><span class="dv">70</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_dens</span>(<span class="op">~</span>mean_daughter)</code></pre>
<p><img src="Redoing_files/figure-html/ch17-galtonC-preditions-1.png" width="576" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Galton_hdi &lt;-
<span class="st">  </span><span class="kw">posterior</span>(galtonC_jags) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mean_daughter =</span> beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span><span class="dv">70</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">hdi</span>(<span class="dt">pars =</span> <span class="st">&quot;mean_daughter&quot;</span>)
Galton_hdi</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">par</th>
<th align="right">lo</th>
<th align="right">hi</th>
<th align="right">prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">mean_daughter</td>
<td align="right">65.89</td>
<td align="right">67.07</td>
<td align="right">0.95</td>
</tr>
</tbody>
</table>
<p>So on average, we would predict the daughters to be about
66 or 67 inches tall.</p>
<p>We can visualize this by drawing a line for each posterior sample.
The HDI should span the middle 95% of these.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_abline</span>(<span class="dt">intercept =</span> <span class="op">~</span>beta0, <span class="dt">slope =</span> <span class="op">~</span>beta1, <span class="dt">alpha =</span> <span class="fl">0.01</span>,
          <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>, 
          <span class="dt">data =</span> <span class="kw">posterior</span>(galtonC_jags) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_n</span>(<span class="dv">2000</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(height <span class="op">~</span><span class="st"> </span>midparent, <span class="dt">data =</span> GaltonW, 
           <span class="dt">inherit =</span> <span class="ot">FALSE</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_errorbar</span>(lo <span class="op">+</span><span class="st"> </span>hi <span class="op">~</span><span class="st"> </span><span class="dv">70</span>, <span class="dt">data =</span> Galton_hdi, <span class="dt">color =</span> <span class="st">&quot;skyblue&quot;</span>, 
              <span class="dt">width =</span> <span class="fl">0.2</span>, <span class="dt">size =</span> <span class="fl">1.2</span>, <span class="dt">inherit =</span> <span class="ot">FALSE</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch17-galtonC-showlines-1.png" width="576" /></p>
<p>But this may not be the sort of prediction we want. Notice that most daughters’
heights are not in the blue band in the picture. That band tells about
the <em>mean</em> but doesn’t take into account how much individuals vary about
that mean. We can add that information in by taking our estimate for
<span class="math inline">\(\sigma\)</span> into account.</p>
<p>Here we generate heights by adding noise to the estimate given by
values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">posterior</span>(galtonC_jags) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">new_ht =</span> beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span><span class="dv">70</span> <span class="op">+</span><span class="st"> </span><span class="kw">rt</span>(<span class="dv">1200</span>, <span class="dt">df =</span> nu) <span class="op">*</span><span class="st"> </span>sigma) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(new_ht <span class="op">~</span><span class="st"> </span><span class="dv">70</span>, <span class="dt">alpha =</span> <span class="fl">0.01</span>, <span class="dt">size =</span> <span class="fl">0.7</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(height <span class="op">~</span><span class="st"> </span>midparent, <span class="dt">data =</span> GaltonW, 
           <span class="dt">inherit =</span> <span class="ot">FALSE</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>) </code></pre>
<p><img src="Redoing_files/figure-html/ch17-galtonC-plot-predictions-1.png" width="576" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Galton_hdi2 &lt;-
<span class="st">  </span><span class="kw">posterior</span>(galtonC_jags) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">new_ht =</span> beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span><span class="dv">70</span> <span class="op">+</span><span class="st"> </span><span class="kw">rt</span>(<span class="dv">1200</span>, <span class="dt">df =</span> nu) <span class="op">*</span><span class="st"> </span>sigma) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">hdi</span>(<span class="dt">regex_pars =</span> <span class="st">&quot;new&quot;</span>) 
Galton_hdi2</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">par</th>
<th align="right">lo</th>
<th align="right">hi</th>
<th align="right">prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">new_ht</td>
<td align="right">62.38</td>
<td align="right">70.53</td>
<td align="right">0.95</td>
</tr>
</tbody>
</table>
<p>So our model expects that most daughters whose parents have a midparent height
of 70 inches are between<br />
62.4 and
70.5
inches tall. Notice that this interval
is taking into account both the uncertainty in our estimates of the
parameters <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\sigma\)</span>, and <span class="math inline">\(\nu\)</span> and the variability in
heights that <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\nu\)</span> indicate.</p>
<p>With a little more work, we can create intervals like this at several different
midparent heights.</p>
<pre class="sourceCode r"><code class="sourceCode r">Post_galtonC &lt;-<span class="st"> </span><span class="kw">posterior</span>(galtonC_jags)

Grid &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">expand.grid</span>(<span class="dt">midparent =</span> <span class="dv">60</span><span class="op">:</span><span class="dv">75</span>, <span class="dt">iter =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Post_galtonC)) 
 
<span class="kw">posterior</span>(galtonC_jags) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">noise =</span> <span class="kw">rt</span>(<span class="dv">12000</span>, <span class="dt">df =</span> nu)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(Grid) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">height =</span> beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>midparent <span class="op">+</span><span class="st"> </span>noise <span class="op">*</span><span class="st"> </span>sigma) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(midparent) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="kw">hdi</span>(., <span class="dt">pars =</span> <span class="st">&quot;height&quot;</span>)) </code></pre>
<pre><code>## Joining, by = &quot;iter&quot;</code></pre>
<pre><code>## # A tibble: 16 x 5
## # Groups:   midparent [16]
##    midparent par       lo    hi  prob
##        &lt;int&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1        60 height  55.2  63.2  0.95
##  2        61 height  55.9  63.9  0.95
##  3        62 height  56.7  64.6  0.95
##  4        63 height  57.4  65.3  0.95
##  5        64 height  58.3  66.1  0.95
##  6        65 height  59.1  66.8  0.95
##  7        66 height  59.7  67.4  0.95
##  8        67 height  60.5  68.2  0.95
##  9        68 height  61.2  69.0  0.95
## 10        69 height  61.9  69.6  0.95
## 11        70 height  62.6  70.4  0.95
## 12        71 height  63.1  70.9  0.95
## 13        72 height  63.9  71.8  0.95
## 14        73 height  64.7  72.5  0.95
## 15        74 height  65.4  73.3  0.95
## 16        75 height  66.0  74.1  0.95</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">posterior</span>(galtonC_jags) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">noise =</span> <span class="kw">rt</span>(<span class="dv">12000</span>, <span class="dt">df =</span> nu)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(Grid) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">avg_height =</span> beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>midparent,
         <span class="dt">height =</span> avg_height <span class="op">+</span><span class="st"> </span>noise <span class="op">*</span><span class="st"> </span>sigma) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(midparent) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="kw">hdi</span>(., <span class="dt">pars =</span> <span class="st">&quot;height&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_ribbon</span>(lo <span class="op">+</span><span class="st"> </span>hi <span class="op">~</span><span class="st"> </span>midparent, <span class="dt">fill =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_errorbar</span>(lo <span class="op">+</span><span class="st"> </span>hi <span class="op">~</span><span class="st"> </span>midparent, <span class="dt">width =</span> <span class="fl">0.2</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">size =</span> <span class="fl">1.2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(height <span class="op">~</span><span class="st"> </span>midparent, <span class="dt">data =</span> GaltonW, 
           <span class="dt">inherit =</span> <span class="ot">FALSE</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>)</code></pre>
<pre><code>## Joining, by = &quot;iter&quot;</code></pre>
<p><img src="Redoing_files/figure-html/ch17-galtonC-ppc-1.png" width="576" /></p>
<p>Comparing the data to the posterior predictions of the model is called a
<strong>posterior predictive check</strong>; we are checking to see whether the data
are consistent with what our posterior distribution would predict.
In this case, things look good: most, but not all of the data is falling
inside the band where our models predicts 95% of new observations would
fall.</p>
<p>If the posterior predictive check indicates systematic problems with our model,
it may lead us to propose another (we hope better) model.</p>

</div>
</div>
<div id="fitting-models-with-stan" class="section level2">
<h2><span class="header-section-number">17.5</span> Fitting models with Stan</h2>
<p>Centering (or standardizing) is sufficient to make JAGS efficient enough to use.
But we can also use Stan, and since Stan is not bothered by correlation in the
posterior the way JAGS is, Stan works well even without reparamterizing the model.</p>
<p>Here is the Stan equivalent to our original JAGS model.</p>
<pre class="stan"><code>data {
  int&lt;lower=0&gt; N;     // N is a non-negative integer
  real y[N];          // y is a length-N vector of reals
  real x[N];          // x is a length-N vector of reals
}
parameters {
  real beta0;  
  real beta1;  
  real&lt;lower=0&gt; sigma;
  real&lt;lower=0&gt; nuMinusOne;
} 
transformed parameters{
  real&lt;lower=0&gt; nu;
  nu = nuMinusOne + 1;
}
model { 
  for (i in 1:N) {
    y[i] ~ student_t(nu, beta0 + beta1 * x[i], sigma);
  }
  beta0 ~ normal(0, 100);
  beta1 ~ normal(0, 4);
  sigma ~ uniform(6.0 / 100.0, 6.0 * 100.0);
  nuMinusOne ~ exponential(1/29.0);
}</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rstan)
galton_stanfit &lt;-
<span class="st">  </span><span class="kw">sampling</span>(
    galton_stan,
    <span class="dt">data =</span> <span class="kw">list</span>(
      <span class="dt">N =</span> <span class="kw">nrow</span>(GaltonW),
      <span class="dt">x =</span> GaltonW<span class="op">$</span>midparent,
      <span class="dt">y =</span> GaltonW<span class="op">$</span>height
    ),
    <span class="dt">chains =</span> <span class="dv">4</span>,
    <span class="dt">iter =</span> <span class="dv">2000</span>,
    <span class="dt">warmup =</span> <span class="dv">1000</span>
  )  </code></pre>
<pre class="sourceCode r"><code class="sourceCode r">galton_stanfit</code></pre>
<pre><code>## Inference for Stan model: d35148cb031c8088132ce1e0ec66e1bb.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##               mean se_mean    sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
## beta0        14.73    0.16  5.59    3.98   11.00   14.71   18.42   25.49  1254    1
## beta1         0.74    0.00  0.08    0.58    0.68    0.74    0.80    0.90  1252    1
## sigma         1.84    0.00  0.13    1.59    1.75    1.84    1.92    2.09  2018    1
## nuMinusOne   31.28    0.57 26.91    5.12   12.76   22.60   41.15  106.34  2230    1
## nu           32.28    0.57 26.91    6.12   13.76   23.60   42.15  107.34  2230    1
## lp__       -250.05    0.04  1.47 -253.84 -250.71 -249.70 -248.98 -248.30  1351    1
## 
## Samples were drawn using NUTS(diag_e) at Wed Mar 27 10:03:01 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_point</span>(beta1 <span class="op">~</span><span class="st"> </span>beta0, <span class="dt">data =</span> <span class="kw">posterior</span>(galton_stanfit), <span class="dt">alpha =</span> <span class="fl">0.5</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch17-galton-stan-look-1.png" width="576" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_combo</span>(<span class="kw">as.mcmc.list</span>(galton_stanfit), 
           <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;beta0&quot;</span>, <span class="st">&quot;beta1&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;nu&quot;</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch17-galton-stan-combo-1.png" width="576" /></p>
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">17.6</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Use Galton’s data on the men to estimate</p>
<ol style="list-style-type: lower-alpha">
<li>The average of height of men whose parents are 65 and 72 inches tall.</li>
<li>The middle 50% of heights of men whose parents are 65 and 72 inches tall.</li>
</ol>
<p>You may use either JAGS or Stan.</p></li>
<li><p>When centering, why did we center x but not y?</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="estimating-one-and-two-means.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Redoing.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
