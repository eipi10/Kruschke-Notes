<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>7 Markov Chain Monte Carlo (MCMC) | (Re)Doing Bayesain Data Analysis</title>
  <meta name="description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="7 Markov Chain Monte Carlo (MCMC) | (Re)Doing Bayesain Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Markov Chain Monte Carlo (MCMC) | (Re)Doing Bayesain Data Analysis" />
  
  <meta name="twitter:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

<meta name="author" content="R Pruim">


<meta name="date" content="2019-02-22">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html">
<link rel="next" href="jags-just-another-gibbs-sampler.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">(Re)Doing Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What’s in These Notes</a></li>
<li class="part"><span><b>I The Basics: Models, Probability, Bayes, and R</b></span></li>
<li class="chapter" data-level="2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html"><i class="fa fa-check"></i><b>2</b> Credibility, Models, and Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#the-steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.1</b> The Steps of Bayesian Data Analysis</a><ul>
<li class="chapter" data-level="2.1.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-code"><i class="fa fa-check"></i><b>2.1.1</b> R code</a></li>
<li class="chapter" data-level="2.1.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-packages"><i class="fa fa-check"></i><b>2.1.2</b> R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-1-which-coin-is-it"><i class="fa fa-check"></i><b>2.2</b> Example 1: Which coin is it?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#freedom-of-choice"><i class="fa fa-check"></i><b>2.2.1</b> Freedom of choice</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#distributions"><i class="fa fa-check"></i><b>2.3</b> Distributions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#beta-distributions"><i class="fa fa-check"></i><b>2.3.1</b> Beta distributions</a></li>
<li class="chapter" data-level="2.3.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#normal-distributions"><i class="fa fa-check"></i><b>2.3.2</b> Normal distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-2-height-vs-weight"><i class="fa fa-check"></i><b>2.4</b> Example 2: Height vs Weight</a><ul>
<li class="chapter" data-level="2.4.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#data"><i class="fa fa-check"></i><b>2.4.1</b> Data</a></li>
<li class="chapter" data-level="2.4.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#describing-a-model-for-the-relationship-between-height-and-weight"><i class="fa fa-check"></i><b>2.4.2</b> Describing a model for the relationship between height and weight</a></li>
<li class="chapter" data-level="2.4.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#prior"><i class="fa fa-check"></i><b>2.4.3</b> Prior</a></li>
<li class="chapter" data-level="2.4.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior"><i class="fa fa-check"></i><b>2.4.4</b> Posterior</a></li>
<li class="chapter" data-level="2.4.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior-predictive-check"><i class="fa fa-check"></i><b>2.4.5</b> Posterior Predictive Check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#where-do-we-go-from-here"><i class="fa fa-check"></i><b>2.5</b> Where do we go from here?</a></li>
<li class="chapter" data-level="2.6" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#ch02-exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#footnotes"><i class="fa fa-check"></i><b>2.7</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html"><i class="fa fa-check"></i><b>3</b> Some Useful Bits of R</a><ul>
<li class="chapter" data-level="3.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#style-guide"><i class="fa fa-check"></i><b>3.1</b> You Gotta Have Style</a><ul>
<li class="chapter" data-level="3.1.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#an-additional-note-about-homework"><i class="fa fa-check"></i><b>3.1.1</b> An additional note about homework</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors-lists-and-data-frames"><i class="fa fa-check"></i><b>3.2</b> Vectors, Lists, and Data Frames</a><ul>
<li class="chapter" data-level="3.2.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors"><i class="fa fa-check"></i><b>3.2.1</b> Vectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#lists"><i class="fa fa-check"></i><b>3.2.2</b> Lists</a></li>
<li class="chapter" data-level="3.2.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#data-frames-for-rectangular-data"><i class="fa fa-check"></i><b>3.2.3</b> Data frames for rectangular data</a></li>
<li class="chapter" data-level="3.2.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#other-types-of-data"><i class="fa fa-check"></i><b>3.2.4</b> Other types of data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#plotting-with-ggformula"><i class="fa fa-check"></i><b>3.3</b> Plotting with ggformula</a></li>
<li class="chapter" data-level="3.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#creating-data-with-expand.grid"><i class="fa fa-check"></i><b>3.4</b> Creating data with expand.grid()</a></li>
<li class="chapter" data-level="3.5" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#transforming-and-summarizing-data-dplyr-and-tidyr"><i class="fa fa-check"></i><b>3.5</b> Transforming and summarizing data dplyr and tidyr</a></li>
<li class="chapter" data-level="3.6" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#writing-functions"><i class="fa fa-check"></i><b>3.6</b> Writing Functions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#why-write-functions"><i class="fa fa-check"></i><b>3.6.1</b> Why write functions?</a></li>
<li class="chapter" data-level="3.6.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#function-parts"><i class="fa fa-check"></i><b>3.6.2</b> Function parts</a></li>
<li class="chapter" data-level="3.6.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#the-function-function-has-its-function"><i class="fa fa-check"></i><b>3.6.3</b> The function() function has its function</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#some-common-error-messages"><i class="fa fa-check"></i><b>3.7</b> Some common error messages</a><ul>
<li class="chapter" data-level="3.7.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#object-not-found"><i class="fa fa-check"></i><b>3.7.1</b> object not found</a></li>
<li class="chapter" data-level="3.7.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#any-message-mentioning-yaml"><i class="fa fa-check"></i><b>3.7.2</b> Any message mentioning yaml</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#ch03-exercises"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#footnotes-1"><i class="fa fa-check"></i><b>3.9</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#some-terminology"><i class="fa fa-check"></i><b>4.1</b> Some terminology</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>4.2</b> Distributions in R</a><ul>
<li class="chapter" data-level="4.2.1" data-path="probability.html"><a href="probability.html#example-normal-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Example: Normal distributions</a></li>
<li class="chapter" data-level="4.2.2" data-path="probability.html"><a href="probability.html#simulating-running-proportions"><i class="fa fa-check"></i><b>4.2.2</b> Simulating running proportions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#joint-marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>4.3</b> Joint, marginal, and conditional distributions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="probability.html"><a href="probability.html#example-hair-and-eye-color"><i class="fa fa-check"></i><b>4.3.1</b> Example: Hair and eye color</a></li>
<li class="chapter" data-level="4.3.2" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>4.3.2</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#ch04-exercises"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#footnotes-2"><i class="fa fa-check"></i><b>4.5</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html"><i class="fa fa-check"></i><b>5</b> Bayes’ Rule and the Grid Method</a><ul>
<li class="chapter" data-level="5.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#the-big-bayesian-idea"><i class="fa fa-check"></i><b>5.1</b> The Big Bayesian Idea</a><ul>
<li class="chapter" data-level="5.1.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#likelihood"><i class="fa fa-check"></i><b>5.1.1</b> Likelihood</a></li>
<li class="chapter" data-level="5.1.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#when-bayes-is-easy"><i class="fa fa-check"></i><b>5.1.2</b> When Bayes is easy</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#estimating-the-bias-in-a-coin-using-the-grid-method"><i class="fa fa-check"></i><b>5.2</b> Estimating the bias in a coin using the Grid Method</a><ul>
<li class="chapter" data-level="5.2.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#creating-a-grid"><i class="fa fa-check"></i><b>5.2.1</b> Creating a Grid</a></li>
<li class="chapter" data-level="5.2.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#hdi-from-the-grid"><i class="fa fa-check"></i><b>5.2.2</b> HDI from the grid</a></li>
<li class="chapter" data-level="5.2.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#automating-the-grid"><i class="fa fa-check"></i><b>5.2.3</b> Automating the grid</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#working-on-the-log-scale"><i class="fa fa-check"></i><b>5.3</b> Working on the log scale</a></li>
<li class="chapter" data-level="5.4" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#discrete-params"><i class="fa fa-check"></i><b>5.4</b> Discrete Parameters</a></li>
<li class="chapter" data-level="5.5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#ch05-exercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
<li class="chapter" data-level="5.6" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#footnotes-3"><i class="fa fa-check"></i><b>5.6</b> Footnotes</a></li>
</ul></li>
<li class="part"><span><b>II Inferring a Binomial Probability</b></span></li>
<li class="chapter" data-level="6" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><i class="fa fa-check"></i><b>6</b> Inferring a Binomial Probability via Exact Mathematical Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-distributions-1"><i class="fa fa-check"></i><b>6.1</b> Beta distributions</a></li>
<li class="chapter" data-level="6.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-and-bayes"><i class="fa fa-check"></i><b>6.2</b> Beta and Bayes</a><ul>
<li class="chapter" data-level="6.2.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#the-bernoulli-likelihood-function"><i class="fa fa-check"></i><b>6.2.1</b> The Bernoulli likelihood function</a></li>
<li class="chapter" data-level="6.2.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#a-convenient-prior"><i class="fa fa-check"></i><b>6.2.2</b> A convenient prior</a></li>
<li class="chapter" data-level="6.2.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#pros-and-cons-of-conjugate-priors"><i class="fa fa-check"></i><b>6.2.3</b> Pros and Cons of conjugate priors</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#getting-to-know-the-beta-distributions"><i class="fa fa-check"></i><b>6.3</b> Getting to know the Beta distributions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#important-facts"><i class="fa fa-check"></i><b>6.3.1</b> Important facts</a></li>
<li class="chapter" data-level="6.3.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#alternative-parameterizations-of-beta-distributions"><i class="fa fa-check"></i><b>6.3.2</b> Alternative parameterizations of Beta distributions</a></li>
<li class="chapter" data-level="6.3.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta_params"><i class="fa fa-check"></i><b>6.3.3</b> beta_params()</a></li>
<li class="chapter" data-level="6.3.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#automating-bayesian-updates-for-a-proportion-beta-prior"><i class="fa fa-check"></i><b>6.3.4</b> Automating Bayesian updates for a proportion (beta prior)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#what-if-the-prior-isnt-a-beta-distribution"><i class="fa fa-check"></i><b>6.4</b> What if the prior isn’t a beta distribution?</a></li>
<li class="chapter" data-level="6.5" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#ch06-exercises"><i class="fa fa-check"></i><b>6.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html"><i class="fa fa-check"></i><b>7</b> Markov Chain Monte Carlo (MCMC)</a><ul>
<li class="chapter" data-level="7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#king-markov-and-adviser-metropolis"><i class="fa fa-check"></i><b>7.1</b> King Markov and Adviser Metropolis</a></li>
<li class="chapter" data-level="7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#quick-intro-to-markov-chains"><i class="fa fa-check"></i><b>7.2</b> Quick Intro to Markov Chains</a><ul>
<li class="chapter" data-level="7.2.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#more-info-please"><i class="fa fa-check"></i><b>7.2.1</b> More info, please</a></li>
<li class="chapter" data-level="7.2.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#definition"><i class="fa fa-check"></i><b>7.2.2</b> Definition</a></li>
<li class="chapter" data-level="7.2.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#time-homogeneous-markov-chains"><i class="fa fa-check"></i><b>7.2.3</b> Time-Homogeneous Markov Chains</a></li>
<li class="chapter" data-level="7.2.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#matrix-representation"><i class="fa fa-check"></i><b>7.2.4</b> Matrix representation</a></li>
<li class="chapter" data-level="7.2.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#regular-markov-chains"><i class="fa fa-check"></i><b>7.2.5</b> Regular Markov Chains</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#back-to-king-markov"><i class="fa fa-check"></i><b>7.3</b> Back to King Markov</a></li>
<li class="chapter" data-level="7.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#how-well-does-the-metropolis-algorithm-work"><i class="fa fa-check"></i><b>7.4</b> How well does the Metropolis Algorithm work?</a><ul>
<li class="chapter" data-level="7.4.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-to-any-island"><i class="fa fa-check"></i><b>7.4.1</b> Jumping to any island</a></li>
<li class="chapter" data-level="7.4.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-only-to-neighbor-islands"><i class="fa fa-check"></i><b>7.4.2</b> Jumping only to neighbor islands</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#markov-chains-and-posterior-sampling"><i class="fa fa-check"></i><b>7.5</b> Markov Chains and Posterior Sampling</a><ul>
<li class="chapter" data-level="7.5.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#example-1-estimating-a-proportion"><i class="fa fa-check"></i><b>7.5.1</b> Example 1: Estimating a proportion</a></li>
<li class="chapter" data-level="7.5.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#example-2-estimating-mean-and-variance"><i class="fa fa-check"></i><b>7.5.2</b> Example 2: Estimating mean and variance</a></li>
<li class="chapter" data-level="7.5.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#issues-with-metropolis-algorithm"><i class="fa fa-check"></i><b>7.5.3</b> Issues with Metropolis Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#two-coins"><i class="fa fa-check"></i><b>7.6</b> Two coins</a><ul>
<li class="chapter" data-level="7.6.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#the-model"><i class="fa fa-check"></i><b>7.6.1</b> The model</a></li>
<li class="chapter" data-level="7.6.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#exact-analysis"><i class="fa fa-check"></i><b>7.6.2</b> Exact analysis</a></li>
<li class="chapter" data-level="7.6.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#metropolis"><i class="fa fa-check"></i><b>7.6.3</b> Metropolis</a></li>
<li class="chapter" data-level="7.6.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#gibbs-sampling"><i class="fa fa-check"></i><b>7.6.4</b> Gibbs sampling</a></li>
<li class="chapter" data-level="7.6.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#advantages-and-disadvantages-of-gibbs-vs-metropolis"><i class="fa fa-check"></i><b>7.6.5</b> Advantages and Disadvantages of Gibbs vs Metropolis</a></li>
<li class="chapter" data-level="7.6.6" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#so-what-do-we-learn-about-the-coins"><i class="fa fa-check"></i><b>7.6.6</b> So what do we learn about the coins?</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#mcmc-posterior-sampling-big-picture"><i class="fa fa-check"></i><b>7.7</b> MCMC posterior sampling: Big picture</a><ul>
<li class="chapter" data-level="7.7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#mcmc-markov-chain-monte-carlo"><i class="fa fa-check"></i><b>7.7.1</b> MCMC = Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="7.7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#posterior-sampling-random-walk-through-the-posterior"><i class="fa fa-check"></i><b>7.7.2</b> Posterior sampling: Random walk through the posterior</a></li>
<li class="chapter" data-level="7.7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#where-do-we-go-from-here-1"><i class="fa fa-check"></i><b>7.7.3</b> Where do we go from here?</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#ch07-exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html"><i class="fa fa-check"></i><b>8</b> JAGS – Just Another Gibbs Sampler</a><ul>
<li class="chapter" data-level="8.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#what-jags-is"><i class="fa fa-check"></i><b>8.1</b> What JAGS is</a><ul>
<li class="chapter" data-level="8.1.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#updating-c-and-clang"><i class="fa fa-check"></i><b>8.1.1</b> Updating C and CLANG</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-1-estimating-a-proportion-1"><i class="fa fa-check"></i><b>8.2</b> Example 1: estimating a proportion</a><ul>
<li class="chapter" data-level="8.2.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-1"><i class="fa fa-check"></i><b>8.2.1</b> The Model</a></li>
<li class="chapter" data-level="8.2.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#load-data"><i class="fa fa-check"></i><b>8.2.2</b> Load Data</a></li>
<li class="chapter" data-level="8.2.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#specify-the-model"><i class="fa fa-check"></i><b>8.2.3</b> Specify the model</a></li>
<li class="chapter" data-level="8.2.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#run-the-model"><i class="fa fa-check"></i><b>8.2.4</b> Run the model</a></li>
<li class="chapter" data-level="8.2.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-coda"><i class="fa fa-check"></i><b>8.2.5</b> Using coda</a></li>
<li class="chapter" data-level="8.2.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-bayesplot"><i class="fa fa-check"></i><b>8.2.6</b> Using bayesplot</a></li>
<li class="chapter" data-level="8.2.7" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-kruschkes-functions"><i class="fa fa-check"></i><b>8.2.7</b> Using Kruschke’s functions</a></li>
<li class="chapter" data-level="8.2.8" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#optional-arguments-to-jags"><i class="fa fa-check"></i><b>8.2.8</b> Optional arguments to jags()</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-2-comparing-two-proportions"><i class="fa fa-check"></i><b>8.3</b> Example 2: comparing two proportions</a><ul>
<li class="chapter" data-level="8.3.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-data"><i class="fa fa-check"></i><b>8.3.1</b> The data</a></li>
<li class="chapter" data-level="8.3.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-2"><i class="fa fa-check"></i><b>8.3.2</b> The model</a></li>
<li class="chapter" data-level="8.3.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#describing-the-model-to-jags"><i class="fa fa-check"></i><b>8.3.3</b> Describing the model to JAGS</a></li>
<li class="chapter" data-level="8.3.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#fitting-the-model"><i class="fa fa-check"></i><b>8.3.4</b> Fitting the model</a></li>
<li class="chapter" data-level="8.3.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#inspecting-the-results"><i class="fa fa-check"></i><b>8.3.5</b> Inspecting the results</a></li>
<li class="chapter" data-level="8.3.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#difference-in-proportions"><i class="fa fa-check"></i><b>8.3.6</b> Difference in proportions</a></li>
<li class="chapter" data-level="8.3.7" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#sampling-from-the-prior"><i class="fa fa-check"></i><b>8.3.7</b> Sampling from the prior</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#ch08-exercises"><i class="fa fa-check"></i><b>8.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="heierarchical-models.html"><a href="heierarchical-models.html"><i class="fa fa-check"></i><b>9</b> Heierarchical Models</a><ul>
<li class="chapter" data-level="9.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#one-coin-from-one-mint"><i class="fa fa-check"></i><b>9.1</b> One coin from one mint</a></li>
<li class="chapter" data-level="9.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-one-mint"><i class="fa fa-check"></i><b>9.2</b> Multiple coins from one mint</a></li>
<li class="chapter" data-level="9.3" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-multiple-mints"><i class="fa fa-check"></i><b>9.3</b> Multiple coins from multiple mints</a></li>
<li class="chapter" data-level="9.4" data-path="heierarchical-models.html"><a href="heierarchical-models.html#ch09-exercises"><i class="fa fa-check"></i><b>9.4</b> Exerciess</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">(Re)Doing Bayesain Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="markov-chain-monte-carlo-mcmc" class="section level1">
<h1><span class="header-section-number">7</span> Markov Chain Monte Carlo (MCMC)</h1>
<div id="king-markov-and-adviser-metropolis" class="section level2">
<h2><span class="header-section-number">7.1</span> King Markov and Adviser Metropolis</h2>
<p>King Markov is king of a chain of 5 islands. Rather than live in a palace, he
lives in a royal boat. Each night the royal boat anchors in the harbor of one
of the islands. The law declares that <strong>the king must harbor at each island in
proportion to the population of the island</strong>.</p>
<p><strong>Question 1:</strong> If the populations of the islands are 100, 200, 300, 400, and
500 people, how often must King Markov harbor at each island?</p>
<p>King Markov has some personality quirks:</p>
<ul>
<li><p>He can’t stand record keeping. So he doesn’t know the populations on his islands
and doesn’t keep track of which islands he has visited when.</p></li>
<li><p>He can’t stand routine (variety is the spice of his life),
so he doesn’t want to know each night where he will be the next night.</p></li>
</ul>
<p>He asks Adviser Metropolis to devise a way for him to obey the law but that</p>
<ul>
<li>randomly picks which island to stay at each night,</li>
<li>doesn’t require him to remember where he has been in the past, and</li>
<li>doesn’t require him to remember the populations of all the islands.</li>
</ul>
<p>He can ask the clerk on any island what the island’s population is whenever he needs to know.
But it takes half a day to sail from one island to another, so he is limited in
how much information he can obtain this way each day.</p>
<p>Metropolis devises the following scheme:</p>
<ul>
<li><p>Each morning, have breakfast with the island clerk and inquire about the
population of the current island.</p></li>
<li><p>Then randomly pick one of the 4 other islands (a proposal island) and travel
there in the morning</p>
<ul>
<li><p>Let <span class="math inline">\(J(b \mid a)\)</span> be the conditional probability of selecting island <span class="math inline">\(b\)</span> as the
candidate if <span class="math inline">\(a\)</span> is the current island.</p></li>
<li><p><span class="math inline">\(J\)</span> does not depend on the populations of the islands (since the King can’t
remember them).</p></li>
</ul></li>
<li><p>Over lunch at the proposal island, inquire about its population.</p>
<ul>
<li><p>If the proposal island has more people, stay at the proposal island for the night
(since the king should prefer more populated islands).</p></li>
<li><p>If the proposal island has fewer people, stay at the proposal island with
probability <span class="math inline">\(A\)</span>, else return to the “current” island (ie, last night’s island).</p></li>
</ul></li>
</ul>
<p>Metropolis is convinced that for the right choices of <span class="math inline">\(J\)</span> and <span class="math inline">\(A\)</span>, this will satisfy the law.</p>
<p>He quickly determines that <span class="math inline">\(A\)</span> cannot be 0 and cannot be 1:</p>
<p><strong>Question 2.</strong> What happens if <span class="math inline">\(A = 0\)</span>? What happens if <span class="math inline">\(A = 1\)</span>?</p>
<p>It seems like <span class="math inline">\(A\)</span> might need to depend on the populations of the
current and proposal islands. When we want to emphasize that, we’ll
denote it as <span class="math inline">\(A = A(b \mid a)\)</span>.
But how? If <span class="math inline">\(A\)</span> is too large, the king will visit small islands too often. If
<span class="math inline">\(A\)</span> is too small, he will visit large islands too often.</p>
<p>Fortunately, Metropolis knows about Markov Chains. Unfortunately, some of you
may not. So let’s learn a little bit about Markov Chains and then figure out how
Metropolis should choose <span class="math inline">\(J\)</span> and <span class="math inline">\(A\)</span>.</p>
</div>
<div id="quick-intro-to-markov-chains" class="section level2">
<h2><span class="header-section-number">7.2</span> Quick Intro to Markov Chains</h2>
<div id="more-info-please" class="section level3">
<h3><span class="header-section-number">7.2.1</span> More info, please</h3>
<p>This is going to be very quick.
You can learn more, if you are interested, by going to</p>
<ul>
<li><a href="https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf">https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf</a></li>
</ul>
</div>
<div id="definition" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Definition</h3>
<p>Consider a random process that proceeds in discrete steps (often referred to as time).
Let <span class="math inline">\(X_t\)</span> represent the “state” of the process at time <span class="math inline">\(t\)</span>. Since this is a
random process, <span class="math inline">\(X_t\)</span> is random, and we can ask probability questions like
``What is the probability of being in state ____ at time ____?&quot;, ie, What is <span class="math inline">\(\mathrm{Pr}(X_t = x)\)</span>?</p>
<p>If
<span class="math display">\[
\mathrm{Pr}(X_{t+1} = x \mid X_t = x_t, X_{t-1} = x_{t-1}, \dots , X_0 = x_0) = \mathrm{Pr}(X_{t+1} \mid X_{t} = x_t)
\]</span>
then we say that the process is a <strong>Markov Chain</strong>. The intuition is that
(the probabilities of) what happens next depends only on the current state and not on
previous history.</p>
</div>
<div id="time-homogeneous-markov-chains" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Time-Homogeneous Markov Chains</h3>
<p>The simplest version of a Markov Chain is one that is <strong>time-homogeneous</strong>:</p>
<p><span class="math display">\[
\mathrm{Pr}(X_{t+1} = b \mid X_t = a) = p_{ab}
\]</span>
That is, the (conditional) probability of moving from state
<span class="math inline">\(a\)</span> to state <span class="math inline">\(b\)</span> in one step is the same at every time.</p>
</div>
<div id="matrix-representation" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Matrix representation</h3>
<p>A time-homogeneous Markov Chain can be represented by a square matrix <span class="math inline">\(M\)</span> with</p>
<p><span class="math display">\[
M_{ij} = p_{ij} = \mbox{probability of transition from state $i$ to state $j$ in one step}
\]</span>
(This will be an infinite matrix if the state space in infinite,
but we’ll start with simple examples with small, finite state spaces.)
<span class="math inline">\(M_{ij}\)</span> is the probability of moving in one step from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>.<br />
More generally, we will write <span class="math inline">\(M^{(k)}_{ij}\)</span> for the probability of moving
from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in <span class="math inline">\(k\)</span> steps.</p>
<p><strong>Small Example:</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">M &lt;-<span class="st"> </span><span class="kw">rbind</span>( <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>), <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>))
M</code></pre>
<table>
<tbody>
<tr class="odd">
<td align="right">0.00</td>
<td align="right">0.50</td>
<td align="right">0.5</td>
</tr>
<tr class="even">
<td align="right">0.25</td>
<td align="right">0.25</td>
<td align="right">0.5</td>
</tr>
<tr class="odd">
<td align="right">0.50</td>
<td align="right">0.30</td>
<td align="right">0.2</td>
</tr>
</tbody>
</table>
<p><strong>Question 3:</strong></p>
<ul>
<li><p>How many states does this process have?</p></li>
<li><p>What is the probability of moving from state 1 to state 3 in 1 step?</p></li>
<li><p>What is the probability of moving from state 1 to state 3 in 2 steps? (Hint:
what are the possible stopping points along the way?)</p></li>
<li><p>How do we obtain <span class="math inline">\(M^{(2)}\)</span> from <span class="math inline">\(M\)</span>?</p></li>
<li><p>How do we obtain <span class="math inline">\(M^{(k)}\)</span> from <span class="math inline">\(M\)</span>?</p></li>
</ul>
<p><strong>Question 4:</strong> The Metropolis Algorithm as a Markov process</p>
<ul>
<li><p>What are the states of the Metropolis algorithm?</p></li>
<li><p>If King Markov is on island 2, what is the probability of moving to Island 3?</p></li>
<li><p>If King Markov is on island 3, what is the probability of moving to Island 2?</p></li>
<li><p>What is the general formula for the probability of moving from island
<span class="math inline">\(a\)</span> to island <span class="math inline">\(b\)</span> (in one step)? (<span class="math inline">\(\mathrm{Pr}(X_{t+1}=b \mid X_t = a)\)</span>)</p></li>
</ul>
</div>
<div id="regular-markov-chains" class="section level3">
<h3><span class="header-section-number">7.2.5</span> Regular Markov Chains</h3>
<p>A time-homogeneous Markov Chain, is called <strong>regular</strong> if there is a number <span class="math inline">\(k\)</span> such that</p>
<ul>
<li>every state is reachable from every other state with non-zero probability in <span class="math inline">\(k\)</span>
steps</li>
</ul>
<p><strong>Question 5a</strong></p>
<ul>
<li>Is our small example regular? If so, how many steps are required?</li>
</ul>
<p><strong>Question 5b</strong></p>
<ul>
<li>Under what conditions is the Metropolis algorithm regular?</li>
</ul>
<p>Regular Markov Chains have a very nice property:</p>
<p><span class="math display">\[
\lim_{k \to \infty} M^{(k)} = W
\]</span>
where every row of <span class="math inline">\(W\)</span> is the same. This says that, no matter where
you start the process, the long-run probability of being in each state
will be the same.</p>
<p>In our small example above, convergence is quite rapid:</p>
<pre class="sourceCode r"><code class="sourceCode r">M <span class="op">%^%</span><span class="st"> </span><span class="dv">20</span></code></pre>
<table>
<tbody>
<tr class="odd">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
<tr class="even">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
<tr class="odd">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r">M <span class="op">%^%</span><span class="st"> </span><span class="dv">21</span></code></pre>
<table>
<tbody>
<tr class="odd">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
<tr class="even">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
<tr class="odd">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
</tbody>
</table>
<p>Note: If we apply the matrix <span class="math inline">\(M\)</span> to the limiting probability
(<span class="math inline">\(w\)</span>, one row of <span class="math inline">\(W\)</span>), we just get <span class="math inline">\(w\)</span> back again:</p>
<p><span class="math display">\[w M = w\]</span></p>
<pre class="sourceCode r"><code class="sourceCode r">W &lt;-<span class="st"> </span>M <span class="op">%^%</span><span class="st"> </span><span class="dv">30</span>
W[<span class="dv">1</span>,]</code></pre>
<pre><code>## [1] 0.2769 0.3385 0.3846</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">W[<span class="dv">1</span>,] <span class="op">%*%</span><span class="st"> </span>M</code></pre>
<table>
<tbody>
<tr class="odd">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
</tbody>
</table>
<p>In fact, this is a necessary and sufficient condition for the limiting probability.</p>
<p>So, here’s what Metropolis needs to do: Choose <span class="math inline">\(J\)</span> and <span class="math inline">\(A\)</span> so that</p>
<ul>
<li>his algorithm is a regular Markov Chain with matrix <span class="math inline">\(M\)</span></li>
<li>If <span class="math inline">\(w = \langle p(1), p(2), p(3), p(4), p(5) \rangle\)</span> is the law-prescribed probabilities for island harboring, then <span class="math inline">\(w M = w\)</span>.</li>
</ul>
</div>
</div>
<div id="back-to-king-markov" class="section level2">
<h2><span class="header-section-number">7.3</span> Back to King Markov</h2>
<p>If <span class="math inline">\(A\)</span> is between 0 and 1, and the jumping rule allows us to get to all the
islands (eventually), then the Markov Chain will be regular, so there will be a
limiting distribution. But the limiting distribution must be the one the law
requires. It suffices to show that if the law is satisfied at time <span class="math inline">\(t\)</span> it is
satisfied at time <span class="math inline">\(t+1\)</span> (<span class="math inline">\(wM = w\)</span>):</p>
<p><span class="math display">\[ 
\mathrm{Pr}(X_t = a) = p(a) \mbox{ for all $a$ } \Rightarrow \mathrm{Pr}(X_{t+1} = a) = p(a) \mbox{ for all $a$}
\]</span></p>
<p>Here’s the trick: We will choose <span class="math inline">\(J\)</span> and <span class="math inline">\(A\)</span> so that
the following two <strong>unconditional</strong> probabilities are equal.</p>
<p><span class="math display">\[ \mathrm{Pr}(a \to_t b) = \mathrm{Pr}(b \to_t a) \]</span>
where <span class="math inline">\(\mathrm{Pr}(a \to_t b) = \mathrm{Pr}(X_t = a \ \&amp; \ X_{t+1} = b)\)</span>.</p>
<p>Why does this work?</p>
<ul>
<li><p>Suppose <span class="math inline">\(\mathrm{Pr}(X_t = a) = p(a)\)</span> as the law prescribes.</p></li>
<li><p><span class="math inline">\(\mathrm{Pr}(a \to_t b) = \mathrm{Pr}(b \to_t a)\)</span> makes the joint distribution symmetric:
For any <span class="math inline">\(a\)</span> and any <span class="math inline">\(b\)</span>.</p></li>
</ul>
<p><span class="math display">\[\mathrm{Pr}(X_{t} = a \ \&amp; \ X_{t+1} = b) = \mathrm{Pr}(X_{t} = b \ \&amp; \  X_{t+1} = a)\]</span></p>
<ul>
<li>This means that both marginals are the same, so for any <span class="math inline">\(a\)</span>:</li>
</ul>
<p><span class="math display">\[\mathrm{Pr}(X_t = a) = \mathrm{Pr}(X_{t+1} = a)\]</span></p>
<ul>
<li>In other words, the probability of the current island will be the same as the probability
of the next island: <span class="math inline">\(w M = w\)</span>.</li>
</ul>
<p>Time for some algebra (and probability)! How do we choose <span class="math inline">\(J\)</span> and <span class="math inline">\(A\)</span>? Recall the ingredients:</p>
<ul>
<li><span class="math inline">\(P(a)\)</span> be the population of island <span class="math inline">\(a\)</span></li>
<li><span class="math inline">\(p(a)\)</span> be the proportion of the total population living on island <span class="math inline">\(a\)</span>: <span class="math inline">\(p(a) = \frac{p(a)}{\sum_x p(x)}\)</span></li>
<li><span class="math inline">\(J(b \mid a)\)</span> is the conditional probability of selecting island <span class="math inline">\(b\)</span> as the candidate
when <span class="math inline">\(a\)</span> is the current island. (J for Jump probability)</li>
<li><span class="math inline">\(A(b \mid a)\)</span> is the probability of accepting proposal island <span class="math inline">\(b\)</span> if
it is proposed from island <span class="math inline">\(a\)</span>.</li>
</ul>
<p><strong>Question 6:</strong> Consider two islands – <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> – with <span class="math inline">\(P(b) &gt; P(a)\)</span>.
Assume that probability of being on island <span class="math inline">\(x\)</span> is <span class="math inline">\(p(x)\)</span>.
Calculate the following probabilities (in terms of things like <span class="math inline">\(p\)</span>, <span class="math inline">\(J\)</span>,
and <span class="math inline">\(A\)</span>).</p>
<ul>
<li>(Unconditional) probability of moving from <span class="math inline">\(a\)</span> to <span class="math inline">\(b = \mathrm{Pr}(a \to_t b) =\)</span></li>
</ul>
<!-- $p(a) J(b \mid a) \cdot 1$ -->
<ul>
<li>(Unconditional) probability of moving from <span class="math inline">\(b\)</span> to <span class="math inline">\(a = \mathrm{Pr}(b \to_t a) =\)</span></li>
</ul>
<p><!-- $p(b) J(a \mid b) \cdot A(a \mid b)$. --></p>
<p><strong>Question 7:</strong> How do we choose J and A to make these probabilities equal?</p>
<p><span class="math display">\[ 
A(a \mid b) = 
\phantom{\frac{p(a) J(b \mid a)}{p(b) J(a \mid b)}}
\]</span></p>
<p><strong>Question 8:</strong> Symmetric jump rules.</p>
<ul>
<li><p>Is it possible to do this with symmetric jump rules? That is,
can we require <span class="math inline">\(J(b \mid a) = J(a \mid b)\)</span>?
(Remember, the king doesn’t like to remember stuff,
and this means half as much stuff to remember about the jump rules).</p></li>
<li><p>Does using a symmetric jump rule make the acceptance rule <span class="math inline">\(A\)</span> any
simpler or more complicated? (The king won’t be so happy if
the simpler jump rule makes the acceptance rule a lot more complicated.)</p></li>
</ul>
<!-- Answer:  -->
<!-- We need to choose a jump rule ($J()$) so that the Markov chain is regular (ie, so that -->
<!-- is it is possible to get from any island to any island in some fixed number of steps). -->
<!-- Given such a $J()$, we can let $A$ be defined by -->
<!-- $$ -->
<!-- A = \frac{p(a) J(b \mid a)}{p(b) J(a \mid b)} -->
<!-- $$ -->
<!-- An especially easy case is when $J(b \mid a) = J(a \mid b)$.  In that case -->
<!-- $$ -->
<!-- A = \frac{p(a)}{p(b)} -->
<!-- $$ -->
<p><strong>Question 9:</strong> Constant jump rules.</p>
<ul>
<li><p>Is it possible to do this if we require that
<span class="math inline">\(J(y \mid x) = J(y&#39; \mid x&#39;)\)</span> for all <span class="math inline">\(y \neq x\)</span> and <span class="math inline">\(y&#39; \neq x&#39;\)</span>?
(This would make life even easier for the king.)</p></li>
<li><p>For King Markov, what would <span class="math inline">\(J\)</span> be if we did it this way?</p></li>
</ul>
<!-- Answer: $J(y \mid x) = 1/4$ whenever $x \neq y$. -->
<p>The original Metropolis algorithm used symmetric jump rules. The later generalization
(Metropolis-Hastings) employed non-symmetric jump rules to get better performance
of the Markov Chain.</p>
</div>
<div id="how-well-does-the-metropolis-algorithm-work" class="section level2">
<h2><span class="header-section-number">7.4</span> How well does the Metropolis Algorithm work?</h2>
<p>Let’s let the computer simulate this algorithm. And since the computer
is doing all the work, let’s make a general function so we can
experiment a bit.</p>
<pre class="sourceCode r"><code class="sourceCode r">KingMarkov &lt;-<span class="st"> </span><span class="cf">function</span>(
  <span class="dt">num_steps    =</span> <span class="fl">1e5</span>,
  <span class="dt">population   =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,
  <span class="dt">island_names =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(population),
  <span class="dt">start        =</span> <span class="dv">1</span>,
  <span class="dt">J =</span> <span class="cf">function</span>(a, b) {<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="kw">length</span>(population) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)}
  ) {
  
  num_islands  &lt;-<span class="st"> </span><span class="kw">length</span>(population)
  island_seq   &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  proposal_seq &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  current      &lt;-<span class="st"> </span>start
  proposal     &lt;-<span class="st"> </span><span class="ot">NA</span>
  
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>num_steps) {
    <span class="co"># record current island</span>
    island_seq[i]   &lt;-<span class="st"> </span>current
    proposal_seq[i] &lt;-<span class="st"> </span>proposal
    
    <span class="co"># propose one of the other islands</span>
    other_islands &lt;-<span class="st"> </span><span class="kw">setdiff</span>(<span class="dv">1</span><span class="op">:</span>num_islands, current)
    proposal &lt;-<span class="st"> </span>
<span class="st">      </span><span class="kw">sample</span>(other_islands, <span class="dv">1</span>, 
             <span class="dt">prob =</span> purrr<span class="op">::</span><span class="kw">map</span>(other_islands, <span class="op">~</span><span class="st"> </span><span class="kw">J</span>(current, .x)))
    <span class="co"># move?</span>
    prob_move &lt;-<span class="st"> </span>population[proposal] <span class="op">/</span><span class="st"> </span>population[current]
    <span class="co"># new current island (either current current or proposal)</span>
    current   &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&lt;</span><span class="st"> </span>prob_move, proposal, current)
  }
  <span class="kw">tibble</span>(
    <span class="dt">step     =</span> <span class="dv">1</span><span class="op">:</span>num_steps,
    <span class="dt">island   =</span> island_names[island_seq],
    <span class="dt">proposal =</span> island_names[proposal_seq]
  )
}</code></pre>
<p><strong>Question 10:</strong> Look at the code above and answer the following.</p>
<ul>
<li><p>What are the default populations of the islands?</p></li>
<li><p>What is the default jump rule?</p></li>
<li><p>Explain what each of the following bits of code are doing:</p>
<ul>
<li><code>other_islands &lt;- setdiff(1:num_islands, current)</code></li>
<li><code>prob = purrr::map(other_islands, ~ J(current, .x))</code></li>
<li>the call to <code>sample()</code></li>
<li><code>prob_move &lt;- population[proposal] / population[current]</code></li>
<li><code>current   &lt;- ifelse(runif(1) &lt; prob_move, proposal, current)</code></li>
<li>the call to <code>tibble()</code></li>
</ul></li>
</ul>
<div id="jumping-to-any-island" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Jumping to any island</h3>
<p>Now let’s simulate the first 5000 nights of King Markov’s reign.</p>
<pre class="sourceCode r"><code class="sourceCode r">Tour &lt;-<span class="st"> </span><span class="kw">KingMarkov</span>(<span class="dv">5000</span>) 
Target &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">island =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">prop =</span> (<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>)<span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>))
<span class="kw">gf_line</span>(island <span class="op">~</span><span class="st"> </span>step, <span class="dt">data =</span> Tour <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">&lt;=</span><span class="st"> </span><span class="dv">200</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(proposal <span class="op">~</span><span class="st"> </span>step, <span class="dt">data =</span> Tour <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">&lt;=</span><span class="st"> </span><span class="dv">200</span>),
           <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)) </code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<p><img src="Redoing_files/figure-html/ch07-any-island-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dhistogram</span>( <span class="op">~</span><span class="st"> </span>island, <span class="dt">data =</span> Tour, <span class="dt">binwidth =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(prop <span class="op">~</span><span class="st"> </span>island, <span class="dt">data =</span> Target, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-any-island-2.png" width="65%" /></p>
<p><strong>Question 11:</strong>
Look at the first plot. It shows where the king stayed each
of the first 200 nights.</p>
<ul>
<li><p>Did the king ever stay two consecutive nights on the same island?
(How can you tell from the plot?)</p></li>
<li><p>Did the king ever stay two consecutive nights on the smallest island?
(How can you answer this without looking at the plot?)</p></li>
</ul>
</div>
<div id="jumping-only-to-neighbor-islands" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Jumping only to neighbor islands</h3>
<p>What if we only allow jumping to neighboring islands? (Imagine the
islands are arranged in a circle and we only sail clockwise or
counterclockwise around the circle to the nearest island.)</p>
<pre class="sourceCode r"><code class="sourceCode r">neighbor &lt;-<span class="st"> </span><span class="cf">function</span>(a, b) <span class="kw">as.numeric</span>(<span class="kw">abs</span>(a<span class="op">-</span>b) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>)) 
Tour &lt;-<span class="st"> </span><span class="kw">KingMarkov</span>(<span class="dv">10000</span>, <span class="dt">population =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">J =</span> neighbor) 
Target &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">island =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">prop =</span> (<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>)<span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>))
<span class="kw">gf_line</span>(island <span class="op">~</span><span class="st"> </span>step, <span class="dt">data =</span> Tour <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">&lt;=</span><span class="st"> </span><span class="dv">200</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-neighbor-05-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dhistogram</span>( <span class="op">~</span><span class="st"> </span>island, <span class="dt">data =</span> Tour, <span class="dt">binwidth =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(prop <span class="op">~</span><span class="st"> </span>island, <span class="dt">data =</span> Target, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-neighbor-05-2.png" width="65%" /></p>
<p>The effect of visiting only neighbors is more dramatic with more islands.</p>
<pre class="sourceCode r"><code class="sourceCode r">neighbor &lt;-<span class="st"> </span><span class="cf">function</span>(a, b) <span class="kw">as.numeric</span>(<span class="kw">abs</span>(a<span class="op">-</span>b) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">9</span>)) 
Tour &lt;-<span class="st"> </span><span class="kw">KingMarkov</span>(<span class="dv">10000</span>, <span class="dt">population =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">J =</span> neighbor) 
Target &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">island =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">prop =</span> (<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)<span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))
<span class="kw">gf_line</span>(island <span class="op">~</span><span class="st"> </span>step, <span class="dt">data =</span> Tour <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">&lt;=</span><span class="st"> </span><span class="dv">200</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-neighbor-10-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dhistogram</span>( <span class="op">~</span><span class="st"> </span>island, <span class="dt">data =</span> Tour, <span class="dt">binwidth =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(prop <span class="op">~</span><span class="st"> </span>island, <span class="dt">data =</span> Target, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-neighbor-10-2.png" width="65%" /></p>

</div>
</div>
<div id="markov-chains-and-posterior-sampling" class="section level2">
<h2><span class="header-section-number">7.5</span> Markov Chains and Posterior Sampling</h2>
<p>That was a nice story, and some nice probability theory. But what does it have
to do with Bayesian computation?
Regular Markov Chains (and some generalizations of them) can be used to sample
from a posterior distribution:</p>
<ul>
<li>state = island = set of parameter values
<ul>
<li>in typical applications, this will be an infinite state space</li>
</ul></li>
<li>population = prior * likelihood
<ul>
<li>importantly, we do not need to normalize the posterior; that would typically
be a very computationally expensive thing to do</li>
</ul></li>
<li>start in any island = start at any parameter values
<ul>
<li>convergence may be faster from some starting states than from others,
but in principle, any state will do</li>
</ul></li>
<li>randomly choose a proposal island = randomly select a proposal set of parameter values
<ul>
<li>if the posterior is greater there, move</li>
<li>if the posterior is smaller, move anyway with probability</li>
</ul></li>
</ul>
<p><span class="math display">\[A = \frac{\mbox{proposal `posterior&#39;}}{\mbox{current `posterior&#39;}}\]</span></p>
<p>Metropolis-Hastings variation:</p>
<ul>
<li>More choices for <span class="math inline">\(J()\)</span> (need not be symmetric)
gives more opportunity to tune for convergence</li>
</ul>
<p>Other variations:</p>
<ul>
<li>Can allow <span class="math inline">\(M\)</span> to change over the course of the algorithm. (No longer
time-homogeneous.)</li>
</ul>
<div id="example-1-estimating-a-proportion" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Example 1: Estimating a proportion</h3>
<p>To see how this works in practice, let’s consider our familiar model
that has a Bernoulli likelihood and a beta prior:</p>
<ul>
<li><span class="math inline">\(Y_i \sim {\sf Bern}(\theta)\)</span></li>
<li><span class="math inline">\(\theta \sim {\sf Beta}(a, b)\)</span></li>
</ul>
<p>Since we are already familiar with situation, we know that the
posterior should be a beta distribution when the prior is a beta
distribution. We can use this information to see how well the
algorithm works in that situation.</p>
<p>Let’s code up our Metropolis algorithm for this situation. There
is a new wrinkle, however:
The state space for the parameter is a continuous interval [0,1].
So we need a new kind of jump rule</p>
<ul>
<li><p>Instead of sampling from a finite state space, we use <code>rnorm()</code></p></li>
<li><p>The standard deviation of the normal distribution (called <code>size</code>
in the code below) controls how large a step we take (on average).
This number has nothing to do with the model,
it is a <strong>tuning parameter</strong> of the algorithm.</p></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">metro_bern &lt;-<span class="st"> </span><span class="cf">function</span>(
  x, n,            <span class="co"># x = successes, n = trials</span>
  <span class="dt">size =</span> <span class="fl">0.01</span>,     <span class="co"># sd of jump distribution</span>
  <span class="dt">start =</span> <span class="fl">0.5</span>,     <span class="co"># value of theta to start at</span>
  <span class="dt">num_steps =</span> <span class="fl">1e4</span>, <span class="co"># number of steps to run the algorithm</span>
  <span class="dt">prior =</span> dunif,   <span class="co"># function describing prior</span>
  ...              <span class="co"># additional arguments for prior</span>
  ) {
  
  theta             &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  proposed_theta    &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  move              &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  theta[<span class="dv">1</span>]          &lt;-<span class="st"> </span>start
 
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(num_steps<span class="dv">-1</span>)) {
    <span class="co"># head to new &quot;island&quot;</span>
    proposed_theta[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, theta[i], size)
    
    <span class="cf">if</span> (proposed_theta[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span>
<span class="st">        </span>proposed_theta[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>) {
      prob_move &lt;-<span class="st"> </span><span class="dv">0</span>           <span class="co"># because prior is 0</span>
    } <span class="cf">else</span> {
      current_prior       &lt;-<span class="st"> </span><span class="kw">prior</span>(theta[i], ...)
      current_likelihood  &lt;-<span class="st"> </span><span class="kw">dbinom</span>(x, n, theta[i])
      current_posterior   &lt;-<span class="st"> </span>current_prior <span class="op">*</span><span class="st"> </span>current_likelihood
      proposed_prior      &lt;-<span class="st"> </span><span class="kw">prior</span>(proposed_theta[i<span class="op">+</span><span class="dv">1</span>], ...)
      proposed_likelihood &lt;-<span class="st"> </span><span class="kw">dbinom</span>(x, n, proposed_theta[i<span class="op">+</span><span class="dv">1</span>])
      proposed_posterior  &lt;-<span class="st"> </span>proposed_prior <span class="op">*</span><span class="st"> </span>proposed_likelihood
      prob_move           &lt;-<span class="st"> </span>proposed_posterior <span class="op">/</span><span class="st"> </span>current_posterior
    }
    
    
    <span class="co"># sometimes we &quot;sail back&quot;</span>
    <span class="cf">if</span> (<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&gt;</span><span class="st"> </span>prob_move) { <span class="co"># sail back</span>
       move[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">FALSE</span>
      theta[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>theta[i]
    } <span class="cf">else</span> {                    <span class="co"># stay</span>
       move[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">TRUE</span>
      theta[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>proposed_theta[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]
    }
  }
  
  <span class="kw">tibble</span>(
    <span class="dt">step =</span> <span class="dv">1</span><span class="op">:</span>num_steps, 
    <span class="dt">theta =</span> theta,
    <span class="dt">proposed_theta =</span> proposed_theta,
    <span class="dt">move =</span> move, <span class="dt">size =</span> size
  )
}</code></pre>
<p><strong>Question 12:</strong> What happens if the proposed value for <span class="math inline">\(\theta\)</span> is not in the
interval <span class="math inline">\([0,1]\)</span>? Why?</p>
<p><strong>Question 13:</strong> What do <code>proposed_posterior</code> and <code>current_posterior</code>
correspond to in the story of King Markov?</p>
<p><strong>Question 14:</strong> Notice that we are using the unnormalized
posterior. Why don’t we need to normalize the posterior?
Why is it important that we don’t have to normalize the posterior?</p>
<div id="looking-at-posterior-samples" class="section level4">
<h4><span class="header-section-number">7.5.1.1</span> Looking at posterior samples</h4>
<p>The purpose of all this was to get samples from the posterior
distribution. We can use histograms or density plots to
see what our MCMC algorithm shows us for the posterior distribution.
When using MCMC algorithms, we won’t typically have ways of
knowing the “right answer”. But in this case, we know the
posterior is Beta(6, 11), so we can compare our posterior samples
to that distribution to see how well things worked.</p>
<p>Let’s try a nice small step size like 0.2%.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">341</span>)
Tour &lt;-<span class="st"> </span><span class="kw">metro_bern</span>(<span class="dv">5</span>, <span class="dv">15</span>, <span class="dt">size =</span> <span class="fl">0.002</span>)
<span class="kw">gf_dhistogram</span>(<span class="op">~</span><span class="st"> </span>theta, <span class="dt">data =</span> Tour, <span class="dt">bins =</span> <span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_dens</span>(<span class="op">~</span><span class="st"> </span>theta, <span class="dt">data =</span> Tour) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="dv">6</span>, <span class="dt">shape2 =</span> <span class="dv">11</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-small-step-1.png" width="65%" /></p>
<p>Hmm. That’s not too good. Let’s see if we can figure out why.</p>
</div>
<div id="trace-plots" class="section level4">
<h4><span class="header-section-number">7.5.1.2</span> Trace Plots</h4>
<p>A trace plot shows the “tour of King Markov’s ship” – the sequence of
parameter values sampled (in order).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(theta <span class="op">~</span><span class="st"> </span>step, <span class="dt">data =</span> Tour) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gf_hline</span>(<span class="dt">yintercept =</span> <span class="dv">5</span><span class="op">/</span><span class="dv">15</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) </code></pre>
<p><img src="Redoing_files/figure-html/ch07-traceplot-1.png" width="65%" /></p>
<p><strong>Question 15:</strong> Why does the trace plot begin at a height of 0.5?</p>
<p><strong>Question 16:</strong> What features of this trace plot indicate
that our posterior sampling isn’t working well? Would making
the step size larger or smaller be more likely to help?</p>
<p><strong>Question 17:</strong> Since we know that the posterior distribution is
Beta(6, 11), how could we use R to show us what an ideal trace plot
would look like?</p>
<!-- With such a small step size, it takes a very long time to get from our  -->
<!-- starting point ($\theta =0.5$) to the more credible values near 1/3. -->

</div>
<div id="comparing-step-sizes" class="section level4">
<h4><span class="header-section-number">7.5.1.3</span> Comparing step sizes</h4>
<p>Let’s see how our choice of <code>step</code> affects the sampling.
Size 0 is sampling from a true Beta(6, 11) distribution.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">341</span>)
Tours &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">bind_rows</span>(
    <span class="kw">metro_bern</span>(<span class="dv">5</span>, <span class="dv">15</span>, <span class="dt">size =</span> <span class="fl">0.02</span>),
    <span class="kw">metro_bern</span>(<span class="dv">5</span>, <span class="dv">15</span>, <span class="dt">size =</span> <span class="fl">0.2</span>),
    <span class="kw">metro_bern</span>(<span class="dv">5</span>, <span class="dv">15</span>, <span class="dt">size =</span> <span class="fl">0.002</span>),
    <span class="kw">tibble</span>(<span class="dt">theta =</span> <span class="kw">rbeta</span>(<span class="fl">1e4</span>, <span class="dv">6</span>, <span class="dv">11</span>), <span class="dt">size =</span> <span class="dv">0</span>, <span class="dt">step =</span> <span class="dv">1</span><span class="op">:</span><span class="fl">1e4</span>)
  )</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dhistogram</span>( <span class="op">~</span><span class="st"> </span>theta <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Tours, <span class="dt">bins =</span> <span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="dv">6</span>, <span class="dt">shape2 =</span> <span class="dv">11</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-metropolis-beta-compare-plots-1.png" width="65%" /></p>
<p>Sizes 0.2 and 0.02 look much better than 0.002.</p>
<p>Looking at the trace plots, we see that the samples using a step size of 0.02
are still highly auto-correlated (neighboring values are similar to
each other).
Thus the “effective sample size” is not nearly as big as the number of
posterior samples we have generated.</p>
<p>With a step size of 0.2, the amount of auto-correlation is substantially
reduced, but not eliminated.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(theta <span class="op">~</span><span class="st"> </span>step, <span class="dt">data =</span> Tours) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_hline</span>(<span class="dt">yintercept =</span> <span class="dv">5</span><span class="op">/</span><span class="dv">14</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gf_facet_grid</span>(size <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_lims</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1000</span>))</code></pre>
<pre><code>## Warning: Removed 9000 rows containing missing values (geom_path).</code></pre>
<p><img src="Redoing_files/figure-html/ch07-beta-trace-1.png" width="65%" /></p>
</div>
<div id="auto-correlation-and-thinning" class="section level4">
<h4><span class="header-section-number">7.5.1.4</span> Auto-correlation and Thinning</h4>
<p>One way to reduce the auto-correlation in posterior samples is by
thinning. This auto-correlation plot suggests that keeping every 7th
or 8th value when <code>size</code> is 0.2 should give us a posterior sample that
is nearly independent.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">acf</span>(Tours <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(size <span class="op">==</span><span class="st"> </span><span class="fl">0.2</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(theta))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-acf-0.2-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(theta <span class="op">~</span><span class="st"> </span>step, <span class="dt">data =</span> Tours <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">%%</span><span class="st"> </span><span class="dv">8</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_hline</span>(<span class="dt">yintercept =</span> <span class="dv">5</span><span class="op">/</span><span class="dv">14</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gf_facet_grid</span>(size <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_lims</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1000</span>))</code></pre>
<pre><code>## Warning: Removed 1125 rows containing missing values (geom_path).</code></pre>
<p><img src="Redoing_files/figure-html/ch07-beta-thin-8-1.png" width="65%" /></p>
<p>Now the idealized trace plot and the trace plot for <code>step = 0.2</code> are
quite similar looking. So the effective sample size for that step size
is approximately 1/8 the number of posterior samples generated.</p>
<p>For <code>step=0.02</code> we must thin even more, which would require generating
many more posterior samples to begin with:</p>
<pre class="sourceCode r"><code class="sourceCode r">ACF &lt;-<span class="st"> </span>
<span class="st">  </span>broom<span class="op">::</span><span class="kw">tidy</span>(<span class="kw">acf</span>(Tours <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(size <span class="op">==</span><span class="st"> </span><span class="fl">0.02</span>, step <span class="op">&gt;</span><span class="st"> </span><span class="dv">1000</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(theta), 
                  <span class="dt">plot =</span> <span class="ot">FALSE</span>, <span class="dt">lag.max =</span> <span class="dv">50</span>))
<span class="kw">gf_col</span>(acf <span class="op">~</span><span class="st"> </span>lag, <span class="dt">data =</span> ACF, <span class="dt">width =</span> <span class="fl">0.2</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-acf-0.02-1.png" width="65%" /></p>

</div>
</div>
<div id="example-2-estimating-mean-and-variance" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Example 2: Estimating mean and variance</h3>
<p>Consider the following simple model:</p>
<ul>
<li><span class="math inline">\(Y_i \sim {\sf Norm}(\mu, \sigma)\)</span></li>
<li><span class="math inline">\(\mu \sim {\sf Norm}(0, 1)\)</span></li>
<li><span class="math inline">\(\log(\sigma) \sim {\sf Norm}(0,1)\)</span></li>
</ul>
<p>In this case the posterior distribution for <span class="math inline">\(\mu\)</span> can be worked out exactly
and should be normal.</p>
<p>Let’s code up our Metropolis algorithm for this situation. New stuff:</p>
<ul>
<li>we have two parameters, so we’ll use separate jumps for each and combine
<ul>
<li>we could use a jump rule based on both values together, but
we’ll keep this simple</li>
</ul></li>
<li>the state space for each parameter is infinite, so we need a new kind of jump rule
<ul>
<li>instead of sampling from a finite state space, we use <code>rnorm()</code></li>
<li>the standard deviation controls how large a step we take (on average)</li>
<li>example below uses same standard deviation for both parameters, but
we should select them individually if the parameters are on different scales</li>
</ul></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">metro_norm &lt;-<span class="st"> </span><span class="cf">function</span>(
  y,                <span class="co"># data vector</span>
  <span class="dt">num_steps =</span> <span class="fl">1e5</span>,  
  <span class="dt">size =</span> <span class="dv">1</span>,         <span class="co"># sd&#39;s of jump distributions</span>
  <span class="dt">start =</span> <span class="kw">list</span>(<span class="dt">mu =</span> <span class="dv">0</span>, <span class="dt">log_sigma =</span> <span class="dv">0</span>)
  ) {
 
  size &lt;-<span class="st"> </span><span class="kw">rep</span>(size, <span class="dv">2</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]        <span class="co"># make sure exactly two values</span>
  mu        &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  log_sigma &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  move      &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  mu[<span class="dv">1</span>] &lt;-<span class="st"> </span>start<span class="op">$</span>mu
  log_sigma[<span class="dv">1</span>] &lt;-<span class="st"> </span>start<span class="op">$</span>log_sigma
  move[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">TRUE</span>
 
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(num_steps <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)) {
    <span class="co"># head to new &quot;island&quot;</span>
    mu[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]        &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, mu[i], size[<span class="dv">1</span>])
    log_sigma[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, log_sigma[i], size[<span class="dv">2</span>])
    move[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">TRUE</span>
    
    log_post_current &lt;-<span class="st"> </span>
<span class="st">      </span><span class="kw">dnorm</span>(mu[i], <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">dnorm</span>(log_sigma[i], <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(y, mu[i], <span class="kw">exp</span>(log_sigma[i]), <span class="dt">log =</span> <span class="ot">TRUE</span>))
    log_post_proposal &lt;-<span class="st"> </span>
<span class="st">      </span><span class="kw">dnorm</span>(mu[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>], <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">dnorm</span>(log_sigma[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>], <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(y, mu[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>], <span class="kw">exp</span>(log_sigma[i<span class="op">+</span><span class="dv">1</span>]), <span class="dt">log =</span> <span class="ot">TRUE</span>))
    prob_move &lt;-<span class="st"> </span><span class="kw">exp</span>(log_post_proposal <span class="op">-</span><span class="st"> </span>log_post_current)
    
    <span class="co"># sometimes we &quot;sail back&quot;</span>
    <span class="cf">if</span> (<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&gt;</span><span class="st"> </span>prob_move) {  
      move[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">FALSE</span>
      mu[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>mu[i]
      log_sigma[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>log_sigma[i]
    } 
    
  }
  <span class="kw">tibble</span>(
    <span class="dt">step =</span> <span class="dv">1</span><span class="op">:</span>num_steps,
    <span class="dt">mu =</span> mu,
    <span class="dt">log_sigma =</span> log_sigma,
    <span class="dt">move =</span> move,
    <span class="dt">size =</span> <span class="kw">paste</span>(size, <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>)
  )
}</code></pre>
<p>Let’s use the algorithm with three different size values and compare results.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">341</span>)
y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">25</span>, <span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># sample of 25 from Norm(1, 2)</span>
Tour1    &lt;-<span class="st"> </span><span class="kw">metro_norm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">1</span>)
Tour0<span class="fl">.1</span>  &lt;-<span class="st"> </span><span class="kw">metro_norm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="fl">0.1</span>)
Tour0<span class="fl">.01</span> &lt;-<span class="st"> </span><span class="kw">metro_norm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="fl">0.01</span>)
Norm_Tours &lt;-<span class="st">  </span><span class="kw">bind_rows</span>(Tour1, Tour0<span class="fl">.1</span>, Tour0<span class="fl">.01</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">df_stats</span>(<span class="op">~</span><span class="st"> </span>move <span class="op">|</span><span class="st"> </span>size, <span class="dt">data =</span> Norm_Tours, props)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">size</th>
<th align="right">prop_FALSE</th>
<th align="right">prop_TRUE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0.01, 0.01</td>
<td align="right">0.0422</td>
<td align="right">0.9578</td>
</tr>
<tr class="even">
<td align="left">0.1, 0.1</td>
<td align="right">0.2380</td>
<td align="right">0.7620</td>
</tr>
<tr class="odd">
<td align="left">1, 1</td>
<td align="right">0.9134</td>
<td align="right">0.0866</td>
</tr>
</tbody>
</table>
<div id="density-plots" class="section level4">
<h4><span class="header-section-number">7.5.2.1</span> Density plots</h4>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dhistogram</span>( <span class="op">~</span><span class="st"> </span>mu <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Norm_Tours, <span class="dt">bins =</span> <span class="dv">100</span>) </code></pre>
<p><img src="Redoing_files/figure-html/ch07-metropolis-norm-compare-plots-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dhistogram</span>( <span class="op">~</span><span class="st"> </span><span class="kw">exp</span>(log_sigma) <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Norm_Tours, <span class="dt">bins =</span> <span class="dv">100</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-metropolis-norm-compare-plots-2.png" width="65%" /></p>
</div>
<div id="trace-plots-1" class="section level4">
<h4><span class="header-section-number">7.5.2.2</span> Trace plots</h4>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Norm_Tours) </code></pre>
<p><img src="Redoing_files/figure-html/ch07-trace-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(log_sigma <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Norm_Tours)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-trace-2.png" width="65%" /></p>
</div>
<div id="comparing-multiple-chains" class="section level4">
<h4><span class="header-section-number">7.5.2.3</span> Comparing Multiple Chains</h4>
<p>If we run multiple chains with different starting points and different random
choices, we hope to see similar trace plots. After all, we don’t want our
analysis to be an analysis of starting points or of random choices.</p>
<pre class="sourceCode r"><code class="sourceCode r">Tour1a &lt;-<span class="st"> </span><span class="kw">metro_norm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">chain =</span> <span class="st">&quot;A&quot;</span>)
Tour1b &lt;-<span class="st"> </span><span class="kw">metro_norm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">chain =</span> <span class="st">&quot;B&quot;</span>)
Tour1c &lt;-<span class="st"> </span><span class="kw">metro_norm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">start =</span> <span class="kw">list</span>(<span class="dt">mu =</span> <span class="dv">10</span>, <span class="dt">log_sigma =</span> <span class="dv">5</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">chain =</span> <span class="st">&quot;C&quot;</span>)
Tour1d &lt;-<span class="st"> </span><span class="kw">metro_norm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">start =</span> <span class="kw">list</span>(<span class="dt">mu =</span> <span class="dv">10</span>, <span class="dt">log_sigma =</span> <span class="dv">5</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">chain =</span> <span class="st">&quot;D&quot;</span>)
Tours1 &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(Tour1a, Tour1b, Tour1c, Tour1d)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step, <span class="dt">color =</span> <span class="op">~</span>chain, <span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">data =</span> Tours1)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-multichain-plot-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step, <span class="dt">color =</span> <span class="op">~</span>chain, <span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">data =</span> Tours1) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gf_facet_grid</span>( chain <span class="op">~</span><span class="st"> </span>.)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-multichain-plot-2.png" width="65%" /></p>
</div>
<div id="comparing-chains-to-an-ideal-chain" class="section level4">
<h4><span class="header-section-number">7.5.2.4</span> Comparing Chains to an Ideal Chain</h4>
<p>Not all posteriors are normal, but here’s what a chain would look like if the posterior is normal and there
is no correlation between draws.</p>
<pre class="sourceCode r"><code class="sourceCode r">Ideal &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">step =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5000</span>, <span class="dt">mu =</span> <span class="kw">rnorm</span>(<span class="dv">5000</span>, <span class="dv">1</span>, <span class="fl">.3</span>), <span class="dt">size =</span> <span class="st">&quot;ideal&quot;</span>)
<span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size, <span class="dt">data =</span> Norm_Tours <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_rows</span>(Ideal))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-ideal-chain01-1.png" width="65%" /></p>
<p>If the draws are correlated, then we might get more ideal behavior if we
selected only a subset –
every 20th or every 30th value, for example. This is
the idea behind “effective sample size”. The effective sample size of a
correlated chain is the length of an ideal chain that contains as much
independent information as the correlated chain.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">acf</span>(Norm_Tours <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(size <span class="op">==</span><span class="st"> &quot;1, 1&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(mu))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-ideal-chain02-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> <span class="kw">bind_rows</span>(Norm_Tours, Ideal) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">%%</span><span class="st"> </span><span class="dv">20</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_labs</span>(<span class="dt">title =</span> <span class="st">&quot;Every 20th draw&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-ideal-chain02-2.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> <span class="kw">bind_rows</span>(Norm_Tours, Ideal) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">%%</span><span class="st"> </span><span class="dv">30</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_labs</span>(<span class="dt">title =</span> <span class="st">&quot;Every 30th draw&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-ideal-chain02-3.png" width="65%" /></p>
<p>If we thin to every 20th value, our chains with <code>size = 1</code> and <code>size = 0.1</code>
look quite similar to the ideal chain.
The chain with <code>size = 0.01</code> still moves too slowly through
the parameter space.
So tuning parameters will affect the effective sample size.</p>
</div>
<div id="discarding-the-first-portion-of-a-chain" class="section level4">
<h4><span class="header-section-number">7.5.2.5</span> Discarding the first portion of a chain</h4>
<p>The first portion of a chain may not work as well. This portion is typically
removed from the analysis since it is more an indication of the starting values
used than the long-run sampling from the posterior.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(log_sigma <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Norm_Tours) </code></pre>
<p><img src="Redoing_files/figure-html/ch07-burnin-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_path</span>(log_sigma <span class="op">~</span><span class="st"> </span>mu <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>(step <span class="op">&gt;</span><span class="st"> </span><span class="dv">500</span>), <span class="dt">data =</span> Norm_Tours, <span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_density2d</span>(log_sigma <span class="op">~</span><span class="st"> </span>mu, <span class="dt">data =</span> Norm_Tours) </code></pre>
<p><img src="Redoing_files/figure-html/ch07-burnin-2.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_histogram</span>( <span class="op">~</span><span class="st"> </span>mu <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>(step <span class="op">&gt;</span><span class="st"> </span><span class="dv">500</span>) , <span class="dt">data =</span> Norm_Tours, <span class="dt">binwidth =</span> <span class="fl">0.1</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-burnin-3.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_histogram</span>( <span class="op">~</span><span class="st"> </span>log_sigma <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>(step <span class="op">&gt;</span><span class="st"> </span><span class="dv">500</span>), <span class="dt">data =</span> Norm_Tours, <span class="dt">binwidth =</span> <span class="fl">0.1</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-burnin-4.png" width="65%" /></p>
</div>
</div>
<div id="issues-with-metropolis-algorithm" class="section level3">
<h3><span class="header-section-number">7.5.3</span> Issues with Metropolis Algorithm</h3>
<p>These are really issues with all MCMC algorithms, not just the Metropolis version:</p>
<ul>
<li>First portion of a chain might not be very good, need to discard it</li>
<li>Tuning can affect performance – how do we tune?</li>
<li>Samples are correlated – although the long-run probabilities are right,
the next stop is not independent of the current one
<ul>
<li>so our effective posterior sample size isn’t as big as it appears</li>
</ul></li>
</ul>
<!-- As it turns out, we won't use the Metropolis or Metropolis-Hastings algorithms in practice. -->
<!-- Instead we will use Stan, which shares many features in common with the Metropolis algorithm -->
<!-- but has demonstrated that it works well in a wide variety (but not all) models with better -->
<!-- performance.  Performance improvements are especially notable when the number of parameters in  -->
<!-- the model is large. -->
</div>
</div>
<div id="two-coins" class="section level2">
<h2><span class="header-section-number">7.6</span> Two coins</h2>
<div id="the-model" class="section level3">
<h3><span class="header-section-number">7.6.1</span> The model</h3>
<p>Suppose we have two coins and want to compare the proportion
of heads each coin generates.</p>
<ul>
<li>Parameters: <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span></li>
<li>Data: <span class="math inline">\(N_1\)</span> flips of coin 1 and <span class="math inline">\(N_2\)</span> flips of coin 2. (Results: <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> heads, respectivly.)</li>
<li>Likelihood: indepdendent Bernoulli (each flip independent of the other flips, each coin independent of the other coin)</li>
<li>Prior: Independent Beta distributions for each <span class="math inline">\(\theta_i\)</span></li>
</ul>
<p>That is,</p>
<ul>
<li><span class="math inline">\(y_{1i} \sim {\sf Bern}(\theta_1), y_{2i} \sim {\sf Bern}(\theta_2)\)</span></li>
<li><span class="math inline">\(\theta_1 \sim {\sf Beta(a_1, b_1)}, \theta_2 \sim {\sf Beta(a_2, b_2)}\)</span> (independent)</li>
</ul>
<p>For the examples below we will use data showing that 6 of 8 tosses of the first coin
were heads and only 2 of 7 tosses of the second coin. But the methods work equally well
with other data sets. While comparing a small number of coin tosses like this is not
so interesting, the method can be used for a wide range of practical things, liking
testing whether two treatments for a condition are equally effective or not.</p>
</div>
<div id="exact-analysis" class="section level3">
<h3><span class="header-section-number">7.6.2</span> Exact analysis</h3>
<p>From this we can work out the posterior as usual.
(Pay attention to the important parts of the kernel, that is,
the numerators.)</p>
<p><span class="math display">\[\begin{align*}
p(\theta_1, \theta_2 \mid D) 
&amp;= p(D \mid \theta_1, \theta_2) p(\theta_1, \theta_2) / p(D) 
\\[3mm]
&amp;= \theta_1^{z_1} (1 − \theta_1)^{N_1−z_1} 
     \theta_1^{z_2} (1 − \theta_2)^{N_2−z_2} 
     p(\theta_1, \theta_2) / p(D)
\\[3mm]
&amp;=
  \frac{
    \theta_1^{z_1}  (1 − \theta_1)^{N_1 − z_1} 
    \theta_1^{z_2}  (1 − \theta_2)^{N_2 − z_2} 
    \theta_1^{a_1−1}(1 − \theta_1)^{b_1 - 1} 
    \theta_2^{a_2−1}(1 − \theta_2)^{b_2 - 1}}
    {p(D)B(a_1, b_1)B(a_2, b_2)}
\\[3mm]
&amp;=
    \frac{
       \theta_1^{z_1 + a_1 − 1}(1 − \theta_1)^{N_1 − z_1 + b_1 − 1}
       \theta_2^{z_2 + a_2 − 1}(1 − \theta_2)^{N_2 − z_2 + b_2 − 1}
    }{p(D)B(a_1, b_1)B(a_2, b_2)}
\end{align*}\]</span></p>
<p>So the posterior distribution of <span class="math inline">\(\langle \theta_1, \theta_2\)</span>
is two independent Beta distributions:
<span class="math inline">\({\sf Beta}(z_1 + a, N_1 - z_1 + b_1)\)</span> and
<span class="math inline">\({\sf Beta}(z_2 + a, N_2 - z_2 + b_2)\)</span>.</p>
<p>Some nice images of these distributions appear on page 167.</p>
<p><img src="images/page167.png" width="65%" style="display: block; margin: auto;" /></p>
</div>
<div id="metropolis" class="section level3">
<h3><span class="header-section-number">7.6.3</span> Metropolis</h3>
<pre class="sourceCode r"><code class="sourceCode r">metro_2coins &lt;-<span class="st"> </span><span class="cf">function</span>(
  z1, n1,              <span class="co"># z = successes, n = trials</span>
  z2, n2,              <span class="co"># z = successes, n = trials</span>
  <span class="dt">size  =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.1</span>), <span class="co"># sds of jump distribution</span>
  <span class="dt">start =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="co"># value of thetas to start at</span>
  <span class="dt">num_steps =</span> <span class="fl">5e4</span>,     <span class="co"># number of steps to run the algorithm</span>
  <span class="dt">prior1 =</span> dbeta,      <span class="co"># function describing prior</span>
  <span class="dt">prior2 =</span> dbeta,      <span class="co"># function describing prior</span>
  <span class="dt">args1 =</span> <span class="kw">list</span>(),      <span class="co"># additional args for prior1</span>
  <span class="dt">args2 =</span> <span class="kw">list</span>()      <span class="co"># additional args for prior2</span>
  ) {
  
  theta1            &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  theta2            &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  proposed_theta1   &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  proposed_theta2   &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  move              &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  theta1[<span class="dv">1</span>]         &lt;-<span class="st"> </span>start[<span class="dv">1</span>]
  theta2[<span class="dv">1</span>]         &lt;-<span class="st"> </span>start[<span class="dv">2</span>]

  size1 &lt;-<span class="st"> </span>size[<span class="dv">1</span>] 
  size2 &lt;-<span class="st"> </span>size[<span class="dv">2</span>] 
  
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(num_steps<span class="dv">-1</span>)) {
    <span class="co"># head to new &quot;island&quot;</span>
    proposed_theta1[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, theta1[i], size1)
    proposed_theta2[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, theta2[i], size2)
    
    <span class="cf">if</span> (proposed_theta1[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span>
<span class="st">        </span>proposed_theta1[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span>
<span class="st">        </span>proposed_theta2[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span>
<span class="st">        </span>proposed_theta2[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>) {
      proposed_posterior &lt;-<span class="st"> </span><span class="dv">0</span>  <span class="co"># because prior is 0</span>
    } <span class="cf">else</span> {
      current_prior &lt;-<span class="st"> </span>
<span class="st">        </span><span class="kw">do.call</span>(prior1, <span class="kw">c</span>(<span class="kw">list</span>(theta1[i]), args1)) <span class="op">*</span>
<span class="st">        </span><span class="kw">do.call</span>(prior2, <span class="kw">c</span>(<span class="kw">list</span>(theta2[i]), args2))
      current_likelihood  &lt;-<span class="st"> </span>
<span class="st">        </span><span class="kw">dbinom</span>(z1, n1, theta1[i]) <span class="op">*</span>
<span class="st">        </span><span class="kw">dbinom</span>(z2, n2, theta2[i])
      current_posterior   &lt;-<span class="st"> </span>current_prior <span class="op">*</span><span class="st"> </span>current_likelihood
      
      proposed_prior &lt;-<span class="st"> </span>
<span class="st">        </span><span class="kw">do.call</span>(prior1, <span class="kw">c</span>(<span class="kw">list</span>(proposed_theta1[i<span class="op">+</span><span class="dv">1</span>]), args1)) <span class="op">*</span>
<span class="st">        </span><span class="kw">do.call</span>(prior2, <span class="kw">c</span>(<span class="kw">list</span>(proposed_theta2[i<span class="op">+</span><span class="dv">1</span>]), args2))
      proposed_likelihood  &lt;-<span class="st"> </span>
<span class="st">        </span><span class="kw">dbinom</span>(z1, n1, proposed_theta1[i<span class="op">+</span><span class="dv">1</span>]) <span class="op">*</span>
<span class="st">        </span><span class="kw">dbinom</span>(z2, n2, proposed_theta2[i<span class="op">+</span><span class="dv">1</span>])
      proposed_posterior   &lt;-<span class="st"> </span>proposed_prior <span class="op">*</span><span class="st"> </span>proposed_likelihood
    }
    prob_move           &lt;-<span class="st"> </span>proposed_posterior <span class="op">/</span><span class="st"> </span>current_posterior
    
    <span class="co"># sometimes we &quot;sail back&quot;</span>
    <span class="cf">if</span> (<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&gt;</span><span class="st"> </span>prob_move) { <span class="co"># sail back</span>
       move[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">FALSE</span>
      theta1[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>theta1[i]
      theta2[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>theta2[i]
    } <span class="cf">else</span> {                    <span class="co"># stay</span>
       move[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">TRUE</span>
      theta1[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>proposed_theta1[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]
      theta2[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>proposed_theta2[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]
    }
  }
  
  <span class="kw">tibble</span>(
    <span class="dt">step =</span> <span class="dv">1</span><span class="op">:</span>num_steps, 
    <span class="dt">theta1 =</span> theta1,
    <span class="dt">theta2 =</span> theta2,
    <span class="dt">proposed_theta1 =</span> proposed_theta1,
    <span class="dt">proposed_theta2 =</span> proposed_theta2,
    <span class="dt">move =</span> move, 
    <span class="dt">size1 =</span> size1,
    <span class="dt">size2 =</span> size2
  )
}</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">Metro_2coinsA &lt;-
<span class="st">  </span><span class="kw">metro_2coins</span>(
    <span class="dt">z1 =</span> <span class="dv">6</span>, <span class="dt">n1 =</span> <span class="dv">8</span>,
    <span class="dt">z2 =</span> <span class="dv">2</span>, <span class="dt">n2 =</span> <span class="dv">7</span>,
    <span class="dt">size =</span> <span class="kw">c</span>(<span class="fl">0.02</span>, <span class="fl">0.02</span>),
    <span class="dt">args1 =</span> <span class="kw">list</span>(<span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>), <span class="dt">args2 =</span> <span class="kw">list</span>(<span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>)  
  )

Metro_2coinsA <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_density2d</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1)</code></pre>
<p><img src="Redoing_files/figure-html/metro-coins-A-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Metro_2coinsA <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_density</span>(<span class="op">~</span><span class="st"> </span>(theta2 <span class="op">-</span><span class="st"> </span>theta1))</code></pre>
<p><img src="Redoing_files/figure-html/metro-coins-A-2.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># effective sample size is much smaller than apparent sample size due to auto-correlation</span>
<span class="kw">acf</span>(Metro_2coinsA<span class="op">$</span>theta2 <span class="op">-</span><span class="st"> </span>Metro_2coinsA<span class="op">$</span>theta1)</code></pre>
<p><img src="Redoing_files/figure-html/metro-coins-A-3.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Metro_2coinsA <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">&lt;</span><span class="st"> </span><span class="dv">500</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_path</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1, <span class="dt">color =</span> <span class="op">~</span><span class="st"> </span>step, <span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1, <span class="dt">color =</span> <span class="op">~</span><span class="st"> </span>step, <span class="dt">alpha =</span> <span class="fl">0.5</span>) </code></pre>
<p><img src="Redoing_files/figure-html/metro-coins-A-4.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Metro_2coinsB &lt;-
<span class="st">  </span><span class="kw">metro_2coins</span>(
    <span class="dt">z1 =</span> <span class="dv">6</span>, <span class="dt">n1 =</span> <span class="dv">8</span>,
    <span class="dt">z2 =</span> <span class="dv">2</span>, <span class="dt">n2 =</span> <span class="dv">7</span>,
    <span class="dt">size =</span> <span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.2</span>),
    <span class="dt">args1 =</span> <span class="kw">list</span>(<span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>), <span class="dt">args2 =</span> <span class="kw">list</span>(<span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>)  
  )

Metro_2coinsB <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_density2d</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1)</code></pre>
<p><img src="Redoing_files/figure-html/unnamed-chunk-13-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Metro_2coinsB <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_density</span>(<span class="op">~</span><span class="st"> </span>(theta2 <span class="op">-</span><span class="st"> </span>theta1))</code></pre>
<p><img src="Redoing_files/figure-html/unnamed-chunk-13-2.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># effective sample size is better but still quite a bit</span>
<span class="co"># smaller than apparent sample size due to auto-correlation</span>
<span class="kw">acf</span>(Metro_2coinsB<span class="op">$</span>theta2 <span class="op">-</span><span class="st"> </span>Metro_2coinsB<span class="op">$</span>theta1)</code></pre>
<p><img src="Redoing_files/figure-html/unnamed-chunk-13-3.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Metro_2coinsB <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">&lt;</span><span class="st"> </span><span class="dv">500</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_path</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1, <span class="dt">color =</span> <span class="op">~</span><span class="st"> </span>step, <span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1, <span class="dt">color =</span> <span class="op">~</span><span class="st"> </span>step, <span class="dt">alpha =</span> <span class="fl">0.5</span>) </code></pre>
<p><img src="Redoing_files/figure-html/unnamed-chunk-13-4.png" width="65%" /></p>
</div>
<div id="gibbs-sampling" class="section level3">
<h3><span class="header-section-number">7.6.4</span> Gibbs sampling</h3>
<p>Gibbs sampling provides an attempt to improve on the efficiency of the standard
Metropolis algorithm by using a different method to propose new parameter values.
The idea is this:</p>
<ul>
<li><p>Pick one of the parameter values: <span class="math inline">\(\theta_i\)</span></p></li>
<li><p>Determine the posterior distribution of <span class="math inline">\(\theta_i\)</span> using current estimates
of the other parameters <span class="math inline">\(\{\theta_j \mid j \neq i\}\)</span></p>
<ul>
<li>This won’t be exactly right, because those estimates are not exactly right,
but it should be good when the parameters estimates are close to correct.</li>
</ul></li>
<li><p>Sample from the posterior distribution for <span class="math inline">\(\theta_i\)</span> to get a new proposed value for
<span class="math inline">\(\theta_i\)</span>.</p>
<ul>
<li>Always accept this proposal, since it is being sampled from a distribution that
already makes more likely values more likely to be proposed.</li>
</ul></li>
<li><p>Keep cycling through all the parameters, each time updating one using the current
estimates for the others.</p></li>
</ul>
<p><img src="images/Gibbs-figure.png" width="65%" style="display: block; margin: auto;" /></p>
<p>It takes some work to show that this also converges, and in practice it is often more efficient than the basic Metropolis algorithm.
But it is limited to situations where the marginal posterior can be be sampled from.</p>
<p>In our example, we can work out the marginal posterior fairly easily:</p>
<p><span class="math display">\[\begin{align*}
p(\theta_1|\theta_2, D) 
&amp;= p(\theta_1, \theta_2|D)/p(\theta_2|D)
\\
&amp;= p(\theta_1, \theta_2|D) \int d\theta_1 p(\theta_1, \theta_2|D) 
\\
&amp;= \frac{\mathrm{dbeta}(\theta_1, z_1 + a_1, N_1 −z_1 + b_1) \cdot
         \mathrm{dbeta}(\theta_2, z_2 + a_2, N_2 −z_2 + b_2)}
        {\int d\theta_1 \mathrm{dbeta}(\theta_1, z_1 + a_1, N_1 −z_1 +b_1) 
        \cdot
                        \mathrm{dbeta}(\theta_2 \mid z_2 +a_2, N_2 −z_2 +b_2)}
\\
&amp;= \frac{\mathrm{dbeta}(\theta_1, z_1 + a_1, N_1 − z_1 + b_1) \cdot
         \mathrm{dbeta}(\theta_2, z_2 + a_2, N_2 − z_2 + b_2)}
        {\mathrm{dbeta}(\theta_2 \mid z_2 + a_2, N_2 −z_2 +b_2)
          \int d\theta_1 \mathrm{dbeta}(\theta_1, z_1 + a_1, N_1 − z_1 + b_1)}
\\
&amp;= \frac{\mathrm{dbeta}(\theta_1, z_1 + a_1, N_1 − z_1 + b_1)}
        {\int d\theta_1 \mathrm{dbeta}(\theta_1, z_1 + a_1, N_1 − z_1 + b_1)}
\\
&amp;= \mathrm{dbeta}(\theta_1, z_1 + a_1, N_1 − z_1 + b_1)
\end{align*}\]</span></p>
<p>In other words, <span class="math inline">\(p(\theta_1 \mid \theta_2, D) = p(\theta_1, D)\)</span>, which isn’t surprising
since we already know that the posterior distributions <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> are
independent.
(In more complicated models, the marginal posterior may depend
on all of the parameters, so the calculation above might not be so simple.)
Of course, the analogous statement holds for <span class="math inline">\(\theta_2\)</span> in this model.</p>
<p>This examples shows off Gibbs sampling in a situation where it shines: when the posterior
distribution is a collection of independent marginals.</p>
<pre class="sourceCode r"><code class="sourceCode r">gibbs_2coins &lt;-<span class="st"> </span><span class="cf">function</span>(
  z1, n1,              <span class="co"># z = successes, n = trials</span>
  z2, n2,              <span class="co"># z = successes, n = trials</span>
  <span class="dt">start =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="co"># value of thetas to start at</span>
  <span class="dt">num_steps =</span> <span class="fl">1e4</span>,     <span class="co"># number of steps to run the algorithm</span>
  a1, b1,              <span class="co"># params for prior for theta1</span>
  a2, b2               <span class="co"># params for prior for theta2</span>
  ) {
  
  theta1            &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  theta2            &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  theta1[<span class="dv">1</span>]         &lt;-<span class="st"> </span>start[<span class="dv">1</span>]
  theta2[<span class="dv">1</span>]         &lt;-<span class="st"> </span>start[<span class="dv">2</span>]
  
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(num_steps<span class="dv">-1</span>)) {
    <span class="cf">if</span> (i <span class="op">%%</span><span class="st"> </span><span class="dv">2</span> <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { <span class="co"># update theta1</span>
      theta1[i<span class="op">+</span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dv">1</span>, z1 <span class="op">+</span><span class="st"> </span>a1, n1 <span class="op">-</span><span class="st"> </span>z1 <span class="op">+</span><span class="st"> </span>b1)
      theta2[i<span class="op">+</span><span class="dv">1</span>] &lt;-<span class="st"> </span>theta2[i]
    } <span class="cf">else</span> {           <span class="co"># update theta2</span>
      theta1[i<span class="op">+</span><span class="dv">1</span>] &lt;-<span class="st"> </span>theta1[i]
      theta2[i<span class="op">+</span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dv">1</span>, z2 <span class="op">+</span><span class="st"> </span>a2, n2 <span class="op">-</span><span class="st"> </span>z2 <span class="op">+</span><span class="st"> </span>b2)
    }
  }
  
  <span class="kw">tibble</span>(
    <span class="dt">step =</span> <span class="dv">1</span><span class="op">:</span>num_steps, 
    <span class="dt">theta1 =</span> theta1,
    <span class="dt">theta2 =</span> theta2,
  )
}</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">Gibbs &lt;-
<span class="st">  </span><span class="kw">gibbs_2coins</span>(<span class="dt">z1 =</span> <span class="dv">6</span>, <span class="dt">n1 =</span> <span class="dv">8</span>,
               <span class="dt">z2 =</span> <span class="dv">2</span>, <span class="dt">n2 =</span> <span class="dv">7</span>,
               <span class="dt">a1 =</span> <span class="dv">2</span>, <span class="dt">b1 =</span> <span class="dv">2</span>, <span class="dt">a2 =</span> <span class="dv">2</span>, <span class="dt">b2 =</span> <span class="dv">2</span>)  

Gibbs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gf_density2d</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1)</code></pre>
<p><img src="Redoing_files/figure-html/unnamed-chunk-15-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Gibbs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gf_dens</span>( <span class="op">~</span><span class="st"> </span>theta1)</code></pre>
<p><img src="Redoing_files/figure-html/unnamed-chunk-15-2.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">acf</span>(Gibbs<span class="op">$</span>theta1)</code></pre>
<p><img src="Redoing_files/figure-html/unnamed-chunk-15-3.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Gibbs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">&lt;</span><span class="st"> </span><span class="dv">500</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_path</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1, <span class="dt">color =</span> <span class="op">~</span><span class="st"> </span>step, <span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1, <span class="dt">color =</span> <span class="op">~</span><span class="st"> </span>step, <span class="dt">alpha =</span> <span class="fl">0.5</span>) </code></pre>
<p><img src="Redoing_files/figure-html/unnamed-chunk-15-4.png" width="65%" /></p>
<p>Note: Software like JAGS only records results each time it
makes a complete cycle through the parameters.
We could do that by keeping every other row:</p>
<pre class="sourceCode r"><code class="sourceCode r">Gibbs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">%%</span><span class="st"> </span><span class="dv">2</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gf_density2d</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-Gibbs-thin-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Gibbs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">%%</span><span class="st"> </span><span class="dv">2</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gf_density</span>( <span class="op">~</span><span class="st"> </span>(theta2 <span class="op">-</span><span class="st"> </span>theta1))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-Gibbs-thin-2.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Gibbs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">%%</span><span class="st"> </span><span class="dv">2</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">difference =</span> theta2 <span class="op">-</span><span class="st"> </span>theta1) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(difference) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">acf</span>()</code></pre>
<p><img src="Redoing_files/figure-html/ch07-Gibbs-thin-3.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r">Gibbs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">&lt;</span><span class="st"> </span><span class="dv">500</span>, step <span class="op">%%</span><span class="st"> </span><span class="dv">2</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_path</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1, <span class="dt">color =</span> <span class="op">~</span><span class="st"> </span>step, <span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(theta2 <span class="op">~</span><span class="st"> </span>theta1, <span class="dt">color =</span> <span class="op">~</span><span class="st"> </span>step, <span class="dt">alpha =</span> <span class="fl">0.5</span>) </code></pre>
<p><img src="Redoing_files/figure-html/ch07-Gibbs-thin-4.png" width="65%" /></p>
</div>
<div id="advantages-and-disadvantages-of-gibbs-vs-metropolis" class="section level3">
<h3><span class="header-section-number">7.6.5</span> Advantages and Disadvantages of Gibbs vs Metropolis</h3>
<div id="advantages" class="section level4 unnumbered">
<h4>Advantages</h4>
<ul>
<li><strong>No need to tune</strong>. The preformance of the Metropolis algorithm depends on
the jump rule used. Gibbs “auto-tunes” by using the marginal posteriors.</li>
<li>Gibbs sampling has the <strong>potential to sample much more efficiently</strong> than
Metropolis. It doesn’t propose updates only to reject them, and doesn’t suffer
from poor tuning that can make Metropolis search very slowly through the posterior
space.</li>
</ul>
</div>
<div id="disadvantages" class="section level4 unnumbered">
<h4>Disadvantages</h4>
<ul>
<li>Less general: <strong>We need to be able to sample from the marginal posterior</strong>.</li>
<li>Gibbs sampling <strong>can be slow if the posterior has highly correlated parameters</strong>
since given all but one of them, the algorithm with be very certain about the remaining
one and adjust it only a very small amount. It is hard for Gibbs samplers to quickly
move along “diagonal ridges” in the posterior.</li>
</ul>
<p>The good news is that other people have done the hard work of coding up Gibbs sampling for
us, we just need to learn how to describe the model in a way that works with their code.
We will be using JAGS (just another Gibbs Sampler) and later Stan (which generalizes the
metropolis algorithm in a different way) to fit more interesting models that would be
too time consuming to custom code on our own. The examples in the chapter are to help us
understand better how these algorithms work so we can interpret the results we obtian
when using them.</p>
</div>
</div>
<div id="so-what-do-we-learn-about-the-coins" class="section level3">
<h3><span class="header-section-number">7.6.6</span> So what do we learn about the coins?</h3>
<p>We have seen that we can fit this model a number of different ways now, but we haven’t
actually looked at the results. Are the coins different? How much different?</p>
<p>Since the Gibbs sampler is more efficient than the Metropolis algorithm for this
situation, we’ll use the posterior samples from the Gibbs sampler to answer.
We are interested in <span class="math inline">\(\theta_2 - \theta_1\)</span>, the difference in the biases of the two coins.
To learn about that, we simply investigate the distribution of that quantity in
our posterior samples. (Note: You cannot learn about this difference by only
considering <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> sepearately.)</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dhistogram</span>(<span class="op">~</span>(theta2 <span class="op">-</span><span class="st"> </span>theta1), <span class="dt">data =</span> Gibbs) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_dens</span>()</code></pre>
<p><img src="Redoing_files/figure-html/coins-difference-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hdi</span>(Gibbs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">difference =</span> theta2 <span class="op">-</span><span class="st"> </span>theta1), <span class="dt">pars =</span> <span class="st">&quot;difference&quot;</span>)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">par</th>
<th align="right">lo</th>
<th align="right">hi</th>
<th align="right">prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">difference</td>
<td align="right">-0.6623</td>
<td align="right">0.057</td>
<td align="right">0.95</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r">mosaic<span class="op">::</span><span class="kw">prop</span>(<span class="op">~</span>(theta2 <span class="op">-</span><span class="st"> </span>theta1 <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>), <span class="dt">data =</span> Gibbs)</code></pre>
<pre><code>## prop_TRUE 
##    0.0601</code></pre>
<p>We don’t have much data, so the posterior distribution of the difference in biases
is pretty wide. 0 is within the 95% HDI for <span class="math inline">\(\theta_2 - \theta_1\)</span>, so we don’t have
compelling evidence that the two biases are different based on this small data set.</p>
</div>
</div>
<div id="mcmc-posterior-sampling-big-picture" class="section level2">
<h2><span class="header-section-number">7.7</span> MCMC posterior sampling: Big picture</h2>
<div id="mcmc-markov-chain-monte-carlo" class="section level3">
<h3><span class="header-section-number">7.7.1</span> MCMC = Markov chain Monte Carlo</h3>
<ul>
<li>Markov chain because the process is a Markov process with probabilities of
transitioning from one state to another</li>
<li>Monte Carlo because it inovles randomness. (Monte Carlo is a famous casino location.)</li>
</ul>
<p>We will refer to one random walk starting from a pariticular starting
value as a <strong>chain</strong>.</p>
</div>
<div id="posterior-sampling-random-walk-through-the-posterior" class="section level3">
<h3><span class="header-section-number">7.7.2</span> Posterior sampling: Random walk through the posterior</h3>
<p>Our goal, whether we use Metropolis, Gibbs sampling, Stan, or some other MCMC
posterior samping method, is to generate random values from the posterior
distribution. Ideally these samples should be</p>
<ul>
<li><p><strong>representative</strong> of the posterior and not of other artifacts like the starting
location of the chain, or tuning parameters used to generate the walk.</p></li>
<li><p><strong>accurate</strong> to the posterior. If we run the algorithm multiple times, we would like
the results to be similar from run to run. (They won’t match exactly, but if they are
all giving good approximations to the same thing, then they should all close to each other.)</p></li>
<li><p><strong>efficient</strong>. The theory says that all of these methods converge to the correct posterior
distribution, but in practice, we can only do a finite run. If the sampling is not efficient
enough, our finite run might give us a distorted picture (that would eventually have been
corrected had we run things long enough).</p></li>
</ul>
<p>If we are convinced that the posterior sampling is of high enough quality, we can use the
posterior samples to answer all sorts of questions relatively easily.</p>
</div>
<div id="where-do-we-go-from-here-1" class="section level3">
<h3><span class="header-section-number">7.7.3</span> Where do we go from here?</h3>
<ol style="list-style-type: decimal">
<li><p>Learn to design more interesting models to answer more interesting questions.</p></li>
<li><p>Learn to describe these models and hand them to JAGS or Stan for posterior
sampling.</p></li>
<li><p>Learn to diagnose posterior samples to detect potential problems with the
posterior sampling.</p></li>
<li><p>Learn how to interpret the results.</p></li>
</ol>
</div>
</div>
<div id="ch07-exercises" class="section level2">
<h2><span class="header-section-number">7.8</span> Exercises</h2>
<!-- Exercise 7.3. [Purpose: Using a multimodal prior with the Metropolis algorithm, and seeing how chains can transition across modes or get stuck within them.]  -->
<ol style="list-style-type: decimal">
<li><p>In this exercise, you will see how the Metropolis algorithm operates with a multimodal prior.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Define the function <span class="math inline">\(p(\theta) = (cos(4 \pi \theta) + 1)^2/1.5\)</span>
in R.</p></li>
<li><p>Use <code>gf_function()</code> to plot <span class="math inline">\(p(\theta)\)</span> on the interval from
0 to 1. [Hint: Use the <code>xlim</code> argument.]</p></li>
<li><p>Use <code>integrate()</code> to confirm that <span class="math inline">\(p\)</span> is a pdf.</p></li>
<li><p>Run <code>metro_bern()</code> with <span class="math inline">\(p\)</span> as your prior,
with no data (<code>x = 0</code>, <code>n = 0</code>), and with <code>size = 0.2</code>.
Plot the posterior distribution of <span class="math inline">\(\theta\)</span> and explain
why it looks the way it does.</p></li>
<li><p>Now create a posterior histogram or density plot using
<code>x = 2</code>, <code>n = 3</code>. Do the results look reasonable? Explain.</p></li>
<li><p>Now create a posterior histogram or density plot
with <code>x = 1</code>, <code>n = 3</code>, and <code>size = 0.02</code>.
Comment on how this compares to plot you made in the previous item.</p></li>
<li><p>Repeat the previous two items but with <code>start = 0.15</code>
and <code>start = 0.95</code>. How does this help explain what is happening?
Why is it good practice to run MCMC algorithms with several
different starting values as part of the diagnositc process?</p></li>
<li><p>How would looking at trace plots from multiple starting points
help you detect this problem? (What would the trace plots look like
when things are good? What would they look like when things are bad?)</p></li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="jags-just-another-gibbs-sampler.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Redoing.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
