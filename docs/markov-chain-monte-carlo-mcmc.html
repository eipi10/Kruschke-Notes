<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>7 Markov Chain Monte Carlo (MCMC) | (Re)Doing Bayesain Data Analysis</title>
  <meta name="description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="7 Markov Chain Monte Carlo (MCMC) | (Re)Doing Bayesain Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Markov Chain Monte Carlo (MCMC) | (Re)Doing Bayesain Data Analysis" />
  
  <meta name="twitter:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

<meta name="author" content="R Pruim">


<meta name="date" content="2019-02-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html">
<link rel="next" href="jags-just-another-gibbs-sampler.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">(Re)Doing Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What’s in These Notes</a></li>
<li class="part"><span><b>I The Basics: Models, Probability, Bayes, and R</b></span></li>
<li class="chapter" data-level="2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html"><i class="fa fa-check"></i><b>2</b> Credibility, Models, and Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#the-steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.1</b> The Steps of Bayesian Data Analysis</a><ul>
<li class="chapter" data-level="2.1.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-code"><i class="fa fa-check"></i><b>2.1.1</b> R code</a></li>
<li class="chapter" data-level="2.1.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-packages"><i class="fa fa-check"></i><b>2.1.2</b> R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-1-which-coin-is-it"><i class="fa fa-check"></i><b>2.2</b> Example 1: Which coin is it?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#freedom-of-choice"><i class="fa fa-check"></i><b>2.2.1</b> Freedom of choice</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#distribtions"><i class="fa fa-check"></i><b>2.3</b> Distribtions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#beta-distributions"><i class="fa fa-check"></i><b>2.3.1</b> Beta distributions</a></li>
<li class="chapter" data-level="2.3.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#normal-distributions"><i class="fa fa-check"></i><b>2.3.2</b> Normal distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-2-height-vs-weight"><i class="fa fa-check"></i><b>2.4</b> Example 2: Height vs Weight</a><ul>
<li class="chapter" data-level="2.4.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#data"><i class="fa fa-check"></i><b>2.4.1</b> Data</a></li>
<li class="chapter" data-level="2.4.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#describing-a-model-for-the-relationship-between-height-and-weight"><i class="fa fa-check"></i><b>2.4.2</b> Describing a model for the relationship between height and weight</a></li>
<li class="chapter" data-level="2.4.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#prior"><i class="fa fa-check"></i><b>2.4.3</b> Prior</a></li>
<li class="chapter" data-level="2.4.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior"><i class="fa fa-check"></i><b>2.4.4</b> Posterior</a></li>
<li class="chapter" data-level="2.4.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior-predictive-check"><i class="fa fa-check"></i><b>2.4.5</b> Posterior Predictive Check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#where-do-we-go-from-here"><i class="fa fa-check"></i><b>2.5</b> Where do we go from here?</a></li>
<li class="chapter" data-level="2.6" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#footnotes"><i class="fa fa-check"></i><b>2.7</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html"><i class="fa fa-check"></i><b>3</b> Some Useful Bits of R</a><ul>
<li class="chapter" data-level="3.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#you-gotta-have-style"><i class="fa fa-check"></i><b>3.1</b> You Gotta Have Style</a><ul>
<li class="chapter" data-level="3.1.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#an-additional-note-about-homwork"><i class="fa fa-check"></i><b>3.1.1</b> An additional note about homwork</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors-lists-and-data-frames"><i class="fa fa-check"></i><b>3.2</b> Vectors, Lists, and Data Frames</a><ul>
<li class="chapter" data-level="3.2.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors"><i class="fa fa-check"></i><b>3.2.1</b> Vectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#lists"><i class="fa fa-check"></i><b>3.2.2</b> Lists</a></li>
<li class="chapter" data-level="3.2.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#data-frames-for-rectangular-data"><i class="fa fa-check"></i><b>3.2.3</b> Data frames for rectangular data</a></li>
<li class="chapter" data-level="3.2.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#other-types-of-data"><i class="fa fa-check"></i><b>3.2.4</b> Other types of data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#plotting-with-ggformula"><i class="fa fa-check"></i><b>3.3</b> Plotting with ggformula</a></li>
<li class="chapter" data-level="3.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#creating-data-with-expand.grid"><i class="fa fa-check"></i><b>3.4</b> Creating data with expand.grid()</a></li>
<li class="chapter" data-level="3.5" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#transforming-and-summarizing-data-dplyr-and-tidyr"><i class="fa fa-check"></i><b>3.5</b> Transforming and summarizing data dplyr and tidyr</a></li>
<li class="chapter" data-level="3.6" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#writing-functions"><i class="fa fa-check"></i><b>3.6</b> Writing Functions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#why-write-functions"><i class="fa fa-check"></i><b>3.6.1</b> Why write functions?</a></li>
<li class="chapter" data-level="3.6.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#function-parts"><i class="fa fa-check"></i><b>3.6.2</b> Function parts</a></li>
<li class="chapter" data-level="3.6.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#the-function-function-has-its-function"><i class="fa fa-check"></i><b>3.6.3</b> The function() function has its function</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
<li class="chapter" data-level="3.8" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#footnotes-1"><i class="fa fa-check"></i><b>3.8</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#some-terminology"><i class="fa fa-check"></i><b>4.1</b> Some terminology</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>4.2</b> Distributions in R</a><ul>
<li class="chapter" data-level="4.2.1" data-path="probability.html"><a href="probability.html#example-normal-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Example: Normal distributions</a></li>
<li class="chapter" data-level="4.2.2" data-path="probability.html"><a href="probability.html#simulating-running-proportions"><i class="fa fa-check"></i><b>4.2.2</b> Simulating running proportions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#joint-marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>4.3</b> Joint, marginal, and conditional distributions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="probability.html"><a href="probability.html#example-hair-and-eye-color"><i class="fa fa-check"></i><b>4.3.1</b> Example: Hair and eye color</a></li>
<li class="chapter" data-level="4.3.2" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>4.3.2</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#exercises-2"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#footnotes-2"><i class="fa fa-check"></i><b>4.5</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html"><i class="fa fa-check"></i><b>5</b> Bayes’ Rule and the Grid Method</a><ul>
<li class="chapter" data-level="5.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#the-big-baysian-idea"><i class="fa fa-check"></i><b>5.1</b> The Big Baysian Idea</a></li>
<li class="chapter" data-level="5.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#estimating-the-bias-in-a-coin-using-the-grid-method"><i class="fa fa-check"></i><b>5.2</b> Estimating the bias in a coin using the Grid Method</a><ul>
<li class="chapter" data-level="5.2.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#creating-a-grid"><i class="fa fa-check"></i><b>5.2.1</b> Creating a Grid</a></li>
<li class="chapter" data-level="5.2.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#hdi-from-the-grid"><i class="fa fa-check"></i><b>5.2.2</b> HDI from the grid</a></li>
<li class="chapter" data-level="5.2.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#automating-the-grid"><i class="fa fa-check"></i><b>5.2.3</b> Automating the grid</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#exercises-3"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Inferring a Binomial Probability</b></span></li>
<li class="chapter" data-level="6" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><i class="fa fa-check"></i><b>6</b> Inferring a Binomial Probability via Exact Mathematical Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-distributions-1"><i class="fa fa-check"></i><b>6.1</b> Beta distributions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#you-oughta-be-in-pictures"><i class="fa fa-check"></i><b>6.1.1</b> You oughta be in pictures</a></li>
<li class="chapter" data-level="6.1.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#important-facts"><i class="fa fa-check"></i><b>6.1.2</b> Important facts</a></li>
<li class="chapter" data-level="6.1.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#alternative-parametizations"><i class="fa fa-check"></i><b>6.1.3</b> Alternative Parametizations</a></li>
<li class="chapter" data-level="6.1.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta_params"><i class="fa fa-check"></i><b>6.1.4</b> beta_params()</a></li>
<li class="chapter" data-level="6.1.5" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#automating-bayesian-updates-for-a-proportion-beta-prior"><i class="fa fa-check"></i><b>6.1.5</b> Automating Bayesian updates for a proportion (beta prior)</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#what-if-the-prior-isnt-a-beta-distribution"><i class="fa fa-check"></i><b>6.2</b> What if the prior isn’t a beta distribution?</a></li>
<li class="chapter" data-level="6.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#exercises-4"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html"><i class="fa fa-check"></i><b>7</b> Markov Chain Monte Carlo (MCMC)</a><ul>
<li class="chapter" data-level="7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#king-markov-and-advisor-metropolis"><i class="fa fa-check"></i><b>7.1</b> King Markov and Advisor Metropolis</a></li>
<li class="chapter" data-level="7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#quick-intro-to-markov-chains"><i class="fa fa-check"></i><b>7.2</b> Quick Intro to Markov Chains</a><ul>
<li class="chapter" data-level="7.2.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#more-info-please"><i class="fa fa-check"></i><b>7.2.1</b> More info, please</a></li>
<li class="chapter" data-level="7.2.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#definition"><i class="fa fa-check"></i><b>7.2.2</b> Definition</a></li>
<li class="chapter" data-level="7.2.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#time-homogeneous-markov-chains"><i class="fa fa-check"></i><b>7.2.3</b> Time-Homogeneous Markov Chains</a></li>
<li class="chapter" data-level="7.2.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#matrix-representation"><i class="fa fa-check"></i><b>7.2.4</b> Matrix representation</a></li>
<li class="chapter" data-level="7.2.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#regular-markov-chains"><i class="fa fa-check"></i><b>7.2.5</b> Regular Markov Chains</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#back-to-king-markov"><i class="fa fa-check"></i><b>7.3</b> Back to King Markov</a></li>
<li class="chapter" data-level="7.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#how-well-does-the-metropolis-algorithm-work"><i class="fa fa-check"></i><b>7.4</b> How well does the Metropolis Algorithm work?</a><ul>
<li class="chapter" data-level="7.4.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-to-any-island"><i class="fa fa-check"></i><b>7.4.1</b> Jumping to any island</a></li>
<li class="chapter" data-level="7.4.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-only-to-neighbor-islands"><i class="fa fa-check"></i><b>7.4.2</b> Jumping only to neighbor islands</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#markov-chains-and-posterior-sampling"><i class="fa fa-check"></i><b>7.5</b> Markov Chains and Posterior Sampling</a><ul>
<li class="chapter" data-level="7.5.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#estimating-mean-and-variance"><i class="fa fa-check"></i><b>7.5.1</b> Estimating mean and variance</a></li>
<li class="chapter" data-level="7.5.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#assessing-how-well-the-algorithm-worked-and-how-to-tune"><i class="fa fa-check"></i><b>7.5.2</b> Assessing how well the Algorithm worked, and how to tune</a></li>
<li class="chapter" data-level="7.5.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#issues-with-metropolis-algorithm"><i class="fa fa-check"></i><b>7.5.3</b> Issues with Metropolis Algorithm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html"><i class="fa fa-check"></i><b>8</b> JAGS – Just Another Gibbs Sampler</a><ul>
<li class="chapter" data-level="8.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#what-jags-is"><i class="fa fa-check"></i><b>8.1</b> What JAGS is</a></li>
<li class="chapter" data-level="8.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#a-complete-example-estimating-a-proportion"><i class="fa fa-check"></i><b>8.2</b> A Complete Example: estimating a proportion</a><ul>
<li class="chapter" data-level="8.2.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model"><i class="fa fa-check"></i><b>8.2.1</b> The Model</a></li>
<li class="chapter" data-level="8.2.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#load-data"><i class="fa fa-check"></i><b>8.2.2</b> Load Data</a></li>
<li class="chapter" data-level="8.2.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#specify-the-model"><i class="fa fa-check"></i><b>8.2.3</b> Specify the model</a></li>
<li class="chapter" data-level="8.2.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#run-the-model"><i class="fa fa-check"></i><b>8.2.4</b> Run the model</a></li>
<li class="chapter" data-level="8.2.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-coda"><i class="fa fa-check"></i><b>8.2.5</b> Using coda</a></li>
<li class="chapter" data-level="8.2.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-bayesplot"><i class="fa fa-check"></i><b>8.2.6</b> Using bayesplot</a></li>
<li class="chapter" data-level="8.2.7" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-kruschkes-functions"><i class="fa fa-check"></i><b>8.2.7</b> Using Kruschke’s functions</a></li>
<li class="chapter" data-level="8.2.8" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#optional-arguments-to-jags"><i class="fa fa-check"></i><b>8.2.8</b> Optional arguments to jags()</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-2-comparing-two-proportions"><i class="fa fa-check"></i><b>8.3</b> Example 2: comparing two proportions</a><ul>
<li class="chapter" data-level="8.3.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-data"><i class="fa fa-check"></i><b>8.3.1</b> The data</a></li>
<li class="chapter" data-level="8.3.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-1"><i class="fa fa-check"></i><b>8.3.2</b> The model</a></li>
<li class="chapter" data-level="8.3.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#describing-the-model-to-jags"><i class="fa fa-check"></i><b>8.3.3</b> Describing the model to JAGS</a></li>
<li class="chapter" data-level="8.3.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#fitting-the-model"><i class="fa fa-check"></i><b>8.3.4</b> Fitting the model</a></li>
<li class="chapter" data-level="8.3.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#inspecting-the-results"><i class="fa fa-check"></i><b>8.3.5</b> Inspecting the results</a></li>
<li class="chapter" data-level="8.3.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#differnce-in-proportions"><i class="fa fa-check"></i><b>8.3.6</b> Differnce in proportions</a></li>
<li class="chapter" data-level="8.3.7" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#sampling-from-the-prior"><i class="fa fa-check"></i><b>8.3.7</b> Sampling from the prior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="heierarchical-models.html"><a href="heierarchical-models.html"><i class="fa fa-check"></i><b>9</b> Heierarchical Models</a><ul>
<li class="chapter" data-level="9.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#one-coin-from-one-mint"><i class="fa fa-check"></i><b>9.1</b> One coin from one mint</a></li>
<li class="chapter" data-level="9.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-one-mint"><i class="fa fa-check"></i><b>9.2</b> Multiple coins from one mint</a></li>
<li class="chapter" data-level="9.3" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-multiple-mints"><i class="fa fa-check"></i><b>9.3</b> Multiple coins from multiple mints</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">(Re)Doing Bayesain Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="markov-chain-monte-carlo-mcmc" class="section level1">
<h1><span class="header-section-number">7</span> Markov Chain Monte Carlo (MCMC)</h1>
<div id="king-markov-and-advisor-metropolis" class="section level2">
<h2><span class="header-section-number">7.1</span> King Markov and Advisor Metropolis</h2>
<p>King Markov is king of a chain of 5 islands. Rather than live in a palace, he lives in
a royal boat. Each night the royal boat anchors in the harbor of one of the islands.
The law declares that the king must harbor at each island in proportion to the population
of the island.</p>
<p><strong>Example:</strong> if the populations of the islands are 100, 200, 300, 400, and 500 people,
how often must King Markov harbor at each island?</p>
<p>King Markov has some personality quirks:</p>
<ul>
<li><p>He can’t stand record keeping. So he doesn’t know the poplations on his islands
and doesn’t keep track of which islands he has visited when.</p></li>
<li><p>He can’t stand routine (variety is the spice of his life),
so he doesn’t want to know each night where he will be the next night.</p></li>
</ul>
<p>He asks Advisor Metropolis to devise a way for him to obey the law but that</p>
<ul>
<li>randomly picks which island to stay at each night</li>
<li>doesn’t require him to remember where he has been in the past</li>
<li>doesn’t require him to remember the populations of all the islands</li>
</ul>
<p>He can ask the clerk on any island what their population is whenever he needs to know.
But it takes half a day to sail from one island to another, so he is limited in
how much information he can obtain this way each day.</p>
<p>Metropolis devises the following scheme:</p>
<ul>
<li><p>Each morning, randomly pick one of the 4 other islands (a proposal island) and travel
there in the morning, inquiring about the population over lunch.</p>
<ul>
<li><p>Let <span class="math inline">\(J(b \mid a)\)</span> be the conditional probability of selecting island <span class="math inline">\(b\)</span> as the
candidate if <span class="math inline">\(a\)</span> is the current island.</p></li>
<li><p><span class="math inline">\(J\)</span> does not depend on the populations of the islands (since the King can’t
remember them).</p></li>
</ul></li>
<li><p>Before leaving, enquire about the population of the current island.</p></li>
<li><p>When arriving at the proposal island, equire about its population.</p>
<ul>
<li><p>If the proposal island has more people, stay at the proposal island for the night
(since the king should prefer more populated islands).</p></li>
<li><p>If the proposal island has fewer people, stay at the proposal island with
probabilty <span class="math inline">\(R\)</span>, else return to the “current” island (ie, last night’s island).</p></li>
</ul></li>
</ul>
<p>Metropolis is convinced that for the right choices of <span class="math inline">\(J\)</span> and <span class="math inline">\(R\)</span>, this will satify the law.</p>
<p>He quickly determines that <span class="math inline">\(R\)</span> cannot be 0 and cannot be 1:</p>
<ul>
<li><p>What happens if <span class="math inline">\(R = 1\)</span>?</p></li>
<li><p>What happens if <span class="math inline">\(R = 0\)</span>?</p></li>
</ul>
<p>Somehow <span class="math inline">\(R\)</span> must depend on the populations of the current and proposal islands.
But how? If <span class="math inline">\(R\)</span> is too large, the king will visit small islands too often. If
<span class="math inline">\(R\)</span> is too small, he will visit large islands too often.</p>
<p>Fortunately, Metropolis knows about Markov Chains. Unfortunately, some of you may not.
So let’s learn a little bit about Markov Chains and then figure out how Metropolis
should choose <span class="math inline">\(J\)</span> and <span class="math inline">\(R\)</span>.</p>
</div>
<div id="quick-intro-to-markov-chains" class="section level2">
<h2><span class="header-section-number">7.2</span> Quick Intro to Markov Chains</h2>
<div id="more-info-please" class="section level3">
<h3><span class="header-section-number">7.2.1</span> More info, please</h3>
<p>This is going to be very quick.<br />
You can learn more, if you are interested, by going to</p>
<ul>
<li><a href="https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf">https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf</a></li>
</ul>
</div>
<div id="definition" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Definition</h3>
<p>Consider a random process that proceeds in discrete steps (often referred to as time).
Let <span class="math inline">\(X_t\)</span> represent the “state” of the process at time <span class="math inline">\(t\)</span>. Since this is a
random process, <span class="math inline">\(X_t\)</span> is random, and we can ask probability questions like
``What is the probability of being in state ____ at time ____?&quot;, ie, What is <span class="math inline">\(P(X_t = x)\)</span>?</p>
<p>If
<span class="math display">\[
P(X_{t+1} = x \mid X_t = x_t, X_{t-1} = x_{t-1}, \dots , X_0 = x_0) = P(X_{t+1} \mid X_{t} = x_t)
\]</span>
then we say that the process is a <strong>Markov Chain</strong>. The intuition is that
(the probabilities of) what happens next depends only on the current state and not on
previous history.</p>
</div>
<div id="time-homogeneous-markov-chains" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Time-Homogeneous Markov Chains</h3>
<p>The simplest version of a Markov Chain is one that is <strong>time-homogeneous</strong>:</p>
<p><span class="math display">\[
P(X_{t+1} = x_j \mid X_t = x_i) = p_{ij}
\]</span>
That is, the probability of moving from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> is one step is the
same at every step.</p>
</div>
<div id="matrix-representation" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Matrix representation</h3>
<p>A time-homogeneous Markov Chain can be represented my a square matrix <span class="math inline">\(M\)</span> with</p>
<p><span class="math display">\[
M_{ij} = p_{ij} = \mbox{probability of transition from state $i$ to state $j$ in one step}
\]</span>
(This will be an infinite matrix if the state space in infinite,
but we’ll start with simple examples with small, finite state spaces.)
<span class="math inline">\(M_{ij}\)</span> is the probability of moving in one step from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>.<br />
More generally, we will write <span class="math inline">\(M^{(k)}_{ij}\)</span> for the probability of moving
from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in <span class="math inline">\(k\)</span> steps.</p>
<p><strong>Small Example:</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">M &lt;-<span class="st"> </span><span class="kw">rbind</span>( <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>), <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>))
M</code></pre>
<table>
<tbody>
<tr class="odd">
<td align="right">0.00</td>
<td align="right">0.50</td>
<td align="right">0.5</td>
</tr>
<tr class="even">
<td align="right">0.25</td>
<td align="right">0.25</td>
<td align="right">0.5</td>
</tr>
<tr class="odd">
<td align="right">0.50</td>
<td align="right">0.30</td>
<td align="right">0.2</td>
</tr>
</tbody>
</table>
<ul>
<li>How many states does this process have?</li>
<li>What is the probability of moving from state 1 to state 3 in 1 step?</li>
<li>What is the probability of moving from state 1 to state 3 in 2 steps? (Hint:
what are the possible stopping points along the way?)</li>
<li>How do we obtain <span class="math inline">\(M^{(2)}\)</span> from <span class="math inline">\(M\)</span>?</li>
<li>How do we obtain <span class="math inline">\(M^{(k)}\)</span> from <span class="math inline">\(M\)</span>?</li>
</ul>
<p><strong>The Metropolis Algorithm:</strong></p>
<ul>
<li>What are the states of the Metropolis algorithm?</li>
<li>If King Markov is on island 2, what is the probability of moving to Island 3?</li>
<li>If King Markov is on island 3, what is the probability of moving to Island 2?</li>
<li>What is the general formula for the transition from island <span class="math inline">\(a\)</span> to island <span class="math inline">\(b\)</span>?
(<span class="math inline">\(P(X_{t+1}=b \mid X_t = a)\)</span>)</li>
</ul>
</div>
<div id="regular-markov-chains" class="section level3">
<h3><span class="header-section-number">7.2.5</span> Regular Markov Chains</h3>
<p>A time-homegeneous Markov Chain, is called regular if there is a number <span class="math inline">\(k\)</span> such that</p>
<ul>
<li>every state is reachable from every other state with non-zero probability in <span class="math inline">\(k\)</span>
steps</li>
</ul>
<p><strong>Small Example:</strong></p>
<ul>
<li>Is our small example regular? How many steps are required?</li>
</ul>
<p><strong>Metropolis Algorithm:</strong></p>
<ul>
<li>Under what conditions is the Metropolis algorithm regular?</li>
</ul>
<p>Regular Markov Chains have a very nice property:</p>
<p><span class="math display">\[
\lim_{k \to \infty} M^{(k)} = W
\]</span>
where every row of <span class="math inline">\(W\)</span> is the same. This says that, no matter where
you start the process, the long-run probability of being in each state
will be the same.</p>
<p>In our example above, convergence is quite rapid:</p>
<pre class="sourceCode r"><code class="sourceCode r">M <span class="op">%^%</span><span class="st"> </span><span class="dv">20</span></code></pre>
<table>
<tbody>
<tr class="odd">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
<tr class="even">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
<tr class="odd">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r">M <span class="op">%^%</span><span class="st"> </span><span class="dv">21</span></code></pre>
<table>
<tbody>
<tr class="odd">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
<tr class="even">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
<tr class="odd">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
</tbody>
</table>
<p>Note: If we apply the matrix <span class="math inline">\(M\)</span> to the limiting probability
(<span class="math inline">\(w\)</span>, one row of <span class="math inline">\(W\)</span>), we just get <span class="math inline">\(w\)</span> back again:</p>
<p><span class="math display">\[w M = w\]</span></p>
<pre class="sourceCode r"><code class="sourceCode r">W &lt;-<span class="st"> </span>M <span class="op">%^%</span><span class="st"> </span><span class="dv">30</span>
W[<span class="dv">1</span>,]</code></pre>
<pre><code>## [1] 0.2769 0.3385 0.3846</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">W[<span class="dv">1</span>,] <span class="op">%*%</span><span class="st"> </span>M</code></pre>
<table>
<tbody>
<tr class="odd">
<td align="right">0.2769</td>
<td align="right">0.3385</td>
<td align="right">0.3846</td>
</tr>
</tbody>
</table>
<p>In fact, this is a necessary and sufficient condition for the limiting probability.</p>
<p>So, here’s what Metropolis needs to do: Choose <span class="math inline">\(J\)</span> and <span class="math inline">\(R\)</span> so that</p>
<ul>
<li>his algorithm is a regular Markov Chain with matrix <span class="math inline">\(M\)</span></li>
<li>If <span class="math inline">\(w = \langle f(1), f(2), f(3), f(4), f(5) \rangle\)</span> is the law-prescribed probabilities for island harboring, then <span class="math inline">\(w M = w\)</span>.</li>
</ul>
</div>
</div>
<div id="back-to-king-markov" class="section level2">
<h2><span class="header-section-number">7.3</span> Back to King Markov</h2>
<p>If <span class="math inline">\(R\)</span> is betwen 0 and 1, and the jumping rule allows us to get to all the
islands (eventaully), then the Markov Chain will be regular, so there will be a
limiting distribution. But the limiting distribution must be the one the law
requires. It suffices to show that if the law is satisfied at time <span class="math inline">\(t\)</span> it is
satified at time <span class="math inline">\(t+1\)</span> (<span class="math inline">\(wM = w\)</span>):</p>
<p><span class="math display">\[ 
P(X_t = a) = f(a) \mbox{ for all $a$ } \Rightarrow P(X_{t+1} = a) = f(a) \mbox{ for all $a$}
\]</span></p>
<p>Here’s the trick: We will choose <span class="math inline">\(J\)</span> and <span class="math inline">\(R\)</span> so that
the following two <strong>unconditional</strong> probabilities are equal.</p>
<p><span class="math display">\[ P(a \to_t b) = P(b \to_t a) \]</span></p>
<p>Why does this work?</p>
<ul>
<li><p>Suppose <span class="math inline">\(P(X_t = a) = f(a)\)</span> as the law prescribes.</p></li>
<li><p><span class="math inline">\(P(a \to_t b) = P(b \to_t a)\)</span> makes the joint distribution symmetric:
For any <span class="math inline">\(a\)</span> and any <span class="math inline">\(b\)</span>.</p></li>
</ul>
<p><span class="math display">\[P(X_{t} = a, X_{t+1} = b) = P(X_{t} = b, X_{t+1} = a)\]</span></p>
<ul>
<li>This means that both marginals are the same, so for any <span class="math inline">\(a\)</span>:</li>
</ul>
<p><span class="math display">\[P(X_t = a) = P(X_{t+1} = a)\]</span></p>
<ul>
<li>In other words, the probability of the current island will be the same as the probability
of the next island: <span class="math inline">\(w M = w\)</span>.</li>
</ul>
<p>Time for some algebra (and probability)! How do we choose <span class="math inline">\(J\)</span> and <span class="math inline">\(R\)</span>? Recall the ingredients:</p>
<ul>
<li><span class="math inline">\(F(a)\)</span> be the population of island <span class="math inline">\(a\)</span></li>
<li><span class="math inline">\(f(a)\)</span> be the proportion of the total popluation living on island <span class="math inline">\(a\)</span> (<span class="math inline">\(f(a) = \frac{F(a)}{\sum_x F(x)}\)</span>)</li>
<li><span class="math inline">\(J(b \mid a)\)</span> is the conditional probability of selecting island <span class="math inline">\(b\)</span> as the candidate
when <span class="math inline">\(a\)</span> is the current island. (J for Jump probability)</li>
</ul>
<p>Consider two islands – <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> – with <span class="math inline">\(F(b) &gt; F(a)\)</span>. Let’s calculate the probabilities
of moving between the islands.</p>
<ul>
<li><p>Probability of moving from <span class="math inline">\(a\)</span> to <span class="math inline">\(b\)</span> = <span class="math inline">\(f(a) J(b \mid a) \cdot 1\)</span></p></li>
<li><p>Probability of moving from <span class="math inline">\(b\)</span> to <span class="math inline">\(a\)</span> = <span class="math inline">\(f(b) J(a \mid b) \cdot R\)</span>.</p></li>
</ul>
<p><strong>How do we choose J and R to make these probabilities equal?</strong></p>
<p>Answer:
We need to choose a jump rule (<span class="math inline">\(J()\)</span>) so that the Markov chain is regular (ie, so that
is it is possible to get from any island to any island in some fixed number of steps).
Given such a <span class="math inline">\(J()\)</span>, we can let <span class="math inline">\(R\)</span> be defined by</p>
<p><span class="math display">\[
R = \frac{f(a) J(b \mid a)}{f(b) J(a \mid b)}
\]</span>
An especially easy case is when <span class="math inline">\(J(b \mid a) = J(a \mid b)\)</span>. In that case
<span class="math display">\[
R = \frac{f(a)}{f(b)}
\]</span></p>
<p>The original Metropolis algorithm used symmetric jump rules. The later generalization
(Metropolis-Hastings) employed non-symmetric jump rules to get better performance
of the Markov Chain.</p>
</div>
<div id="how-well-does-the-metropolis-algorithm-work" class="section level2">
<h2><span class="header-section-number">7.4</span> How well does the Metropolis Algorithm work?</h2>
<pre class="sourceCode r"><code class="sourceCode r">KingMarkov &lt;-<span class="st"> </span><span class="cf">function</span>(
  <span class="dt">num_steps =</span> <span class="fl">1e5</span>,
  <span class="dt">population =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,
  <span class="dt">start =</span> <span class="dv">1</span>,
  <span class="dt">J =</span> <span class="cf">function</span>(a,b) <span class="dv">1</span><span class="op">/</span>(<span class="kw">length</span>(population) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)
  ) {
  
  num_islands &lt;-<span class="st"> </span><span class="kw">length</span>(population)
  island &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  current &lt;-<span class="st"> </span>start
  
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>num_steps) {
    <span class="co"># record current island</span>
    island[i] &lt;-<span class="st"> </span>current
    
    <span class="co"># propose any one of the other islands</span>
    other_islands &lt;-<span class="st"> </span><span class="kw">setdiff</span>(<span class="dv">1</span><span class="op">:</span>num_islands, current)
    proposal &lt;-<span class="st"> </span>
<span class="st">      </span><span class="kw">sample</span>(other_islands, <span class="dv">1</span>, 
             <span class="dt">prob =</span> purrr<span class="op">::</span><span class="kw">map</span>(other_islands, <span class="op">~</span><span class="st"> </span><span class="kw">J</span>(current, .x)))
    
    <span class="co"># move?</span>
    prob_move &lt;-<span class="st"> </span>population[proposal] <span class="op">/</span><span class="st"> </span>population[current]
    current &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&lt;</span><span class="st"> </span>prob_move, proposal, current)
  }
  <span class="kw">tibble</span>(
    <span class="dt">step =</span> <span class="dv">1</span><span class="op">:</span>num_steps,
    <span class="dt">island =</span> island
  )
}</code></pre>
<div id="jumping-to-any-island" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Jumping to any island</h3>
<pre class="sourceCode r"><code class="sourceCode r">Tour &lt;-<span class="st"> </span><span class="kw">KingMarkov</span>(<span class="dv">5000</span>) 
Target &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">island =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">prop =</span> (<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)<span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))
<span class="kw">gf_line</span>(island <span class="op">~</span><span class="st"> </span>step, <span class="dt">data =</span> Tour <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">&lt;</span><span class="st"> </span><span class="dv">200</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-any-island-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dhistogram</span>( <span class="op">~</span><span class="st"> </span>island, <span class="dt">data =</span> Tour, <span class="dt">binwidth =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(prop <span class="op">~</span><span class="st"> </span>island, <span class="dt">data =</span> Target, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-any-island-2.png" width="672" /></p>
</div>
<div id="jumping-only-to-neighbor-islands" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Jumping only to neighbor islands</h3>
<pre class="sourceCode r"><code class="sourceCode r">neighbor &lt;-<span class="st"> </span><span class="cf">function</span>(a, b) <span class="kw">as.numeric</span>(<span class="kw">abs</span>(a<span class="op">-</span>b) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">9</span>))
Tour &lt;-<span class="st"> </span><span class="kw">KingMarkov</span>(<span class="dv">5000</span>, <span class="dt">J =</span> neighbor) 
Target &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">island =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">prop =</span> (<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)<span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))
<span class="kw">gf_line</span>(island <span class="op">~</span><span class="st"> </span>step, <span class="dt">data =</span> Tour <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">&lt;</span><span class="st"> </span><span class="dv">200</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-neighbor-island-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dhistogram</span>( <span class="op">~</span><span class="st"> </span>island, <span class="dt">data =</span> Tour, <span class="dt">binwidth =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_point</span>(prop <span class="op">~</span><span class="st"> </span>island, <span class="dt">data =</span> Target, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-neighbor-island-2.png" width="672" /></p>
</div>
</div>
<div id="markov-chains-and-posterior-sampling" class="section level2">
<h2><span class="header-section-number">7.5</span> Markov Chains and Posterior Sampling</h2>
<p>That was a nice story, and some nice probability theory. But what does it have
to do with Bayesian computation?
Regular Markov Chains (and some generalizations of them) can be used to sample
from a posterior distribution:</p>
<ul>
<li>state = island = set of parameter values
<ul>
<li>in typical applications, this will be an infinite state space</li>
</ul></li>
<li>population = prior * likelihood
<ul>
<li>importantly, we do not need to normalize the posterior; that would typically
be a very computationally expensive thing to do</li>
</ul></li>
<li>start in any island = start at any parameter values
<ul>
<li>convergence may be faster from some starting states than from others,
but in princple, any state will do</li>
</ul></li>
<li>randomly choose a proposal island = randomly select a proposal set of parameter values
<ul>
<li>if the posterior is greater there, move</li>
<li>if the posterior is smaller, move anyway with probability</li>
</ul></li>
</ul>
<p><span class="math display">\[R = \frac{\mbox{proposal &quot;posterior&quot;}}{\mbox{current &quot;posterior&quot;}}\]</span></p>
<p>Metropolis-Hastings variation:</p>
<ul>
<li>More choices for <span class="math inline">\(J()\)</span> gives more opportunity to tune for convergence</li>
</ul>
<p>Other variations:</p>
<ul>
<li>Can allow <span class="math inline">\(M\)</span> to change over the course of the algorithm. (No longer
time-homonegeous.)</li>
</ul>
<div id="estimating-mean-and-variance" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Estimating mean and variance</h3>
<p>Consider the following simple model:</p>
<ul>
<li><span class="math inline">\(y \sim Norm(\mu, \sigma)\)</span></li>
<li><span class="math inline">\(\mu \sim Norm(0, 1)\)</span></li>
<li><span class="math inline">\(\log(\sigma) \sim Norm(0,1)\)</span></li>
</ul>
<p>In this case the posterior distribution for <span class="math inline">\(\mu\)</span> can be worked out exactly
and should be normal.</p>
<p>Let’s code up our Metropolis algorithm for this situation. New stuff:</p>
<ul>
<li>we have two parameters, so we’ll use separate jumps for each and combine
<ul>
<li>we could use a jump rule based on both values together, but
we’ll keep this simple</li>
</ul></li>
<li>the state space for each parameter is infinite, so we need a new kind of jump rule
<ul>
<li>instead of sampling from a finite state space, we use <code>rnorm()</code></li>
<li>the standard deviation controls how large a step we take (on average)</li>
<li>example below uses same standard deviation for both parameters, but
we should select them individually if the parameters are on different scales</li>
</ul></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">MetropolisNorm &lt;-<span class="st"> </span><span class="cf">function</span>(
  <span class="dt">num_steps =</span> <span class="fl">1e5</span>,
  y,
  <span class="dt">size =</span> <span class="dv">1</span>,  <span class="co"># sd of jump distribution</span>
  <span class="dt">start =</span> <span class="kw">list</span>(<span class="dt">mu =</span> <span class="dv">0</span>, <span class="dt">log_sigma =</span> <span class="dv">0</span>)
  ) {
  
  mu        &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  log_sigma &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  move      &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, num_steps)  <span class="co"># trick to pre-alocate memory</span>
  mu[<span class="dv">1</span>] &lt;-<span class="st"> </span>start<span class="op">$</span>mu
  log_sigma[<span class="dv">1</span>] &lt;-<span class="st"> </span>start<span class="op">$</span>log_sigma
  move[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">TRUE</span>
 
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(num_steps<span class="dv">-1</span>)) {
    <span class="co"># head to new &quot;island&quot;</span>
    mu[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]        &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, mu[i], size)
    log_sigma[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, log_sigma[i], size)
    move[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">TRUE</span>
    
    log_post_current &lt;-<span class="st"> </span>
<span class="st">      </span><span class="kw">dnorm</span>(mu[i], <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">dnorm</span>(log_sigma[i], <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(y, mu[i], <span class="kw">exp</span>(log_sigma[i]), <span class="dt">log =</span> <span class="ot">TRUE</span>))
    log_post_proposal &lt;-<span class="st"> </span>
<span class="st">      </span><span class="kw">dnorm</span>(mu[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>], <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">dnorm</span>(log_sigma[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>], <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(y, mu[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>], <span class="kw">exp</span>(log_sigma[i<span class="op">+</span><span class="dv">1</span>]), <span class="dt">log =</span> <span class="ot">TRUE</span>))
    prob_move &lt;-<span class="st"> </span><span class="kw">exp</span>(log_post_proposal <span class="op">-</span><span class="st"> </span>log_post_current)
    
    <span class="co"># sometimes we &quot;sail back&quot;</span>
    <span class="cf">if</span> (<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&gt;</span><span class="st"> </span>prob_move) {  
      move[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">FALSE</span>
      mu[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>mu[i]
      log_sigma[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>log_sigma[i]
    } 
    
  }
  <span class="kw">tibble</span>(
    <span class="dt">step =</span> <span class="dv">1</span><span class="op">:</span>num_steps,
    <span class="dt">mu =</span> mu,
    <span class="dt">log_sigma =</span> log_sigma,
    <span class="dt">move =</span> move,
    <span class="dt">size =</span> size
  )
}</code></pre>
</div>
<div id="assessing-how-well-the-algorithm-worked-and-how-to-tune" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Assessing how well the Algorithm worked, and how to tune</h3>
<p>Let’s use the algorithm with three different size values and compare results.</p>
<pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">25</span>, <span class="dv">10</span>, <span class="dv">3</span>)
Tour1 &lt;-<span class="st"> </span><span class="kw">MetropolisNorm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">1</span>)
Tour0<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">MetropolisNorm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="fl">0.2</span>)
Tour5 &lt;-<span class="st"> </span><span class="kw">MetropolisNorm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">5</span>)
Tours &lt;-<span class="st">  </span><span class="kw">bind_rows</span>(Tour1, Tour0<span class="fl">.2</span>, Tour5)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prop</span>(<span class="op">~</span><span class="st"> </span>move <span class="op">|</span><span class="st"> </span>size, <span class="dt">data =</span> Tours)</code></pre>
<pre><code>## prop_TRUE.0.2   prop_TRUE.1   prop_TRUE.5 
##        0.5858        0.1438        0.0162</code></pre>
<div id="density-plots" class="section level4">
<h4><span class="header-section-number">7.5.2.1</span> Density plots</h4>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dens</span>( <span class="op">~</span><span class="st"> </span>mu <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Tours) </code></pre>
<p><img src="Redoing_files/figure-html/ch07-metropolis-norm-compare-plots-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dens</span>( <span class="op">~</span><span class="st"> </span>log_sigma <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Tours)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-metropolis-norm-compare-plots-2.png" width="672" /></p>
</div>
<div id="trace-plots" class="section level4">
<h4><span class="header-section-number">7.5.2.2</span> Trace plots</h4>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Tours) </code></pre>
<p><img src="Redoing_files/figure-html/ch07-trace-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(log_sigma <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Tours)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-trace-2.png" width="672" /></p>
</div>
<div id="comparing-multiple-chains" class="section level4">
<h4><span class="header-section-number">7.5.2.3</span> Comparing Multiple Chains</h4>
<p>If we run multiple chains with different starting points and different random
choices, we hope to see similar trace plots. After all, we don’t want our
analysis to be an analysis of starting points or of random choices.</p>
<pre class="sourceCode r"><code class="sourceCode r">Tour1a &lt;-<span class="st"> </span><span class="kw">MetropolisNorm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">chain =</span> <span class="st">&quot;A&quot;</span>)
Tour1b &lt;-<span class="st"> </span><span class="kw">MetropolisNorm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">chain =</span> <span class="st">&quot;B&quot;</span>)
Tour1c &lt;-<span class="st"> </span><span class="kw">MetropolisNorm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">start =</span> <span class="kw">list</span>(<span class="dt">mu =</span> <span class="dv">10</span>, <span class="dt">log_sigma =</span> <span class="dv">5</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">chain =</span> <span class="st">&quot;C&quot;</span>)
Tour1d &lt;-<span class="st"> </span><span class="kw">MetropolisNorm</span>(<span class="dt">y =</span> y, <span class="dt">num_steps =</span> <span class="dv">5000</span>, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">start =</span> <span class="kw">list</span>(<span class="dt">mu =</span> <span class="dv">10</span>, <span class="dt">log_sigma =</span> <span class="dv">5</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">chain =</span> <span class="st">&quot;D&quot;</span>)
Tours1 &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(Tour1a, Tour1b, Tour1c, Tour1d)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step, <span class="dt">color =</span> <span class="op">~</span>chain, <span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">data =</span> Tours1)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-multichain-plot-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step, <span class="dt">color =</span> <span class="op">~</span>chain, <span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">data =</span> Tours1) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gf_facet_grid</span>( chain <span class="op">~</span><span class="st"> </span>.)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-multichain-plot-2.png" width="672" /></p>
</div>
<div id="comparing-chains-to-an-ideal-chain" class="section level4">
<h4><span class="header-section-number">7.5.2.4</span> Comparing Chains to an Ideal Chain</h4>
<p>Not all posteriors are normal, but here’s what a chain would look like if the posterior is normal and there
is no correlation between draws.</p>
<pre class="sourceCode r"><code class="sourceCode r">Ideal &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">step =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5000</span>, <span class="dt">mu =</span> <span class="kw">rnorm</span>(<span class="dv">5000</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">size =</span> <span class="dv">0</span>)
<span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size, <span class="dt">data =</span> Tours <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_rows</span>(Ideal))</code></pre>
<p><img src="Redoing_files/figure-html/ch07-ideal-chain01-1.png" width="672" /></p>
<p>If the draws are correlated, then we might get more ideal behavior if we
selected only a subset – every 10th or every 20th value, for example. This is
the idea behind “effective sample size”. The effective sample size of a
correlated chain is the length of an ideal chain that contains as much
independent information as the correlated chain.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> <span class="kw">bind_rows</span>(Tours, Ideal) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_labs</span>(<span class="dt">title =</span> <span class="st">&quot;Every 10th draw&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-ideal-chain02-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(mu <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> <span class="kw">bind_rows</span>(Tours, Ideal) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(step <span class="op">%%</span><span class="st"> </span><span class="dv">20</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_labs</span>(<span class="dt">title =</span> <span class="st">&quot;Every 20th draw&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-ideal-chain02-2.png" width="672" /></p>
<p>If we thin to every 20th value, our chain with <code>size = 1</code> is looking quite
similar to the ideal chain. The chain with <code>size = 0.2</code> still moves to slowly
from place to place, and the chain with <code>size = 5</code> is still getting stuck in one
place too long. So tuning paramters will affect the effective sample size.</p>
</div>
<div id="discarding-the-first-portion-of-a-chain" class="section level4">
<h4><span class="header-section-number">7.5.2.5</span> Discarding the first portion of a chain</h4>
<p>The first portion of a chain may not work as well. This portion is typically
removed from the analysis since it is more an indication of the starting values
used than the long-run sampling from the posterior.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_line</span>(log_sigma <span class="op">~</span><span class="st"> </span>step <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Tours) </code></pre>
<p><img src="Redoing_files/figure-html/ch07-burnin-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_path</span>(log_sigma <span class="op">~</span><span class="st"> </span>mu <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>(step <span class="op">&gt;</span><span class="st"> </span><span class="dv">500</span>), <span class="dt">data =</span> Tours, <span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_density2d</span>(log_sigma <span class="op">~</span><span class="st"> </span>mu, <span class="dt">data =</span> Tours) </code></pre>
<p><img src="Redoing_files/figure-html/ch07-burnin-2.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_histogram</span>( <span class="op">~</span><span class="st"> </span>mu <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>(step <span class="op">&gt;</span><span class="st"> </span><span class="dv">500</span>) , <span class="dt">data =</span> Tours, <span class="dt">binwidth =</span> <span class="fl">0.1</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-burnin-3.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_histogram</span>( <span class="op">~</span><span class="st"> </span>log_sigma <span class="op">|</span><span class="st"> </span>size <span class="op">~</span><span class="st"> </span>(step <span class="op">&gt;</span><span class="st"> </span><span class="dv">500</span>), <span class="dt">data =</span> Tours, <span class="dt">binwidth =</span> <span class="fl">0.1</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch07-burnin-4.png" width="672" /></p>
</div>
</div>
<div id="issues-with-metropolis-algorithm" class="section level3">
<h3><span class="header-section-number">7.5.3</span> Issues with Metropolis Algorithm</h3>
<p>These are really issues with all MCMC algorithms, not just the Metropolis version:</p>
<ul>
<li>First portion of a chain might not be very good, need to discard it</li>
<li>Tuning can affect performance – how do we tune?</li>
<li>Samples are correlated – although the long-run probabilities are right,
the next stop is not independent of the current one
<ul>
<li>so our effective posterior sample size isn’t as big as it appears</li>
</ul></li>
</ul>
<p>As it turns out, we won’t use the Metropolis or Metropolis-Hastings algorithms in practice.
Insead we will use Stan, which shares many features in common with the Metropolis algorithm
but has demonsrated that it works well in a wide variety (but not all) models with better
performance. Performance improvements are especially notable when the number of parameters in
the model is large.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="jags-just-another-gibbs-sampler.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Redoing.pdf", "Redoing.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
