<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>6 Inferring a Binomial Probability via Exact Mathematical Analysis | (Re)Doing Bayesain Data Analysis</title>
  <meta name="description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="6 Inferring a Binomial Probability via Exact Mathematical Analysis | (Re)Doing Bayesain Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Inferring a Binomial Probability via Exact Mathematical Analysis | (Re)Doing Bayesain Data Analysis" />
  
  <meta name="twitter:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

<meta name="author" content="R Pruim">


<meta name="date" content="2019-03-02">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="bayes-rule-and-the-grid-method.html">
<link rel="next" href="markov-chain-monte-carlo-mcmc.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">(Re)Doing Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What’s in These Notes</a></li>
<li class="part"><span><b>I The Basics: Models, Probability, Bayes, and R</b></span></li>
<li class="chapter" data-level="2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html"><i class="fa fa-check"></i><b>2</b> Credibility, Models, and Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#the-steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.1</b> The Steps of Bayesian Data Analysis</a><ul>
<li class="chapter" data-level="2.1.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-code"><i class="fa fa-check"></i><b>2.1.1</b> R code</a></li>
<li class="chapter" data-level="2.1.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-packages"><i class="fa fa-check"></i><b>2.1.2</b> R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-1-which-coin-is-it"><i class="fa fa-check"></i><b>2.2</b> Example 1: Which coin is it?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#freedom-of-choice"><i class="fa fa-check"></i><b>2.2.1</b> Freedom of choice</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#distributions"><i class="fa fa-check"></i><b>2.3</b> Distributions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#beta-distributions"><i class="fa fa-check"></i><b>2.3.1</b> Beta distributions</a></li>
<li class="chapter" data-level="2.3.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#normal-distributions"><i class="fa fa-check"></i><b>2.3.2</b> Normal distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-2-height-vs-weight"><i class="fa fa-check"></i><b>2.4</b> Example 2: Height vs Weight</a><ul>
<li class="chapter" data-level="2.4.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#data"><i class="fa fa-check"></i><b>2.4.1</b> Data</a></li>
<li class="chapter" data-level="2.4.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#describing-a-model-for-the-relationship-between-height-and-weight"><i class="fa fa-check"></i><b>2.4.2</b> Describing a model for the relationship between height and weight</a></li>
<li class="chapter" data-level="2.4.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#prior"><i class="fa fa-check"></i><b>2.4.3</b> Prior</a></li>
<li class="chapter" data-level="2.4.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior"><i class="fa fa-check"></i><b>2.4.4</b> Posterior</a></li>
<li class="chapter" data-level="2.4.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior-predictive-check"><i class="fa fa-check"></i><b>2.4.5</b> Posterior Predictive Check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#where-do-we-go-from-here"><i class="fa fa-check"></i><b>2.5</b> Where do we go from here?</a></li>
<li class="chapter" data-level="2.6" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#ch02-exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#footnotes"><i class="fa fa-check"></i><b>2.7</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html"><i class="fa fa-check"></i><b>3</b> Some Useful Bits of R</a><ul>
<li class="chapter" data-level="3.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#style-guide"><i class="fa fa-check"></i><b>3.1</b> You Gotta Have Style</a><ul>
<li class="chapter" data-level="3.1.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#an-additional-note-about-homework"><i class="fa fa-check"></i><b>3.1.1</b> An additional note about homework</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors-lists-and-data-frames"><i class="fa fa-check"></i><b>3.2</b> Vectors, Lists, and Data Frames</a><ul>
<li class="chapter" data-level="3.2.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors"><i class="fa fa-check"></i><b>3.2.1</b> Vectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#lists"><i class="fa fa-check"></i><b>3.2.2</b> Lists</a></li>
<li class="chapter" data-level="3.2.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#data-frames-for-rectangular-data"><i class="fa fa-check"></i><b>3.2.3</b> Data frames for rectangular data</a></li>
<li class="chapter" data-level="3.2.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#other-types-of-data"><i class="fa fa-check"></i><b>3.2.4</b> Other types of data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#plotting-with-ggformula"><i class="fa fa-check"></i><b>3.3</b> Plotting with ggformula</a></li>
<li class="chapter" data-level="3.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#creating-data-with-expand.grid"><i class="fa fa-check"></i><b>3.4</b> Creating data with expand.grid()</a></li>
<li class="chapter" data-level="3.5" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#transforming-and-summarizing-data-dplyr-and-tidyr"><i class="fa fa-check"></i><b>3.5</b> Transforming and summarizing data dplyr and tidyr</a></li>
<li class="chapter" data-level="3.6" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#writing-functions"><i class="fa fa-check"></i><b>3.6</b> Writing Functions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#why-write-functions"><i class="fa fa-check"></i><b>3.6.1</b> Why write functions?</a></li>
<li class="chapter" data-level="3.6.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#function-parts"><i class="fa fa-check"></i><b>3.6.2</b> Function parts</a></li>
<li class="chapter" data-level="3.6.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#the-function-function-has-its-function"><i class="fa fa-check"></i><b>3.6.3</b> The function() function has its function</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#some-common-error-messages"><i class="fa fa-check"></i><b>3.7</b> Some common error messages</a><ul>
<li class="chapter" data-level="3.7.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#object-not-found"><i class="fa fa-check"></i><b>3.7.1</b> object not found</a></li>
<li class="chapter" data-level="3.7.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#any-message-mentioning-yaml"><i class="fa fa-check"></i><b>3.7.2</b> Any message mentioning yaml</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#ch03-exercises"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#footnotes-1"><i class="fa fa-check"></i><b>3.9</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#some-terminology"><i class="fa fa-check"></i><b>4.1</b> Some terminology</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>4.2</b> Distributions in R</a><ul>
<li class="chapter" data-level="4.2.1" data-path="probability.html"><a href="probability.html#example-normal-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Example: Normal distributions</a></li>
<li class="chapter" data-level="4.2.2" data-path="probability.html"><a href="probability.html#simulating-running-proportions"><i class="fa fa-check"></i><b>4.2.2</b> Simulating running proportions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#joint-marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>4.3</b> Joint, marginal, and conditional distributions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="probability.html"><a href="probability.html#example-hair-and-eye-color"><i class="fa fa-check"></i><b>4.3.1</b> Example: Hair and eye color</a></li>
<li class="chapter" data-level="4.3.2" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>4.3.2</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#ch04-exercises"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#footnotes-2"><i class="fa fa-check"></i><b>4.5</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html"><i class="fa fa-check"></i><b>5</b> Bayes’ Rule and the Grid Method</a><ul>
<li class="chapter" data-level="5.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#the-big-bayesian-idea"><i class="fa fa-check"></i><b>5.1</b> The Big Bayesian Idea</a><ul>
<li class="chapter" data-level="5.1.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#likelihood"><i class="fa fa-check"></i><b>5.1.1</b> Likelihood</a></li>
<li class="chapter" data-level="5.1.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#when-bayes-is-easy"><i class="fa fa-check"></i><b>5.1.2</b> When Bayes is easy</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#estimating-the-bias-in-a-coin-using-the-grid-method"><i class="fa fa-check"></i><b>5.2</b> Estimating the bias in a coin using the Grid Method</a><ul>
<li class="chapter" data-level="5.2.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#creating-a-grid"><i class="fa fa-check"></i><b>5.2.1</b> Creating a Grid</a></li>
<li class="chapter" data-level="5.2.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#hdi-from-the-grid"><i class="fa fa-check"></i><b>5.2.2</b> HDI from the grid</a></li>
<li class="chapter" data-level="5.2.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#automating-the-grid"><i class="fa fa-check"></i><b>5.2.3</b> Automating the grid</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#working-on-the-log-scale"><i class="fa fa-check"></i><b>5.3</b> Working on the log scale</a></li>
<li class="chapter" data-level="5.4" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#discrete-params"><i class="fa fa-check"></i><b>5.4</b> Discrete Parameters</a></li>
<li class="chapter" data-level="5.5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#ch05-exercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
<li class="chapter" data-level="5.6" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#footnotes-3"><i class="fa fa-check"></i><b>5.6</b> Footnotes</a></li>
</ul></li>
<li class="part"><span><b>II Inferring a Binomial Probability</b></span></li>
<li class="chapter" data-level="6" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><i class="fa fa-check"></i><b>6</b> Inferring a Binomial Probability via Exact Mathematical Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-distributions-1"><i class="fa fa-check"></i><b>6.1</b> Beta distributions</a></li>
<li class="chapter" data-level="6.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-and-bayes"><i class="fa fa-check"></i><b>6.2</b> Beta and Bayes</a><ul>
<li class="chapter" data-level="6.2.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#the-bernoulli-likelihood-function"><i class="fa fa-check"></i><b>6.2.1</b> The Bernoulli likelihood function</a></li>
<li class="chapter" data-level="6.2.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#a-convenient-prior"><i class="fa fa-check"></i><b>6.2.2</b> A convenient prior</a></li>
<li class="chapter" data-level="6.2.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#pros-and-cons-of-conjugate-priors"><i class="fa fa-check"></i><b>6.2.3</b> Pros and Cons of conjugate priors</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#getting-to-know-the-beta-distributions"><i class="fa fa-check"></i><b>6.3</b> Getting to know the Beta distributions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#important-facts"><i class="fa fa-check"></i><b>6.3.1</b> Important facts</a></li>
<li class="chapter" data-level="6.3.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#alternative-parameterizations-of-beta-distributions"><i class="fa fa-check"></i><b>6.3.2</b> Alternative parameterizations of Beta distributions</a></li>
<li class="chapter" data-level="6.3.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta_params"><i class="fa fa-check"></i><b>6.3.3</b> beta_params()</a></li>
<li class="chapter" data-level="6.3.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#automating-bayesian-updates-for-a-proportion-beta-prior"><i class="fa fa-check"></i><b>6.3.4</b> Automating Bayesian updates for a proportion (beta prior)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#what-if-the-prior-isnt-a-beta-distribution"><i class="fa fa-check"></i><b>6.4</b> What if the prior isn’t a beta distribution?</a></li>
<li class="chapter" data-level="6.5" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#ch06-exercises"><i class="fa fa-check"></i><b>6.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html"><i class="fa fa-check"></i><b>7</b> Markov Chain Monte Carlo (MCMC)</a><ul>
<li class="chapter" data-level="7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#king-markov-and-adviser-metropolis"><i class="fa fa-check"></i><b>7.1</b> King Markov and Adviser Metropolis</a></li>
<li class="chapter" data-level="7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#quick-intro-to-markov-chains"><i class="fa fa-check"></i><b>7.2</b> Quick Intro to Markov Chains</a><ul>
<li class="chapter" data-level="7.2.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#more-info-please"><i class="fa fa-check"></i><b>7.2.1</b> More info, please</a></li>
<li class="chapter" data-level="7.2.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#definition"><i class="fa fa-check"></i><b>7.2.2</b> Definition</a></li>
<li class="chapter" data-level="7.2.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#time-homogeneous-markov-chains"><i class="fa fa-check"></i><b>7.2.3</b> Time-Homogeneous Markov Chains</a></li>
<li class="chapter" data-level="7.2.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#matrix-representation"><i class="fa fa-check"></i><b>7.2.4</b> Matrix representation</a></li>
<li class="chapter" data-level="7.2.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#regular-markov-chains"><i class="fa fa-check"></i><b>7.2.5</b> Regular Markov Chains</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#back-to-king-markov"><i class="fa fa-check"></i><b>7.3</b> Back to King Markov</a></li>
<li class="chapter" data-level="7.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#how-well-does-the-metropolis-algorithm-work"><i class="fa fa-check"></i><b>7.4</b> How well does the Metropolis Algorithm work?</a><ul>
<li class="chapter" data-level="7.4.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-to-any-island"><i class="fa fa-check"></i><b>7.4.1</b> Jumping to any island</a></li>
<li class="chapter" data-level="7.4.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-only-to-neighbor-islands"><i class="fa fa-check"></i><b>7.4.2</b> Jumping only to neighbor islands</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#markov-chains-and-posterior-sampling"><i class="fa fa-check"></i><b>7.5</b> Markov Chains and Posterior Sampling</a><ul>
<li class="chapter" data-level="7.5.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#example-1-estimating-a-proportion"><i class="fa fa-check"></i><b>7.5.1</b> Example 1: Estimating a proportion</a></li>
<li class="chapter" data-level="7.5.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#example-2-estimating-mean-and-variance"><i class="fa fa-check"></i><b>7.5.2</b> Example 2: Estimating mean and variance</a></li>
<li class="chapter" data-level="7.5.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#issues-with-metropolis-algorithm"><i class="fa fa-check"></i><b>7.5.3</b> Issues with Metropolis Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#two-coins"><i class="fa fa-check"></i><b>7.6</b> Two coins</a><ul>
<li class="chapter" data-level="7.6.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#the-model"><i class="fa fa-check"></i><b>7.6.1</b> The model</a></li>
<li class="chapter" data-level="7.6.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#exact-analysis"><i class="fa fa-check"></i><b>7.6.2</b> Exact analysis</a></li>
<li class="chapter" data-level="7.6.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#metropolis"><i class="fa fa-check"></i><b>7.6.3</b> Metropolis</a></li>
<li class="chapter" data-level="7.6.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#gibbs-sampling"><i class="fa fa-check"></i><b>7.6.4</b> Gibbs sampling</a></li>
<li class="chapter" data-level="7.6.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#advantages-and-disadvantages-of-gibbs-vs-metropolis"><i class="fa fa-check"></i><b>7.6.5</b> Advantages and Disadvantages of Gibbs vs Metropolis</a></li>
<li class="chapter" data-level="7.6.6" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#so-what-do-we-learn-about-the-coins"><i class="fa fa-check"></i><b>7.6.6</b> So what do we learn about the coins?</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#mcmc-posterior-sampling-big-picture"><i class="fa fa-check"></i><b>7.7</b> MCMC posterior sampling: Big picture</a><ul>
<li class="chapter" data-level="7.7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#mcmc-markov-chain-monte-carlo"><i class="fa fa-check"></i><b>7.7.1</b> MCMC = Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="7.7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#posterior-sampling-random-walk-through-the-posterior"><i class="fa fa-check"></i><b>7.7.2</b> Posterior sampling: Random walk through the posterior</a></li>
<li class="chapter" data-level="7.7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#where-do-we-go-from-here-1"><i class="fa fa-check"></i><b>7.7.3</b> Where do we go from here?</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#ch07-exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html"><i class="fa fa-check"></i><b>8</b> JAGS – Just Another Gibbs Sampler</a><ul>
<li class="chapter" data-level="8.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#what-jags-is"><i class="fa fa-check"></i><b>8.1</b> What JAGS is</a><ul>
<li class="chapter" data-level="8.1.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#jags-documentation"><i class="fa fa-check"></i><b>8.1.1</b> JAGS documentation</a></li>
<li class="chapter" data-level="8.1.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#updating-c-and-clang"><i class="fa fa-check"></i><b>8.1.2</b> Updating C and CLANG</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-1-estimating-a-proportion-1"><i class="fa fa-check"></i><b>8.2</b> Example 1: estimating a proportion</a><ul>
<li class="chapter" data-level="8.2.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-1"><i class="fa fa-check"></i><b>8.2.1</b> The Model</a></li>
<li class="chapter" data-level="8.2.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#load-data"><i class="fa fa-check"></i><b>8.2.2</b> Load Data</a></li>
<li class="chapter" data-level="8.2.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#specify-the-model"><i class="fa fa-check"></i><b>8.2.3</b> Specify the model</a></li>
<li class="chapter" data-level="8.2.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#run-the-model"><i class="fa fa-check"></i><b>8.2.4</b> Run the model</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#extracting-information-from-a-jags-run"><i class="fa fa-check"></i><b>8.3</b> Extracting information from a JAGS run</a><ul>
<li class="chapter" data-level="8.3.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#posterior-1"><i class="fa fa-check"></i><b>8.3.1</b> posterior()</a></li>
<li class="chapter" data-level="8.3.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#side-note-posterior-sampling-and-the-grid-method"><i class="fa fa-check"></i><b>8.3.2</b> Side note: posterior sampling and the grid method</a></li>
<li class="chapter" data-level="8.3.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-coda"><i class="fa fa-check"></i><b>8.3.3</b> Using coda</a></li>
<li class="chapter" data-level="8.3.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-bayesplot"><i class="fa fa-check"></i><b>8.3.4</b> Using bayesplot</a></li>
<li class="chapter" data-level="8.3.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-kruschkes-functions"><i class="fa fa-check"></i><b>8.3.5</b> Using Kruschke’s functions</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#optional-arguments-to-jags"><i class="fa fa-check"></i><b>8.4</b> Optional arguments to jags()</a><ul>
<li class="chapter" data-level="8.4.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#number-and-size-of-chains"><i class="fa fa-check"></i><b>8.4.1</b> Number and size of chains</a></li>
<li class="chapter" data-level="8.4.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#starting-point-for-chains"><i class="fa fa-check"></i><b>8.4.2</b> Starting point for chains</a></li>
<li class="chapter" data-level="8.4.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#running-chains-in-parallel"><i class="fa fa-check"></i><b>8.4.3</b> Running chains in parallel</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-2-comparing-two-proportions"><i class="fa fa-check"></i><b>8.5</b> Example 2: comparing two proportions</a><ul>
<li class="chapter" data-level="8.5.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-data"><i class="fa fa-check"></i><b>8.5.1</b> The data</a></li>
<li class="chapter" data-level="8.5.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-2"><i class="fa fa-check"></i><b>8.5.2</b> The model</a></li>
<li class="chapter" data-level="8.5.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#describing-the-model-to-jags"><i class="fa fa-check"></i><b>8.5.3</b> Describing the model to JAGS</a></li>
<li class="chapter" data-level="8.5.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#fitting-the-model"><i class="fa fa-check"></i><b>8.5.4</b> Fitting the model</a></li>
<li class="chapter" data-level="8.5.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#inspecting-the-results"><i class="fa fa-check"></i><b>8.5.5</b> Inspecting the results</a></li>
<li class="chapter" data-level="8.5.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#difference-in-proportions"><i class="fa fa-check"></i><b>8.5.6</b> Difference in proportions</a></li>
<li class="chapter" data-level="8.5.7" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#sampling-from-the-prior"><i class="fa fa-check"></i><b>8.5.7</b> Sampling from the prior</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#ch08-exercises"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="heierarchical-models.html"><a href="heierarchical-models.html"><i class="fa fa-check"></i><b>9</b> Heierarchical Models</a><ul>
<li class="chapter" data-level="9.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#gamma-distributions"><i class="fa fa-check"></i><b>9.1</b> Gamma Distributions</a></li>
<li class="chapter" data-level="9.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#one-coin-from-one-mint"><i class="fa fa-check"></i><b>9.2</b> One coin from one mint</a></li>
<li class="chapter" data-level="9.3" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-one-mint"><i class="fa fa-check"></i><b>9.3</b> Multiple coins from one mint</a></li>
<li class="chapter" data-level="9.4" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-multiple-mints"><i class="fa fa-check"></i><b>9.4</b> Multiple coins from multiple mints</a></li>
<li class="chapter" data-level="9.5" data-path="heierarchical-models.html"><a href="heierarchical-models.html#therapeutic-touch"><i class="fa fa-check"></i><b>9.5</b> Therapeutic Touch</a><ul>
<li class="chapter" data-level="9.5.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#abstract"><i class="fa fa-check"></i><b>9.5.1</b> Abstract</a></li>
<li class="chapter" data-level="9.5.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#data-1"><i class="fa fa-check"></i><b>9.5.2</b> Data</a></li>
<li class="chapter" data-level="9.5.3" data-path="heierarchical-models.html"><a href="heierarchical-models.html#a-heierarchical-model"><i class="fa fa-check"></i><b>9.5.3</b> A heierarchical model</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="heierarchical-models.html"><a href="heierarchical-models.html#other-parameterizations-we-might-have-tried"><i class="fa fa-check"></i><b>9.6</b> Other parameterizations we might have tried</a><ul>
<li class="chapter" data-level="9.6.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#shape-parameters-for-beta"><i class="fa fa-check"></i><b>9.6.1</b> Shape parameters for Beta</a></li>
<li class="chapter" data-level="9.6.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#mean-instead-of-mode"><i class="fa fa-check"></i><b>9.6.2</b> Mean instead of mode</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="heierarchical-models.html"><a href="heierarchical-models.html#shrinkage"><i class="fa fa-check"></i><b>9.7</b> Shrinkage</a></li>
<li class="chapter" data-level="9.8" data-path="heierarchical-models.html"><a href="heierarchical-models.html#example-baseball-batting-average"><i class="fa fa-check"></i><b>9.8</b> Example: Baseball Batting Average</a></li>
<li class="chapter" data-level="9.9" data-path="heierarchical-models.html"><a href="heierarchical-models.html#ch09-exercises"><i class="fa fa-check"></i><b>9.9</b> Exerciess</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">(Re)Doing Bayesain Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inferring-a-binomial-probability-via-exact-mathematical-analysis" class="section level1">
<h1><span class="header-section-number">6</span> Inferring a Binomial Probability via Exact Mathematical Analysis</h1>

<div id="beta-distributions-1" class="section level2">
<h2><span class="header-section-number">6.1</span> Beta distributions</h2>
<p>A few important facts about beta distributions</p>
<ul>
<li><p>two parameters: <span class="math inline">\(\alpha = a =\)</span> <code>shape1</code>; <span class="math inline">\(\beta = b =\)</span> <code>shape2</code>.</p></li>
<li><p>kernel: <span class="math inline">\(x^{\alpha - 1} (1-x)^{\beta -1}\)</span> on <span class="math inline">\([0, 1]\)</span></p></li>
<li><p>area under the kernel: <span class="math inline">\(B(a, b)\)</span> [<span class="math inline">\(B()\)</span> is the beta function, <code>beta()</code> in R]</p></li>
<li><p>scaling contant: <span class="math inline">\(1 / B(a, b)\)</span></p></li>
<li><p>We can use <code>gf_dist()</code> to see what a beta distribution looks like.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="dv">5</span>, <span class="dt">shape2 =</span> <span class="dv">3</span>, <span class="dt">color =</span> <span class="op">~</span><span class="st"> &quot;Beta(5, 3)&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="dv">3</span>, <span class="dt">shape2 =</span> <span class="dv">5</span>, <span class="dt">color =</span> <span class="op">~</span><span class="st"> &quot;Beta(3, 5)&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="fl">0.9</span>, <span class="dt">shape2 =</span> <span class="fl">0.9</span>, <span class="dt">color =</span> <span class="op">~</span><span class="st"> &quot;Beta(0.9, 0.9)&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="fl">0.9</span>, <span class="dt">shape2 =</span> <span class="fl">1.1</span>, <span class="dt">color =</span> <span class="op">~</span><span class="st"> &quot;Beta(0.9, 1.1)&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_labs</span>(<span class="dt">title =</span> <span class="st">&quot;Some Beta distributions&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;distribution&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch06-beta-pics-1.png" width="65%" /></p></li>
</ul>
</div>
<div id="beta-and-bayes" class="section level2">
<h2><span class="header-section-number">6.2</span> Beta and Bayes</h2>
<p>Suppose we want to estimate a proportion <span class="math inline">\(\theta\)</span> by repeating some random
process (like a coin toss) <span class="math inline">\(N\)</span> times. We will code each result using a 0 (failure)
or a 1 (success): <span class="math inline">\(Y_1, Y_2, \dots, Y_N\)</span>.
Here’s our model.<br />
The prior, to be determined shortly, is indicated as ??? for the moment.
<span class="math display">\[\begin{align*}
Y_i &amp; \sim {\sf Bern}(\theta)
\\
\theta &amp; \sim{} ???
\end{align*}\]</span></p>
<div id="the-bernoulli-likelihood-function" class="section level3">
<h3><span class="header-section-number">6.2.1</span> The Bernoulli likelihood function</h3>
<p>The first line turns into the following likelihood function – the probability
of observing <span class="math inline">\(y_i\)</span> for a give parameter value <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[\begin{align*}
Pr{Pr}(Y_i = y_i \mid \theta) = 
p(y_i \mid \theta) 
  &amp;= \begin{cases} \theta &amp; y_i = 1 \\ (1-\theta) &amp; y_i = 0 \end{cases}
  \\
  &amp;= \theta^{y_i} (1-\theta)^{y_i}
\end{align*}\]</span></p>
<p>The likelihood for the entire data set is then
<span class="math display">\[\begin{align*}
p(\langle y_1, y_2, \dots, y_N \rangle \mid \theta) 
  &amp;= \prod_{i = 1}^N \theta^{y_i} (1-\theta)^{y_i}
  \\
  &amp;= \theta^{x} (1-\theta)^{N - x}
\end{align*}\]</span>
where <span class="math inline">\(x\)</span> is the number of “successes” and <span class="math inline">\(N\)</span> is the number of trials.
Since the likelihood only depends on <span class="math inline">\(x\)</span> and <span class="math inline">\(N\)</span>, not the particular order
in which the 0’s and 1’s are observed, we will write the likelihood as</p>
<p><span class="math display">\[\begin{align*}
p(x, N \mid \theta) 
  &amp;= \theta^{x} (1-\theta)^{N - x}
\end{align*}\]</span></p>
<p>Reminder: If we think of this expression as a function of <span class="math inline">\(\theta\)</span> for fixed data
(rather than as a function of the data for fixed <span class="math inline">\(\theta\)</span>), we see that it
is the kernel of a <span class="math inline">\({\sf Beta}(x + 1, N - x + 1)\)</span> distribution.
But even thought of this way,
the likelihood need not be a PDF – the total sum or integral need not be 1.
But we will sometimes normalize likelihood functions if we want to
display them on plots with priors and posteriors.</p>
</div>
<div id="a-convenient-prior" class="section level3">
<h3><span class="header-section-number">6.2.2</span> A convenient prior</h3>
<p>Now let think about our posterior:</p>
<p><span class="math display">\[\begin{align*}
p(\theta \mid x, N) 
 &amp; = \overbrace{p(x, N \mid \theta)}^{\mathrm{likelihood}} \cdot \overbrace{p(\theta)}^{\mathrm{prior}} / p(x, N) 
 \\
 &amp; = {\theta^x (1-\theta)^{N - x}} \cdot {p(\theta)} / p(x, N) 
\end{align*}\]</span>
If we let
<span class="math inline">\(p(\theta) = \theta^a (1-\theta^b)\)</span>, the product is epsecially easy to evaluate:
<span class="math display">\[\begin{align*}
p(\theta \mid x, N) 
 &amp; = \overbrace{p(x, N \mid \theta)}^{\mathrm{likelihood}} \cdot \overbrace{p(\theta)}^{\mathrm{prior}} / p(x, N) 
 \\
 &amp; = {\theta^x (1-\theta)^{N - x}} \cdot {\theta^a (1-\theta)^b)} / p(x, N) 
 \\
 &amp; = {\theta^{x+a} (1-\theta)^{N - x + b}} / p(x, N) 
\end{align*}\]</span>
In this happy situation, when mutlipying the likelihood and the prior leads to a
posterior with same form as the prior, we say that the prior is a
<strong>conjugate prior</strong> (for that particular likelihood function).
So beta priors are conjugate priors for the Bernoulli likelihood, and if we
use a beta prior, we will get a beta posterior and it is easy to calculate
which one:</p>
<table>
<thead>
<tr class="header">
<th align="center">prior</th>
<th align="center">data</th>
<th align="center">posterior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\sf{Beta}(a, b)\)</span></td>
<td align="center"><span class="math inline">\(x, N\)</span></td>
<td align="center"><span class="math inline">\({\sf Beta}(x + a, N - x + b)\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="pros-and-cons-of-conjugate-priors" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Pros and Cons of conjugate priors</h3>
<p><strong>Pros:</strong> Easy and fast calculation; can reason about the relationship
between prior, likelihood, and posterior based on a known distributions.</p>
<p><strong>Cons:</strong> We are restricted to using a conjugate prior, and that isn’t always
the prior we want; many situations don’t have natural conjugate priors available;
the computations are often not as simple as in our current example.</p>
</div>
</div>
<div id="getting-to-know-the-beta-distributions" class="section level2">
<h2><span class="header-section-number">6.3</span> Getting to know the Beta distributions</h2>
<div id="important-facts" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Important facts</h3>
<p>You can often look up this sort of information
on the Wikipedia page for a family of distributions.
If you go to <a href="https://en.wikipedia.org/wiki/Beta_distribution" class="uri">https://en.wikipedia.org/wiki/Beta_distribution</a>
you will find, among other things, the following:</p>
<table>
<colgroup>
<col width="15%" />
<col width="84%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th> </th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Notation</td>
<td>Beta(<span class="math inline">\(\alpha, \beta\)</span>)</td>
</tr>
<tr class="even">
<td>Parameters</td>
<td><span class="math inline">\(\alpha &gt; 0\)</span> shape (real)<br />
<span class="math inline">\(\beta &gt; 0\)</span> shape (real)</td>
</tr>
<tr class="odd">
<td>Support</td>
<td><span class="math inline">\(\displaystyle x\in [0,1]\)</span> or <span class="math inline">\(\displaystyle x\in (0,1)\)</span></td>
</tr>
<tr class="even">
<td>PDF</td>
<td><span class="math inline">\(\displaystyle \frac{x^{\alpha -1}(1-x)^{\beta -1}}{\mathrm{B}(\alpha, \beta )}\)</span></td>
</tr>
<tr class="odd">
<td>Mean</td>
<td><span class="math inline">\(\displaystyle \frac{\alpha }{\alpha +\beta }\)</span></td>
</tr>
<tr class="even">
<td>Mode</td>
<td><span class="math inline">\(\displaystyle {\frac{\alpha -1}{\alpha +\beta -2}}\)</span> for <span class="math inline">\(\alpha, \beta &gt; 1\)</span><br />
0 for <span class="math inline">\(\alpha = 1, \beta &gt; 1\)</span><br />
1 for <span class="math inline">\(\alpha &gt; 1, \beta = 1\)</span></td>
</tr>
<tr class="odd">
<td>Variance</td>
<td><span class="math inline">\(\displaystyle \frac{\alpha \beta}{(\alpha +\beta )^{2}(\alpha +\beta +1)}\)</span></td>
</tr>
<tr class="even">
<td>Concentration</td>
<td><span class="math inline">\(\displaystyle \kappa =\alpha +\beta\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="alternative-parameterizations-of-beta-distributions" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Alternative parameterizations of Beta distributions</h3>
<p>There are several different parameterizations of the
beta distributions that can be helpful in selecting
a prior or interpreting a posterior.</p>
<div id="mode-and-concentration" class="section level4">
<h4><span class="header-section-number">6.3.2.1</span> Mode and concentration</h4>
<p>Let the concentration be defined as
<span class="math inline">\(\kappa =\alpha +\beta\)</span>. Since the mode (<span class="math inline">\(\omega\)</span>) is
<span class="math inline">\(\displaystyle {\frac{\alpha -1}{\alpha +\beta -2}}\)</span> for <span class="math inline">\(\alpha, \beta &gt; 1\)</span>, we can solve for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to get</p>
<p><span class="math display">\[\begin{align}
\alpha &amp;= \omega (\kappa - 2) + 1\\
\beta  &amp;= (1-\omega)(\kappa -2) + 1
\end{align}\]</span></p>
</div>
<div id="mean-and-concentration" class="section level4">
<h4><span class="header-section-number">6.3.2.2</span> Mean and concentration</h4>
<p>The beta distribution may also be reparameterized in terms of
its mean <span class="math inline">\(\mu\)</span> and the concentration <span class="math inline">\(\kappa\)</span>.
If we solve for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, we get</p>
<p><span class="math display">\[\begin{align}
\alpha &amp;= \mu \kappa \\
\beta &amp;= (1 - \mu) \kappa
\end{align}\]</span></p>
<!-- Under this parameterization, one may place an uninformative prior probability over the mean, and a vague prior probability (such as an exponential or gamma distribution) over the positive reals for the sample size, if they are independent, and prior data and/or beliefs justify it. -->
</div>
<div id="mean-and-variance-or-standard-deviation" class="section level4">
<h4><span class="header-section-number">6.3.2.3</span> Mean and variance (or standard deviation)</h4>
<p>We can also parameterize with the mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.
Solving the system of equations for mean and variance
given in the table above, we get</p>
<p><span class="math display">\[\begin{align}
\kappa &amp;=\alpha +\beta = \frac{\mu (1-\mu )}{\sigma^2} - 1 \\
\alpha &amp;=\mu \kappa = \mu 
          \left({\frac{\mu (1-\mu )}{\sigma^2}}-1\right) \\
\beta &amp;= (1-\mu )\kappa = (1-\mu )
         \left({\frac{\mu (1-\mu)}
               {\sigma^2}}        -1 \right),
\end{align}\]</span>
provided <span class="math inline">\(\sigma^2 &lt; \mu (1-\mu)\)</span>.</p>
</div>
</div>
<div id="beta_params" class="section level3">
<h3><span class="header-section-number">6.3.3</span> beta_params()</h3>
<p><code>CalvinBayes::beta_params()</code> will compute several summaries of a beta
distribution given any of these 2-parameter summaries. This can be very handy for
converting from one type of information about a beta distribution to another.</p>
<p>For example. Suppose you want a beta distribution with mean 0.3 and standard
deviation 0.1. Which beta distribution is it?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(CalvinBayes)
<span class="kw">beta_params</span>(<span class="dt">mean =</span> <span class="fl">0.3</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">shape1</th>
<th align="right">shape2</th>
<th align="right">mean</th>
<th align="right">mode</th>
<th align="right">sd</th>
<th align="right">concentration</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">6</td>
<td align="right">14</td>
<td align="right">0.3</td>
<td align="right">0.2778</td>
<td align="right">0.1</td>
<td align="right">20</td>
</tr>
</tbody>
</table>
<p>We can do a similar thing with other combinations.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">bind_rows</span>(
  <span class="kw">beta_params</span>(<span class="dt">mean =</span> <span class="fl">0.3</span>, <span class="dt">concentration =</span> <span class="dv">10</span>),
  <span class="kw">beta_params</span>(<span class="dt">mode =</span> <span class="fl">0.3</span>, <span class="dt">concentration =</span> <span class="dv">10</span>),
  <span class="kw">beta_params</span>(<span class="dt">mean =</span> <span class="fl">0.3</span>, <span class="dt">sd =</span> <span class="fl">0.2</span>),
  <span class="kw">beta_params</span>(<span class="dt">shape1 =</span> <span class="dv">5</span>, <span class="dt">shape2 =</span> <span class="dv">10</span>),
)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">shape1</th>
<th align="right">shape2</th>
<th align="right">mean</th>
<th align="right">mode</th>
<th align="right">sd</th>
<th align="right">concentration</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">3.000</td>
<td align="right">7.000</td>
<td align="right">0.3000</td>
<td align="right">0.2500</td>
<td align="right">0.1382</td>
<td align="right">10.00</td>
</tr>
<tr class="even">
<td align="right">3.400</td>
<td align="right">6.600</td>
<td align="right">0.3400</td>
<td align="right">0.3000</td>
<td align="right">0.1428</td>
<td align="right">10.00</td>
</tr>
<tr class="odd">
<td align="right">1.275</td>
<td align="right">2.975</td>
<td align="right">0.3000</td>
<td align="right">0.1222</td>
<td align="right">0.2000</td>
<td align="right">4.25</td>
</tr>
<tr class="even">
<td align="right">5.000</td>
<td align="right">10.000</td>
<td align="right">0.3333</td>
<td align="right">0.3077</td>
<td align="right">0.1179</td>
<td align="right">15.00</td>
</tr>
</tbody>
</table>
</div>
<div id="automating-bayesian-updates-for-a-proportion-beta-prior" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Automating Bayesian updates for a proportion (beta prior)</h3>
<p>Since we have formulas for this case, we can write a function handle
any beta prior and any data set very simply. (Much simpler than doing the
grid method each time).</p>
<pre class="sourceCode r"><code class="sourceCode r">quick_bern_beta &lt;-<span class="st"> </span>
<span class="st">  </span><span class="cf">function</span>(
    x, n,     <span class="co"># data, successes and trials</span>
    ...       <span class="co"># see clever trick below</span>
  ) 
  {
    pars &lt;-<span class="st"> </span><span class="kw">beta_params</span>(...)
    a &lt;-<span class="st"> </span>pars<span class="op">$</span>shape1
    b &lt;-<span class="st"> </span>pars<span class="op">$</span>shape2
    
    theta_hat &lt;-<span class="st"> </span>x <span class="op">/</span><span class="st"> </span>n  <span class="co"># value that makes likelihood largest</span>
    posterior_mode &lt;-<span class="st"> </span>(a <span class="op">+</span><span class="st"> </span>x <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(a <span class="op">+</span><span class="st"> </span>b <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>)  
    
    <span class="co"># scale likelihood to be as tall as the posterior</span>
    likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {
      <span class="kw">dbinom</span>(x, n, theta) <span class="op">/</span><span class="st"> </span><span class="kw">dbinom</span>(x, n, theta_hat) <span class="op">*</span>
<span class="st">        </span><span class="kw">dbeta</span>(posterior_mode, a <span class="op">+</span><span class="st"> </span>x, b <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span>x)  <span class="co"># posterior height at mode</span>
    }
    
    <span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> a, <span class="dt">shape2 =</span> b, 
            <span class="dt">color =</span> <span class="op">~</span><span class="st"> &quot;prior&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">size =</span> <span class="fl">1.2</span>) <span class="op">%&gt;%</span>
<span class="st">      </span><span class="kw">gf_function</span>(likelihood, 
                  <span class="dt">color =</span> <span class="op">~</span><span class="st"> &quot;likelihood&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">size =</span> <span class="fl">1.2</span>) <span class="op">%&gt;%</span>
<span class="st">      </span><span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> a <span class="op">+</span><span class="st"> </span>x, <span class="dt">shape2 =</span> b <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span>x, 
              <span class="dt">color =</span> <span class="op">~</span><span class="st"> &quot;posterior&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">size =</span> <span class="fl">1.6</span>) <span class="op">%&gt;%</span>
<span class="st">      </span><span class="kw">gf_labs</span>(
        <span class="dt">color =</span> <span class="st">&quot;function&quot;</span>,
        <span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;posterior: Beta(&quot;</span>, a <span class="op">+</span><span class="st"> </span>x, <span class="st">&quot;, &quot;</span>, b <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span>x, <span class="st">&quot;)&quot;</span>)
      ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">gf_refine</span>(
        <span class="kw">scale_color_manual</span>(
          <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;prior&quot;</span> =<span class="st"> &quot;gray50&quot;</span>, <span class="st">&quot;likelihood&quot;</span> =<span class="st"> &quot;forestgreen&quot;</span>, 
                     <span class="st">&quot;posterior&quot;</span> =<span class="st"> &quot;steelblue&quot;</span>)))
  } </code></pre>
<p>With such a function in hand, we can explore examples very quickly.
Here are three examples from <em>DBDA2e</em> (pp. 134-135).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quick_bern_beta</span>(<span class="dv">17</span>, <span class="dv">20</span>, <span class="dt">mode =</span> <span class="fl">0.5</span>, <span class="dt">k =</span> <span class="dv">500</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch06-quick-bern-beta-examples-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quick_bern_beta</span>(<span class="dv">17</span>, <span class="dv">20</span>, <span class="dt">mode =</span> <span class="fl">0.75</span>, <span class="dt">k =</span> <span class="dv">25</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch06-quick-bern-beta-examples-2.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quick_bern_beta</span>(<span class="dv">17</span>, <span class="dv">20</span>, <span class="dt">a =</span> <span class="dv">1</span>, <span class="dt">b =</span> <span class="dv">1</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch06-quick-bern-beta-examples-3.png" width="65%" /></p>
</div>
</div>
<div id="what-if-the-prior-isnt-a-beta-distribution" class="section level2">
<h2><span class="header-section-number">6.4</span> What if the prior isn’t a beta distribution?</h2>
<p>Unless it is some other distribution where we can work things out
mathematically, we are back to the grid method.</p>
<p>Here’s an example like the one on page 136.</p>
<pre class="sourceCode r"><code class="sourceCode r">dtwopeaks &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="fl">0.48</span> <span class="op">*</span><span class="st"> </span>triangle<span class="op">::</span><span class="kw">dtriangle</span>(x, <span class="fl">0.2</span>, <span class="fl">0.3</span>) <span class="op">+</span>
<span class="st">  </span><span class="fl">0.48</span> <span class="op">*</span><span class="st"> </span>triangle<span class="op">::</span><span class="kw">dtriangle</span>(x, <span class="fl">0.7</span>, <span class="fl">0.8</span>) <span class="op">+</span>
<span class="st">  </span><span class="fl">0.04</span> <span class="op">*</span><span class="st"> </span><span class="kw">dunif</span>(x)
}

<span class="kw">BernGrid</span>(<span class="dt">data =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">13</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">14</span>)), <span class="dt">prior =</span> dtwopeaks) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_function</span>(<span class="cf">function</span>(theta) <span class="fl">0.3</span> <span class="op">*</span><span class="st"> </span><span class="kw">dbinom</span>(<span class="dv">13</span>, <span class="dv">27</span>, theta), <span class="dt">color =</span> <span class="st">&quot;forestgreen&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch06-two-peaks-1.png" width="65%" /></p>
</div>
<div id="ch06-exercises" class="section level2">
<h2><span class="header-section-number">6.5</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Show that if <span class="math inline">\(\alpha, \beta &gt; 1\)</span>,
then the mode of a Beta(<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>) distribution
is <span class="math inline">\(\displaystyle {\frac{\alpha -1}{\alpha +\beta -2}}\)</span>.</p>
<p>Hint: What would you do if you were in Calculus I?</p></li>
</ol>
<!-- Exercise 6.4.  -->
<!-- [Purpose: To explore an unusual prior and learn about the beta distribution in the process.]  -->
<ol start="2" style="list-style-type: decimal">
<li><p>Suppose we have a coin that we know comes from a magic-trick store, and
therefore we believe that the coin is strongly biased either usually to come up
heads or usually to come up tails, but we don’t know which.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Express this belief as a beta prior. That is, find shape parameters
that lead to a beta distribution that corresponds to this belief.</p></li>
<li><p>Now we flip the coin 5 times and it comes up heads in 4 of
the 5 flips. What is the posterior distribution?</p></li>
<li><p>Use <code>quick_bern_beta()</code> or a similar function of your own
creation to show the prior and posterior graphically.</p></li>
</ol></li>
<li><p>Suppose we estimate a proprtion <span class="math inline">\(\theta\)</span> using a <span class="math inline">\({\sf Beta}(10, 10)\)</span>
prior and a observe 26 successes and 48 failures.</p>
<ol style="list-style-type: lower-alpha">
<li>What is the posterior distribution?</li>
<li>What is the mean of the posterior distribution?</li>
<li>What is the mode of the posterior distribution?</li>
<li>Compute a 90% HDI for <span class="math inline">\(\theta\)</span>. [Hint: <code>qbeta()</code>]
<!-- Exercise 6.2. 
[Purpose: Connecting HDIs to the real world, with iterative data collection.]  --></li>
</ol></li>
<li><p>Suppose a state-wide election is approaching,
and you are interested in knowing whether
the general population prefers the democrat or the republican.
There is a just-published poll in the newspaper, which states that of 100
randomly sampled people, 58 preferred the republican and the remainder
preferred the democrat.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose that before the newspaper poll, your prior belief was a
uniform distribution. What is the 95% HDI on your beliefs after
learning of the newspaper poll results?</p></li>
<li><p>Based on what you know about elections, why is a uniform prior not
a great choice? Repeat part (a) with a prior the conforms better to
what you know about elections. How much does the change of prior
affect the 95% HDI?</p></li>
<li><p>You find another poll conducted by a different news organization
In this second poll, 56 of 100 people preferred the republican.
Assuming that peoples’ opinions have not changed between polls,
what is the 95% HDI on the posterior taking both polls into account.
Make it clear which prior you are using.</p></li>
<li><p>Based on this data (and your choice of prior, and assuming public opinion
doesn’t change between the time of the polls and election day), what is the
probability that the republican will win the election.</p></li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayes-rule-and-the-grid-method.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="markov-chain-monte-carlo-mcmc.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Redoing.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
