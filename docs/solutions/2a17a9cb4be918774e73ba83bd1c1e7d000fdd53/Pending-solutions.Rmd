---
title: "Pending Solutions to Problems in (Re)doing Bayesian Data Analysis"
author: "R Pruim"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

<style>

.solution{
  border: 1px solid gray;
  color: navy;
  margin: 20px;
  padding: 8px;
  size: larger;
} 
</style>

```{r setup-pending, include = FALSE}
library(CalvinBayes)
library(ggformula)
library(mosaic)
knitr::opts_chunk$set(
  cache = TRUE,
  fig.width = 9, fig.height = 3, out.width = "80%")
theme_set(theme_bw())
options(digits = 4)
```

## Chapter 4



### Chapter 5

1. More testing.

    a. Suppose that the population consists of 100,000 people. Compute how many
people would be expected to fall into each cell of Table 5.4 on page 104 of 
*DBDA2e*. (To compute the expected number of people in a cell, 
just multiply the cell probability by the size of the population.) 

    You should find that out of 100,000 people, only 100 have the disease, 
    while 99,900 do not have the disease. 
    These marginal frequencies instantiate the prior
    probability that $p(\theta = \frown) = 0.001$. 
    Notice also the cell frequencies in
    the column $\theta = \frown$, which indicate that of 100 people 
    with the disease, 99 have a positive test result and 1 has a negative 
    test result. These cell
    frequencies instantiate the hit rate of 0.99. 
    Your job for this part of the exercise is to fill in the frequencies of 
    the remaining cells of the table.
    
    b. Take a good look at the frequencies in the table you just computed
    for the previous part. These are the so-called "natural frequencies" of the
    events, as opposed to the somewhat unintuitive expression in terms of
    conditional probabilities (Gigerenzer & Hoffrage, 1995). From the cell
    frequencies alone, determine the proportion of people who have the disease,
    given that their test result is positive. 
    
    Your answer should match the result from applying Bayes' rule to
    the probabilities.
    
    c. Now we'll consider a related representation of the probabilities in terms
    of natural frequencies, which is especially useful when we accumulate more
    data. This type of representation is called a "Markov" representation by
    Krauss, Martignon, and Hoffrage (1999). Suppose now we start with a
    population of $N = 10,000,000$ people. We expect 99.9% of them (i.e.,
    9,990,000) not to have the disease, and just 0.1% (i.e., 10,000) to have the
    disease. Now consider how many people we expect to test positive. Of the
    10,000 people who have the disease, 99%, (i.e., 9,900) will be expected to
    test positive. Of the 9,990,000 people who do not have the disease, 5%
    (i.e., 499,500) will be expected to test positive. Now consider re-testing
    everyone who has tested positive on the first test. How many of them are
    expected to show a negative result on the re-test? 

    d. What proportion of people who test positive at first and then negative on
    retest, actually have the disease? In other words, of the total number of
    people at the bottom of the diagram in the previous part (those are the
    people who tested positive then negative), what proportion of them are in
    the left branch of the tree? How does the result compare with your answer to
    Exercise 5.1?

<!-- Exercise 5.1. [Purpose: Iterative application of Bayesâ€™ rule, and seeing how posterior probabilities change with inclusion of more data.]  -->

2. Suppose we have a test with a 97% specificity and a 99% sensitivity just
like in Section \@ref(discrete-params).
Now suppose that a random person is selected, has a first test that is positive,
then is retested and has a second test that is negative.  
Taking into account both tests, and assuming the results of the two tests
are independent, what is the probability that the person has the disease?

    Hint: We can use the the posterior after the first test as a prior for the 
    second test. Be sure to keep as many decimal digits as possible (use R and 
    don't round intermediate results).
    
    Note: In this problem we are assuming the the results of the two tests
    are independent, which might not be the case for some medical tests.

<div class = "solution">    
```{r}
expand.grid(
  status = c("healthy", "diseased")
  ) %>%
  mutate(
    prior = c(999/1000, 1/1000)
  )
```
</div>

<!-- Exercise 5.3. [Purpose: To see a hands-on example of data-order invariance.] -->

3. Consider again the disease and diagnostic test of the previous exercise and Section \@ref(discrete-params).

    a. Suppose that a person selected at random from the population gets the
    test and it comes back negative. Compute the probability that the person has
    the disease.
    
    b. The person then gets re-tested, and on the second test the result is
    positive. Compute the probability that the person has the disease. 
    c. How does the result compare with your answer in the 
    previous exercise?

<div class = "solution">    
</div>

4. Modify `MyBernGrid()` so that it takes an argument specifying the
probability for the HDI.  Use it to create a plot showing 50% HDI for
theta using a symmetric triangle prior and data consisting of 3 success and 
5 failures.

<div class = "solution">    
</div>

<!-- Grid with two parameters -->
5. Let's try the grid method for a model with two parameters. Suppose we 
want to estimate the mean and standard deviation of the heights of 21-year-old
American men or women (your choice which group).  First, lets get some data.

    ```{r ch05-nhanes}
library(NHANES)
Men   <- NHANES %>% filter(Gender == "male", Age == 21)
Women <- NHANES %>% filter(Gender == "female", Age == 21)
    ```

    **Likelihood**
    Our model is that heights are normally distributed with mean $\mu$ and
    standard deviation $\sigma$:

    **Prior.** For our prior, let's use something informed just a little bit by
    what we know about people (we could do better with these priors, but that's
    not our goal at the moment):

    * the mean height is somewhere between 5 and 7 feet (let's use 150 and 200 cm which
    are close to that)
    * the standard deviation is positive, but no more than 20 cm (so 95% of people are within 40 cm (~ 16 inches) of average -- that seems like a pretty safe bet).
    * we will use a uniform prior over these ranges (even though you probably believe 
    that some parts of the ranges are much more credible than others).

    So our model is 
    \begin{align*}
    \mathrm{Height} & \sim {\sf Norm}(\mu, \sigma) \\
    \mu             & \sim {\sf Unif}(150, 200) \\
    \sigma          & \sim {\sf Unif}(0, 20) 
    \end{align*}

    **Grid.** Use a grid that has 200-500 values for each parameter.
    Fill in the ?? below to create and update your grid.  

    Notes:

     * It is more numerically stable to work on the log-scale as much as possible,
     * You may normalize if you want to, but it isn't necessary for this problem.

    ```{r ch05-nhanes-grid-update-template, eval = FALSE}
    library(purrr)
    Height_Grid <- 
      expand.grid(
        mu      = seq(??, ??, ?? = ??),
        sigma   = seq(??, ??, ?? = ??)    
      ) %>%
      filter(sigma != 0) %>%   # remove sigma = 0
      mutate(
        prior = ??,
        logprior = ??,
        loglik = 
          map2_dbl(mu, sigma, 
                   ~ ?? )      # use .x for mu and .y for sigma
        logpost = logprior + loglik,
        posterior = exp(logpost)
      )
    ```

    ```{r include = FALSE, cache = TRUE}
    library(purrr)
    Height_Grid <-
      expand.grid(
        mu      = seq(150, 200, by = 0.1),
        sigma   = seq(0,   20,  by = 0.1)
      ) %>%
      filter(sigma > 0) %>%         # can't have sigma be 0
      mutate(
        prior = 1,
        logprior = 0,
        loglik = 
          map2_dbl(mu, sigma, 
                   ~ sum(dnorm(Men$Height, mean = .x, sd = .y, log = TRUE))),
        logpost = logprior + loglik,
        posterior = exp(logpost),
        posterior1 = posterior / sum(posterior, na.rm = TRUE) / (0.1 * 0.1)
      )
    ```

    Once you have created and updated your grid, you can visualize your
    posterior using a command like this (use `gf_lims()` if you want to zoom in
    a bit):

    ```{r ch05-2dposteriorplot, eval = FALSE}
    gf_tile(posterior ~ mu + sigma, data = Height_Grid) %>%
      gf_contour(posterior ~ mu+ sigma, data = Height_Grid, color = "yellow") 
    ```

    Now answer the following questions.

    a. Using the picture you just created, 
    what would you say are credible values for $\mu$ and for $\sigma$?
    
    b. Now use `hdi_from_grid()` to compute a 90% highest density (posterior)
    intervals for each parameter.  Do those make sense when you compare
    to the picture?
    
    c. Create a plot showing the posterior distribution for $\mu$ and a 90%
    HDI for $\mu$.  [Hint: how can you get a grid for the marginal
    distribution of $\mu$ from the grid for  $\mu$ and $\sigma$?]

    ```{r ch05-nhanes-hdi, include = FALSE}
    hdi_from_grid(Height_Grid, pars = c("mu", "sigma"), prob = 0.9)
    Mu_Grid <- 
      Height_Grid %>% 
      group_by(mu) %>% 
      summarise(posterior = sum(posterior1, na.rm = TRUE)) %>%
      mutate(posterior1 = posterior / sum(posterior) / 0.1)

    gf_line(posterior1 ~ mu, data = Mu_Grid) %>%
      gf_pointrangeh(height ~ mode + lo + hi, 
                    data = hdi_from_grid(Height_Grid, pars = "mu", prob = 0.9),
                    color = "red")
    ```

    
<div class = "solution">    
</div>


6. Redo the previous problem using a triangle prior for each parameter. 
You may choose where to put the peak of the triangle.


<div class = "solution">    
</div>
