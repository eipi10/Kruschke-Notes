---
title: "Pending Solutions to Problems in (Re)doing Bayesian Data Analysis"
author: "R Pruim"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

<style>

.solution{
  border: 1px solid gray;
  color: navy;
  margin: 20px;
  padding: 8px;
  size: larger;
} 
</style>

```{r setup-pending, include = FALSE}
library(CalvinBayes)
library(ggformula)
library(mosaic)
knitr::opts_chunk$set(
  cache = TRUE,
  fig.width = 9, fig.height = 3, out.width = "80%")
theme_set(theme_bw())
options(digits = 4)
```

## Chapter 4



### Chapter 5

1. More testing.

    a. Suppose that the population consists of 100,000 people. Compute how many
people would be expected to fall into each cell of Table 5.4 on page 104 of 
*DBDA2e*. (To compute the expected number of people in a cell, 
just multiply the cell probability by the size of the population.) 

    You should find that out of 100,000 people, only 100 have the disease, 
    while 99,900 do not have the disease. 
    These marginal frequencies instantiate the prior
    probability that $p(\theta = \frown) = 0.001$. 
    Notice also the cell frequencies in
    the column $\theta = \frown$, which indicate that of 100 people 
    with the disease, 99 have a positive test result and 1 has a negative 
    test result. These cell
    frequencies instantiate the hit rate of 0.99. 
    Your job for this part of the exercise is to fill in the frequencies of 
    the remaining cells of the table.
    
    b. Take a good look at the frequencies in the table you just computed
    for the previous part. These are the so-called "natural frequencies" of the
    events, as opposed to the somewhat unintuitive expression in terms of
    conditional probabilities (Gigerenzer & Hoffrage, 1995). From the cell
    frequencies alone, determine the proportion of people who have the disease,
    given that their test result is positive. 
    
    Your answer should match the result from applying Bayes' rule to
    the probabilities.
    
    c. Now we'll consider a related representation of the probabilities in terms
    of natural frequencies, which is especially useful when we accumulate more
    data. This type of representation is called a "Markov" representation by
    Krauss, Martignon, and Hoffrage (1999). Suppose now we start with a
    population of $N = 10,000,000$ people. We expect 99.9% of them (i.e.,
    9,990,000) not to have the disease, and just 0.1% (i.e., 10,000) to have the
    disease. Now consider how many people we expect to test positive. Of the
    10,000 people who have the disease, 99%, (i.e., 9,900) will be expected to
    test positive. Of the 9,990,000 people who do not have the disease, 5%
    (i.e., 499,500) will be expected to test positive. Now consider re-testing
    everyone who has tested positive on the first test. How many of them are
    expected to show a negative result on the re-test? 

    d. What proportion of people who test positive at first and then negative on
    retest, actually have the disease? In other words, of the total number of
    people at the bottom of the diagram in the previous part (those are the
    people who tested positive then negative), what proportion of them are in
    the left branch of the tree? How does the result compare with your answer to
    Exercise 5.1?

<!-- Exercise 5.1. [Purpose: Iterative application of Bayesâ€™ rule, and seeing how posterior probabilities change with inclusion of more data.]  -->

## Chapter 7
<!-- Exercise 7.3. [Purpose: Using a multimodal prior with the Metropolis algorithm, and seeing how chains can transition across modes or get stuck within them.]  -->

1. In this exercise, you will see how the Metropolis algorithm operates with a multimodal prior.

    a. Define the function  $p(\theta) = (cos(4 \pi \theta) + 1)^2/1.5$
    in R.
    
    b. Use `gf_function()` to plot $p(\theta)$ on the interval from
    0 to 1.  [Hint: Use the `xlim` argument.]
    
    c. Use `integrate()` to confirm that $p$ is a pdf.

```{r}
p <- function(theta) (cos(4 * pi * theta) + 1)^2/1.5
gf_function(p, xlim = c(0, 1))
integrate(p, 0, 1)
```
    d. Run `metro_bern()` with $p$ as your prior,
    with no data (`x = 0`, `n = 0`), and with `size = 0.2`.
    Plot the posterior distribution of $\theta$ and explain
    why it looks the way it does.
   
    ```{r}
    Metro_p <- metro_bern(0, 0, size = 0.2, prior = p)
    gf_dhistogram(~ theta, data = Metro_p, bins = 100) %>%
      gf_function(p)
    ```
    
    d. Now create a posterior histogram or density plot using
    `x = 2`, `n = 3`. Do the results look reasonable? Explain.
    
    ```{r}
    metro_bern(x = 2, n = 3, size = 0.2, prior = p, start = 0.15) %>%
    gf_dhistogram(~ theta, bins = 100) %>% gf_dens()
    ```
    
    e. Now create a posterior histogram or density plot 
    with `x = 1`, `n = 3`, and  `size = 0.02`. 
    Comment on how this compares to plot you made in the previous item.
    
    ```{r}
    metro_bern(x = 2, n = 3, size = 0.02, prior = p, start = 0.15) %>%
    gf_dhistogram(~ theta, bins = 100) %>% gf_dens()
    ```
    
    f. Repeat the previous two items but with `start = 0.15`
    and `start = 0.95`. How does this help explain what is happening?
    Why is it good practice to run MCMC algorithms with several 
    different starting values as part of the diagnositc process?

    ```{r}
    metro_bern(x = 2, n = 3, size = 0.2, prior = p, start = 0.15) %>%
    gf_dhistogram(~ theta, bins = 100) %>% gf_dens()
    metro_bern(x = 2, n = 3, size = 0.02, prior = p, start = 0.15) %>%
    gf_dhistogram(~ theta, bins = 100) %>% gf_dens()
    metro_bern(x = 2, n = 3, size = 0.2, prior = p, start = 0.95) %>%
    gf_dhistogram(~ theta, bins = 100) %>% gf_dens()
    metro_bern(x = 2, n = 3, size = 0.02, prior = p, start = 0.95) %>%
    gf_dhistogram(~ theta, bins = 100) %>% gf_dens()
    ```    