---
title: "Solutions to Problems in (Re)doing Bayesian Data Analysis"
author: "R Pruim"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
---

<style>

body{
  color: darkslategray;
}

h1, h2, h3, h4 {
  color: black;
}

.solution{
  border: 2px solid navy;
  color: black;
  margin: 20px;
  padding: 8px;
  size: larger;
} 
</style>

```{r setup-sols, include = FALSE}
library(CalvinBayes)
library(ggformula)
library(mosaic)
theme_set(theme_bw())
knitr::opts_chunk$set(
  cache = TRUE,
  fig.keep = "all",
  fig.show = "hold",
  fig.width = 9, fig.height = 3, out.width = "80%")
```



## Chapter 2 {#ch02-sols}


1. Consider Figure 2.6 on page 29 of *DBDA2E*.
Two of the data points fall above the vertical bars.
Does this mean that the model does not describe the data well?
Briefly explain your answer.

<div class = "solution">
No. The model suggests that roughly 5% of the data should fall outside the range
of these bars.  That looks compatible with the data we have.

Note: These points are ***not* outliers** -- they fit the 
pattern that the model anticipates for the data 
(at least as far as this plot diagnoses) just fine.
The model says there should be some data out there.
</div>

<!-- The next several are similar to Exercise 5.4.
[Purpose: To gain intuition about Bayesian updating by using BernGrid().]  -->

### Some General comments on problems 2-6

In problems 2-6, the point was to draw conclusions from 
a comparison of the pictures presented.
2. Run the following examples in R.  Compare the plots produced
and comment the big idea(s) illustrated by this comparison.

    ```{r, BernGrid-01, message = FALSE}
    library(CalvinBayes)
    BernGrid("H", resolution = 4,  prior = triangle::dtriangle)
    BernGrid("H", resolution = 10, prior = triangle::dtriangle)
    BernGrid("H", prior = 1, resolution = 100, geom = geom_col)
    BernGrid("H", resolution = 100,
             prior = function(p) abs(p - 0.5) > 0.48, geom = geom_col)
    ```

<div class = "solution">
The posterior can only be non-zero if both of the following 
are true:

 * the prior is non-zero
 * the data are possible (ie, the likelihood is not zero)
 
In each example we see that flipping a head drops the posterior
to 0 for a coin that only gives tails because it is impossible
to flip a head with such a coin.  The last example shows that 
if the prior is 0, the posterior will also be 0.  If our 
prior says something is impossible, no amount of data will 
change our minds.  (So generally, we are cautious about having 
the prior be 0 unless something is truly impossible, not just
unlikely or unexpected.)
</div>

3. Run the following examples in R.  Compare the plots produced
and comment the big idea(s) illustrated by this comparison.

    ```{r, BernGrid-02, message = FALSE}
    library(CalvinBayes)
    BernGrid("TTHT", prior = triangle::dtriangle)
    BernGrid("TTHT",
             prior = function(x) triangle::dtriangle(x)^0.1)
    BernGrid("TTHT",
             prior = function(x) triangle::dtriangle(x)^10)
    ```

<div class = "solution">
The only thing changing in these examples is the prior, the data
are the same each time.  The more concentrated the prior,
the more it shapes the posterior.
</div>


4. Run the following examples in R.  Compare the plots produced
and comment the big idea(s) illustrated by this comparison.

    ```{r BernGrid-03}
    library(CalvinBayes)
    dfoo <- function(p) {
      0.02 * dunif(p) +
      0.49 * triangle::dtriangle(p, 0.1, 0.2) +
      0.49 * triangle::dtriangle(p, 0.8, 0.9)
    }
    BernGrid(c(rep(0,13), rep(1,14)), prior = triangle::dtriangle)
    BernGrid(c(rep(0,13), rep(1,14)), resolution = 1000, prior = dfoo)
    ```

<div class = "solution">
Comparing the two priors we see that the large concentrations
of the foo prior near 0.15 and 0.85 result in a posterior
that allocates some credibility there even though the data are 
suggesting a value near 0.5.  Since the triangle distribution
does not assign especially large credibility there, those 
"bumps" near 0.15 and 0.85 are not present in the posterior
when we use a triangle prior.

Also note that the posterior bumps for the foo prior are shifted
toward the center (because the data are near 50% heads).
</div>

5. Run the following examples in R.  Compare the plots produced
and comment the big idea(s) illustrated by this comparison.

    ```{r BernGrid-04}
    library(CalvinBayes)
    dfoo <- function(p) {
      0.02 * dunif(p) +
      0.49 * triangle::dtriangle(p, 0.1, 0.2) +
      0.49 * triangle::dtriangle(p, 0.8, 0.9)
    }
    BernGrid(c(rep(0, 3), rep(1, 3)), prior = dfoo)
    BernGrid(c(rep(0, 10), rep(1, 10)),  prior = dfoo)
    BernGrid(c(rep(0, 30), rep(1, 30)),  prior = dfoo)
    BernGrid(c(rep(0, 100), rep(1, 100)), prior = dfoo)
    ```

<div class = "solution">
With little data, the prior has great sway over the 
posterior.  With more and more data, the influence of the prior
becomes less and less.
</div>

6. Run the following examples in R and compare them to the ones
in the previous exercise. What do you observe?

    ```{r BernGrid-05}
    library(CalvinBayes)
    dfoo <- function(p) {
      0.02 * dunif(p) +
      0.49 * triangle::dtriangle(p, 0.1, 0.2) +
      0.49 * triangle::dtriangle(p, 0.8, 0.9)
    }
    BernGrid(c(rep(0, 3), rep(1, 4)), prior = dfoo)
    BernGrid(c(rep(0, 4), rep(1, 3)), prior = dfoo)
    BernGrid(c(rep(0, 10), rep(1, 11)),  prior = dfoo)
    BernGrid(c(rep(0, 11), rep(1, 10)),  prior = dfoo)
    BernGrid(c(rep(0, 30), rep(1, 31)),  prior = dfoo)
    BernGrid(c(rep(0, 31), rep(1, 30)),  prior = dfoo)
    ```

<div class = "solution">
A very informative/opinionated prior can turn small differences
in a small data set into large differences in the posterior because it pushes the posterior toward one of the regions where the 
prior is large.  But with more and more data, this effect 
becomes less and less pronounced.
</div>

## Chapter 3

1. Create a function in R that converts Fahrenheit temperatures to 
Celsius temperatures.  [Hint: $C = (F-32) \cdot 5/9$.]

    What you turn in should show
    
    a. the code that defines your function.
    b. some test cases that show that your function is working.  (Show that
    -40, 32, 98.6, and 212 convert to -40, 0, 37, and 100.)
    Note: you should be able to test all these cases by calling the function 
    only once.  Use `c(-40, 32, 98.6, 212)` as the input.

<div class = "solution">
```{r}
to_celsius <- function(temp) {
  (temp - 32) * 5 / 9
}
to_celsius(c(-40, 32, 98.6, 212))
```
</div>

## Chapter 4

<!-- similar to Kruschke2-4.4     -->
 1. Suppose a random variable has the pdf $p(x) = 6x (1-x)$ on the interval
$[0,1]$.  (That means it is 0 outside of that interval.)
    a.  Use `function()` to create a function in R that is equivalent to `p(x)`.
    b. Use `gf_function()` to plot the function on the interval $[0, 1]$.
    c. Integrate by hand to show that the total area under the pdf is 1 
    (as it should  be for any pdf).
    c. Now have R compute that same integral (using `integrate()`).
    d. What is the largest value of $p(x)$?  At what value of $x$ does it occur?
      Is it a problem that this value is larger than 1?

        Hint: differentiation might be useful. 

<div class = "solution">
```{r sol04-01}
f <- function(x) {
  (x >= 0) * (x <= 1) * 6 * x * (1 - x)
}

gf_function(f, xlim = c(0, 1))
integrate(f, 0, 1)
```

$\frac{df}{dx} = 6 - 12x = 0$ when $x = 0.5$ and the second 
derivative is negative, so the maximum
value occurs when $x = 0.5$.

</div>

2. Recall that 
$\E(X) = \int x f(x) \;dx$ for a continuous random variable with pdf $f$ and 
$\E(X) = \sum x f(x) \;dx$ for a discrete random variable with pmf $f$.  (The 
integral or sum is over the support of the random variable.)  Compute
the expected value for the following random variables.
    a. $A$ is discrete with pmf $f(x) = x/10$ for $x \in \{1, 2, 3, 4\}$.
    b. $B$ is continuous with kernel $f(x) = x^2(1-x)$ on $[0, 1]$.  
    
        Hint:  first figure out what the pdf is.
   
<div class = "solution">

```{r sol04-02a}
x <- 1:4
p <- function(x) x/10
# Expected value is sum of values times probabilities
sum(x * p(x))
```

```{r sol04-02b}
k <- function(x) x^2 * (1 - x) * (x >= 0) * (x <= 1)
integrate(k, 0, 1)
f <- function(x) k(x) / integrate(k, 0 , 1)$value
integrate(f, 0, 1)
gf_function(f, xlim = c(0, 1))
```

```{r sol04-02c}
# expected value
integrate(function(x) x * f(x), 0, 1)
```

</div>

3. Compute the variance and standard deviation
of each of the distributions in the previous problem.

<!-- similar to Kruschke2-4.5     -->
4. In Bayesian inference, we will often need to come up with a distribution
that matches certain features that correspond to our knowledge or intuition about
a situation.  Find a normal distribution with a mean of 10 such that half of the 
distribution is within 3 of 10 (ie, between 7 and 13).  

    Hint: use `qnorm()` to determine how many standard deviations are between 10 and 7.

<div class = "solution">

```{r sol04-04}
library(mosaic)
z <- qnorm(0.75); z
sigma <- (13 - 10) / z; sigma
xpnorm(c(7, 13), mean = 10, sd = sigma)
```

</div>

<!-- Kruschke2-4.6     -->
5. School children were surveyed regarding their favorite foods. Of the total
sample, 20% were 1st graders, 20% were 6th graders, and 60% were 11th graders.
For each grade, the following table shows the proportion of respondents that
chose each of three foods as their favorite.

    a. From that information, construct a table of joint probabilities of grade and favorite food. 
    
    a. Are grade and favorite food independent?  Explain how you ascertained the answer. 

        grade | Ice cream  | Fruit       | French fries
        :----:|:----------:|:-----------:|:------------:
        1st   | 0.3        | 0.6         | 0.1
        6th   | 0.6        | 0.3         | 0.1
        11th  | 0.3        | 0.1         | 0.6

<!-- 
Hint: You are given p(grade) and p(food|grade). 
You need to determine p(grade,food). -->

<div class = "solution">
```{r}
conditional <- 
  rbind(c(0.3, 0.6, 0.1), c(0.6, 0.3, 0.1), c(0.3, 0.1, 0.6))
conditional
marginal <- 
  cbind(c(0.2, 0.2, 0.6), c(0.2, 0.2, 0.6), c(0.2, 0.2, 0.6))
marginal
joint <- conditional * marginal; joint
sum(joint)
```

Grade and favorite food are not independent. 

```{r sol04-05b}
conditional == marginal
all(conditional == marginal)
```

</div>


6. Three cards are placed in a hat.  One is black on both sides, one is white on both sides, and the third is white on one side and black on the other.  One card is selected at random from the three cards in the hat and placed on the table.  
The top of the card is black.

    a. What is the probability that the bottom is also black?
    b. What notation should we use for this probability?

<div class = "solution">
There are other ways to do this, but here is a way that looks
like a Bayesian update of a prior using a likelihood.

```{r sol04-06}
expand.grid(
  card = c("BB", "BW", "WW")
) %>%
  mutate(
    prior      = 1/3,   # all equally likely to be drawn
    likelihood = c(1, 0.5, 0),   # chance of see black on top
    posterior  = prior * likelihood,
    posterior  = posterior / sum(posterior)
  )
```

</div>

7. The three cards from the previous problem are returned to the hat.
Once again a card is pulled and placed on the table, and once again the top 
is black.  This time a second card is drawn and placed on the table.  The
top side of the second card is white.

    a. What is the probability that the bottom side of the first card is black?
    b. What is the probability that the bottom side of the second card is black?
    c. What is the probability that the bottom side of both cards is black?

<div class = "solution">
```{r sol04-07}
Grid <-
  expand.grid(
    card1 = 0:2,    # number of black sides on card
    card2 = 0:2     # number of black sides on card
  ) %>%
  mutate(
    prior0     = ifelse( card1 == card2, 0, 1),
    prior      = prior0 / sum(prior0),
    likelihood = card1 / 2 * (2 - card2) / 2,
    posterior  = prior * likelihood,
    posterior  = posterior / sum(posterior)
  ) 
Grid %>%
  DT::datatable() %>% 
  DT::formatRound(
    columns = ~ prior + likelihood + posterior, 
    digits = 3)

# answers
# a: card 1 has two black sides
Grid %>% filter(card1 == 2) %>% pull(posterior) %>% sum()
# b: second card has 1 black side
Grid %>% filter(card2 == 1) %>% pull(posterior) %>% sum()
# c: first card has 2 black sides, second has 1 black side
Grid %>% filter(card1 == 2, card2 == 1) %>% 
  pull(posterior) %>% sum()
```
</div>

8. Suppose there are two species of panda, A and B. 
Without a special blood test, it is not possible to tell them apart. 
But it is known that half of pandas are of each species and 
that 10% of births from species A are twins and
20% of births from species B are twins.

    a. If a female panda has twins, what is the probability that she is 
    from species A?

    b. If the same panda later has another set of twins, what is the probability
    that she is from species A?
    
    c. A different panda has twins and a year later gives birth to a single panda.
    What is the probability that this panda is from species A?
    
 
<div class = "solution">

#### Using Probability Rules {-}

Let $A$ and $B$ be the the events that a panda is species A or B.
Let $T_i$ be the event that the $i$th birth is twins.

##### part a {-}

\begin{align*}
P(A \mid T_1) 
  &= \frac{P(A \cap T_1)}{P(T_1)} \\
  &= \frac{P(A) \cdot P(T_1 \mid A)}{P(A \cap T_1) + P(B \cap T_1)} \\
  &= \frac{P(A) \cdot P(T_1 \mid A)}{P(A) \cdot P(T_1 \mid A) + P(B) \cdot P(T_1 \mid B)} \\ 
  &= \frac{0.5 \cdot 0.1}{0.5 \cdot 0.1 + 0.5 \cdot 0.2} = \frac{2}{3}
\end{align*}

#### part b {-}

\begin{align*}
P(A \mid T_1 \cap T_2) 
  &= \frac{P(A \cap T_1 \cap T_2)}{P(T_1 \cap T_2)} \\
  &= \frac{P(A) \cdot P(T_1 \cap T_2 \mid A)}
          {P(A \cap T_1 \cap T_2) + P(B \cap T_1 \cap T_2)} \\
  &= \frac{P(A) P(T_1 \cap T_2 \mid A)}
          {P(A) \cdot P(T_1 \cap T_2 \mid A) + P(B) \cdot P(T_1 \cap T_2 \mid B)} \\
  &= \frac{0.5 \cdot 0.1 \cdot 0.1}{0.5 \cdot 0.1 \cdot 0.1 + 0.5 \cdot 0.2 \cdot 0.2} 
   = `r 0.1^2 /(0.1^2 + 0.2^2)`
\end{align*}

#### part b using part a {-}

\begin{align}
P(A \mid T_1 \cap T_2) 
  &= \frac{P(A \cap T_1 \cap T_2)}{P(T_1 \cap T_2)}  
   = \frac{P(A \cap T_1 \cap T_2)}{P(A \cap T_1 \cap T_2) + P(B \cap T_1 \cap T_2)}  
  \\
  &= \frac{P(T_1) \cdot P(A \mid T_1) \cdot P(T_2 \mid A \cap T_1)}
          {P(T_1) \left[ P(A \mid T_1) \cdot P(T_2 \mid A \cap T_1) +
                    P(B \mid T_1) \cdot P(T_2 \mid B \cap T_1) \right]} \\ 
  &= \frac{P(A \mid T_1) \cdot P(T_2 \mid A)}
          {P(A \mid T_1) \cdot P(T_2 \mid A) +
                    P(B \mid T_1) \cdot P(T_2 \mid B)} \\ 
  &= \frac{\frac13 \cdot 0.1}{\frac13 \cdot 0.1 + \frac23 \cdot 0.2} 
   = `r 1/3 * 0.1 /(1/3 * 0.1 + 2/3 * 0.2)`
\end{align}
Notice that this says that we can use the posterior after observing the first
twins as a prior for the observation of the second twins (ignoring the first
twins at this point because the information from that event is now included in
the new prior).  This works because $P(T_2 \mid A \cap T_1) = P(T_2 \mid A)$, 
which follows from the assumption that 
$P(T_1 \cap T_2 \mid A) = P(T_1 \mid A) \cdot P(T_2 \mid A)$ 
(ie, that *for each species* twins in the first birth are independent of twins 
in the second):

\begin{align*}
P(T_2 \mid A \cap T_1) 
  &= \frac{P(T_1 \cap A \cap T_2)}
          {P(A \cap T_1)}
    \\
  &= \frac{P(A) \cdot P(T_1 \cap T_2 \mid A)}
          {P(A) \cdot P(T_1 \mid A)} 
    \\
  &= \frac{P(A) \cdot P(T_1 \mid A) \cdot P(T_2 \mid A)}
          {P(A) \cdot P(T_1 \mid A)} \\
  &= P(T_2 \mid A)
\end{align*}
A similar statement holds with $B$ in place of $A$ by the same argument.

##### part c

Let $T_2$ be the event that the second birth is NOT twins, and the same algebra
above leads to the following arithmetic:

\begin{align*}
  \frac{0.5 \cdot 0.1 \cdot 0.9}{0.5 \cdot 0.1 \cdot 0.9 + 0.5 \cdot 0.2 \cdot 0.8} 
   &= `r 0.1 * 0.9 /(0.1 * 0.9 + 0.2 * 0.8)`
   \\
  \frac{\frac13 \cdot 0.9}{\frac13 \cdot 0.9 + \frac23 \cdot 0.8} 
   &= `r 1/3 * 0.9 /(1/3 * 0.9 + 2/3 * 0.8)`
\end{align*}
</div>

<div class = "solution">

#### Grid Method style {-}

##### part a {-}

```{r sol04-08a}
Panda_Grid1 <-
  expand.grid(
    species = c("A", "B")
  ) %>%
  mutate(
    prior      = c(0.5, 0.5),
    likelihood = c(0.1, 0.2),
    posterior  = prior * likelihood,
    posterior  = posterior / sum(posterior)
  )
Panda_Grid1
```

##### part b {-}

```{r sol04-08b}
Panda_Grid2 <-
  expand.grid(
    species = c("A", "B")
  ) %>%
  mutate(
    prior      = c(0.5, 0.5),
    likelihood = c(0.1 * 0.1, 0.2 * 0.2),
    posterior  = prior * likelihood,
    posterior  = posterior / sum(posterior)
  )
Panda_Grid2

Panda_Grid2a <-
  Panda_Grid1 %>% 
  mutate(
    prior     = posterior,
    posterior = prior * likelihood,
    posterior = posterior  / sum(posterior)
  )
Panda_Grid2a
``` 

##### part c {-}

```{r sol04-08c}
Panda_Grid3 <-
  expand.grid(
    species = c("A", "B")
  ) %>%
  mutate(
    prior      = c(0.5, 0.5),
    likelihood = c(0.1 * 0.9, 0.2 * 0.8),
    posterior  = prior * likelihood,
    posterior  = posterior / sum(posterior)
  )
Panda_Grid3

Panda_Grid3a <-
  Panda_Grid1 %>% 
  mutate(
    prior      = posterior,
    likelihood = c(0.9, 0.8),
    posterior  = prior * likelihood,
    posterior  = posterior  / sum(posterior)
  )
Panda_Grid3a
```

</div>

### Some general notes about howework

1. Always show your work.  If I can't tell how you got your answer, you
probably won't get full credit.

2. For R code, follow the [style guide](https://rpruim.github.io/Kruschke-Notes/some-useful-bits-of-r.html#style-guide).  It is *not optional*.

3. Discussion should not be in R comments.  Use text for that.

4. Take advantage of the ability to [typeset mathematical notation in R Markdown](https://rpruim.github.io/s341/S19/from-class/MathinRmd.html).

5. Leave some margins (on all sides).  And leave some space between problems.

6. Your goal should be that I can read your work easily.  The more time I have
to spend just figuring out what you are doing/saying, the less your work is
worth. Communication is an important skill.  Here's a chance to practice.


### Probability issues

Some comments on probability based on recent homework.

1. Use good notation. 

    a. If you create notation for events, random variables, etc., explain it.
    A good template is "Let _____ be ________".  The first blank is your notation
    and the second is what it means.
    
    b. $P(A \cap B)$, $P(A \mid B)$, and $P(B \mid A)$ mean differeng things. Be sure to use the correct one.

2. Use of the equally likely rule requires justification -- how do you know 
the outcomes are equally likely?

3. Don't make up probability rules. If we don't have the rule you want to use,
take a moment to show that the rule is valid. (If you can't, perhaps it isn't
really a rule after all.)

## Chapter 5
