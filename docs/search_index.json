[
["multiple-metric-predictors.html", "18 Multiple Metric Predictors 18.1 SAT 18.2 Exercises", " 18 Multiple Metric Predictors 18.1 SAT 18.1.1 SAT vs expenditure Does spending more on education result in higher SAT scores? Data from 1999 (published in a paper by Gruber) can be used to explore this question. Among other things, the data includes average total SAT score (on a 400-1600 scale) and the amount of money spent on education (in 1000s of dollars per student) in each state. As a first attempt, we could fit a linear model (sat ~ expend). Using centering, the core of the model looks like this: for (i in 1:length(y)) { y[i] ~ dt(mu[i], 1/sigma^2, nu) mu[i] &lt;- alpha0 + alpha1 * (x[i] - mean(x)) } alpha1 measures how much better SAT performance is for each $1000 spent on education in a state. To fit the model, we need priors on our four parameters: nu: We can use our usual shifted exponential. sigma: {Unif}(?, ?) alpha0: {Norm}(?, ?) alpha1: {Norm}(0, ?) The question marks depend on the scale of our variables. If we build those into our model, and provide the answers as part of our data, we can use the same model for multiple data sets, even if they are at different scales. sat_model &lt;- function() { for (i in 1:length(y)) { y[i] ~ dt(mu[i], 1/sigma^2, nu) mu[i] &lt;- alpha0 + alpha1 * (x[i] - mean(x)) } nuMinusOne ~ dexp(1/29.0) nu &lt;- nuMinusOne + 1 alpha0 ~ dnorm(alpha0mean, 1 / alpha0sd^2) alpha1 ~ dnorm(0, 1 / alpha1sd^2) sigma ~ dunif(sigma_lo, sigma_hi * 1000) log10nu &lt;- log(nu) / log(10) # log10(nu) beta0 &lt;- alpha0 - mean(x) * alpha1 # true intercept } So how do we fill in the question marks for this data set? sigma: {Unif}(?,?) This quantifies the amount of variation from state to state among states that have the same per student expenditure. The scale of the SAT ranges from 400 to 1600. Statewide averages will not be near the extremes of this scale. A 6-order of maginitude window around 1 gives {Unif}(0.001, 1000), both ends of which are plenty far from what we think is reasonable. alpha0: {Norm}(?, ?) alpha0 measures the average SAT score for states that spend an average amount. Since average SATs are around 1000, something like {Norm}(1000, 100) seems reasable. alpha1: {Norm}(0, ?) This is the trickiest one. The slope of a regression line can’t be much more than \\(\\frac{SD_y}{SD_x}\\), so we can either estimate that ratio or compute it from our data to guide our choice of prior. library(R2jags) sat_jags &lt;- jags( model = sat_model, data = list( y = SAT$sat, x = SAT$expend, alpha0mean = 1000, # SAT scores are roughly 500 + 500 alpha0sd = 100, # broad prior on scale of 400 - 1600 alpha1sd = 4 * sd(SAT$sat) / sd(SAT$expend), sigma_lo = 0.001, # 3 o.m. less than 1 sigma_hi = 1000 # 3 o.m. greater than 1 ), parameters.to.save = c(&quot;nu&quot;, &quot;log10nu&quot;, &quot;alpha0&quot;, &quot;beta0&quot;, &quot;alpha1&quot;, &quot;sigma&quot;), n.iter = 4000, n.burnin = 1000, n.chains = 3 ) ## module glm loaded sat_jags ## Inference for Bugs model at &quot;/var/folders/py/txwd26jx5rq83f4nn0f5fmmm0000gn/T//Rtmpokg4pX/model146ee5cf83cde.txt&quot;, fit using jags, ## 3 chains, each with 4000 iterations (first 1000 discarded), n.thin = 3 ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## alpha0 966.30 10.477 945.217 959.654 966.396 973.052 987.086 1.001 3000 ## alpha1 -21.40 7.370 -36.165 -26.181 -21.389 -16.622 -6.772 1.001 2700 ## beta0 1092.68 45.112 1003.147 1063.878 1092.556 1121.854 1182.939 1.001 2100 ## log10nu 1.51 0.328 0.823 1.295 1.524 1.744 2.105 1.001 3000 ## nu 42.31 32.618 6.659 19.731 33.425 55.419 127.370 1.001 3000 ## sigma 69.95 7.860 56.467 64.621 69.186 74.728 87.222 1.001 3000 ## deviance 568.53 2.742 565.260 566.512 567.872 569.868 575.345 1.002 2700 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 3.8 and DIC = 572.3 ## DIC is an estimate of expected predictive error (lower deviance is better). diag_mcmc(as.mcmc(sat_jags)) mcmc_combo(as.mcmc(sat_jags)) Our primary interest is alpha1. summary_df(sat_jags) %&gt;% filter(param == &quot;alpha1&quot;) param mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff alpha1 -21.4 7.37 -36.16 -26.18 -21.39 -16.62 -6.772 1.001 2700 plot_post(posterior(sat_jags)$alpha1, xlab = &quot;alpha1&quot;, ROPE = c(-5, 5)) ## $posterior ## ESS mean median mode ## var1 3000 -21.4 -21.39 -21.16 ## ## $hdi ## prob lo hi ## 1 0.95 -36.24 -6.892 ## ## $ROPE ## lo hi P(&lt; ROPE) P(in ROPE) P(&gt; ROPE) ## 1 -5 5 0.986 0.01367 0.0003333 hdi(sat_jags, pars = &quot;alpha1&quot;, prob = 0.95) par lo hi prob chain alpha1 -37.02 -7.286 0.95 1 alpha1 -36.42 -8.540 0.95 2 alpha1 -35.29 -5.200 0.95 3 This seems odd: Nearly all the credible values for alpha1 are negative? Can we really raise SAT scores by cutting funding to schools? Maybe we should look at the raw data with our model overlaid. gf_point(sat ~ expend, data = SAT) %&gt;% gf_abline(slope = ~ alpha1, intercept = ~ beta0, data = posterior(sat_jags) %&gt;% sample_n(2000), alpha = 0.01, color = &quot;steelblue&quot;) That’s a lot of scatter, and the negative trend is heavily influenced by the 4 states that spend the most (and have relatively low SAT scores). We could do a bit more with this model, for exapmle we could * fit without those 4 states to see how much they are driving the negative trend; * do some PPC to see if the model is reasonable. But instead will will explore another model, one that has two predictors. 18.1.2 SAT vs expenditure and percent taking the test We have some additional data about each state. Let’s fit a model with two predictors: expend and frac. SAT %&gt;% head(4) state expend ratio salary frac verbal math sat Alabama 4.405 17.2 31.14 8 491 538 1029 Alaska 8.963 17.6 47.95 47 445 489 934 Arizona 4.778 19.3 32.17 27 448 496 944 Arkansas 4.459 17.1 28.93 6 482 523 1005 Here’s our model for (robust) multiple linear regression: Coding it in JAGS requires adding in the additional predictor: sat_model2 &lt;- function() { for (i in 1:length(y)) { y[i] ~ dt(mu[i], 1/sigma^2, nu) mu[i] &lt;- alpha0 + alpha1 * (x1[i] - mean(x1)) + alpha2 * (x2[i] - mean(x2)) } nuMinusOne ~ dexp(1/29.0) nu &lt;- nuMinusOne + 1 alpha0 ~ dnorm(alpha0mean, 1 / alpha0sd^2) alpha1 ~ dnorm(0, 1 / alpha1sd^2) alpha2 ~ dnorm(0, 1 / alpha2sd^2) sigma ~ dunif(sigma_lo, sigma_hi * 1000) beta0 &lt;- alpha0 - mean(x1) * alpha1 - mean(x2) * alpha2 log10nu &lt;- log(nu) / log(10) } library(R2jags) sat2_jags &lt;- jags( model = sat_model2, data = list( y = SAT$sat, x1 = SAT$expend, x2 = SAT$frac, alpha0mean = 1000, # SAT scores are roughly 500 + 500 alpha0sd = 100, # broad prior on scale of 400 - 1600 alpha1sd = 4 * sd(SAT$sat) / sd(SAT$expend), alpha2sd = 4 * sd(SAT$frac) / sd(SAT$expend), sigma_lo = 0.001, sigma_hi = 1000 ), parameters.to.save = c(&quot;log10nu&quot;, &quot;alpha0&quot;, &quot;alpha1&quot;, &quot;alpha2&quot;, &quot;beta0&quot;,&quot;sigma&quot;), n.iter = 4000, n.burnin = 1000, n.chains = 3 ) sat2_jags ## Inference for Bugs model at &quot;/var/folders/py/txwd26jx5rq83f4nn0f5fmmm0000gn/T//Rtmpokg4pX/model146ee3e235688.txt&quot;, fit using jags, ## 3 chains, each with 4000 iterations (first 1000 discarded), n.thin = 3 ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## alpha0 965.816 4.732 956.075 962.785 965.757 968.858 975.342 1.001 3000 ## alpha1 12.905 4.347 4.562 10.017 12.855 15.821 21.667 1.001 2300 ## alpha2 -2.885 0.221 -3.307 -3.034 -2.878 -2.736 -2.453 1.001 3000 ## beta0 991.273 22.460 945.522 976.284 991.808 1006.164 1035.546 1.002 1800 ## log10nu 1.373 0.365 0.640 1.117 1.390 1.639 2.033 1.001 3000 ## sigma 31.551 3.870 24.578 28.898 31.363 33.923 39.866 1.001 3000 ## deviance 491.034 2.965 487.310 488.801 490.390 492.598 498.404 1.002 3000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 4.4 and DIC = 495.4 ## DIC is an estimate of expected predictive error (lower deviance is better). diag_mcmc(as.mcmc(sat2_jags)) mcmc_combo(as.mcmc(sat2_jags)) summary_df(sat2_jags) %&gt;% filter(param == &quot;alpha1&quot;) param mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff alpha1 12.9 4.347 4.562 10.02 12.86 15.82 21.67 1.001 2300 plot_post(posterior(sat2_jags)$alpha1, xlab = &quot;alpha1&quot;, ROPE = c(-5, 5)) ## $posterior ## ESS mean median mode ## var1 2584 12.9 12.86 13.37 ## ## $hdi ## prob lo hi ## 1 0.95 4.342 21.31 ## ## $ROPE ## lo hi P(&lt; ROPE) P(in ROPE) P(&gt; ROPE) ## 1 -5 5 0 0.03133 0.9687 hdi(sat2_jags, pars = &quot;alpha1&quot;, prob = 0.95) par lo hi prob chain alpha1 4.043 21.28 0.95 1 alpha1 5.143 21.81 0.95 2 alpha1 4.342 21.25 0.95 3 summary_df(sat2_jags) %&gt;% filter(param == &quot;alpha2&quot;) param mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff alpha2 -2.885 0.221 -3.307 -3.034 -2.878 -2.736 -2.453 1 3000 plot_post(posterior(sat2_jags)$alpha2, xlab = &quot;alpha2&quot;) ## $posterior ## ESS mean median mode ## var1 2402 -2.885 -2.878 -2.839 ## ## $hdi ## prob lo hi ## 1 0.95 -3.308 -2.453 hdi(sat2_jags, pars = &quot;alpha2&quot;, prob = 0.95) par lo hi prob chain alpha2 -3.300 -2.438 0.95 1 alpha2 -3.318 -2.479 0.95 2 alpha2 -3.286 -2.439 0.95 3 gf_point(sat ~ expend, data = SAT) %&gt;% gf_abline(slope = ~ alpha1, intercept = ~ beta0, data = posterior(sat2_jags) %&gt;% sample_n(2000), alpha = 0.01, color = &quot;steelblue&quot;) gf_point(expend ~ frac, data = SAT) 18.1.3 Multiple predictors in pictures 18.1.3.1 If the predictors are uncorrelated 18.1.3.2 Correlated predictors 18.1.3.3 SAT model 18.2 Exercises Fit a model that predicts student-teacher ratio (ratio) from ependiture (expend). Is spending a good predictor of student-teacher ratio? Fit a model that predicts SAT scores from student-teacher ratio (ratio) and the fraction of students who take the SAT (frac). How does this model compare with the model that uses expend and ratio as predictors? "]
]
