[
["index.html", "(Re)Doing Bayesain Data Analysis 1 What’s in These Notes", " (Re)Doing Bayesain Data Analysis R Pruim 2019-01-30 1 What’s in These Notes This “book” is a companion to Kruschke’s Doing Bayesian Data Analysis. The main reasons for this companion are to use a different style of R code that includes: use of modern packages like tidyverse, R2jags, bayesplot, and ggformula; adherence to a different style guide; less reliance on manually editing scripts and more use of resusable code available in packages; a workflow that takes advantage of RStudio and RMarkdown. This is a work in progress. Please accept my apologies in advance for errors, inconsistencies lack of complete coverage But feel free to post an issue on github if you spot things that require attention or care to make suggestions for improvement. I’ll be teaching from this book in Spring 2019, so I expect rapid development during those months. "],
["credibility-models-and-parameters.html", "2 Credibility, Models, and Parameters 2.1 The Steps of Bayesian Data Analysis 2.2 Example 1: Which coin is it? 2.3 Distribtions 2.4 Example 2: Height vs Weight 2.5 Where do we go from here? 2.6 Exercises 2.7 Footnotes", " 2 Credibility, Models, and Parameters 2.1 The Steps of Bayesian Data Analysis In general, Bayesian analysis of data follows these steps: Identify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors? Define a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis. Specify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists. Use Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step). Check that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model. In this chapter we will focus on two examples so we can get an overview of what Bayesian data analysis looks like. In subsequent chapters we will fill in lots of the missing details. 2.1.1 R code Some of the R code used in this chapter has been hidden, and some of it is visible. In any case the point of this chapter is not to understand the details of the R code. It is there mainly for those of you who are curious, or because you might come back and look at this chapter later in the semester. For those of you new to R, we will be learning it as we go along. For those of you who have used R before, some of this will be familiar to you, but other things likely will not be familiar. 2.1.2 R packages We will make use of a number of R packages as we go along. Here is the code used to load the packages used in this chapter. If you try to mimic the code on your own machine, you will need to use these packages. library(ggformula) # for creating plots theme_set(theme_bw()) # change the default graphics settings library(dplyr) # for data wrangling library(mosaic) # includes the previous 2 (and some other stuff) library(CalvinBayes) # includes BernGrid() library(brms) # used to fit the model in the second exmample, # but hidden from view here 2.2 Example 1: Which coin is it? As first simple illustration of the big ideas of Bayesian inference, let’s consider a situation where we have a coin that is known to result in heads in either 0, 20, 40, 60, 80, or 100% of tosses. But we don’t know which. Our plan is to gather data by flipping the coin and recording the results. If we let \\(\\theta\\) be the true probability of tossing a head, we can refer to these 5 possibilities as \\(\\theta = 0\\), \\(\\theta = 0.2\\), \\(\\theta = 0.4\\), \\(\\theta = 0.6\\), \\(\\theta = 0.8\\), and \\(\\theta = 1\\). Before collecting our data, if have no other information, we will consider each coin to be equally credible. We could represent that as follows. Now suppose we toss the coin and obtain a head. What does that do to our credibilities? Clearly \\(\\theta = 0\\) is no longer possible. So the credibility of that option becomes 0. The other credibilities are adjusted as well. We will see later just how, but the following should be intuitive: the options with larger values of \\(\\theta\\) should increase in credibility more than those with lower values of \\(\\theta\\). the total credibility of all options should remain 1 (100%). In fact, the adjusted credibility after one head toss looks like this: This updating of credibility of possible values of \\(\\theta\\) is the key idea in Bayesian inference. Bayesians don’t call these distributions of credibility “before” and “after”, however. Instead they use the longer words “prior” and “posterior”, which mean the same thing. Now suppose we toss the coin again and get another head. Once again we can update the credibility, and once again, the larger values of \\(\\theta\\) will see their credibility increase while the smaller values of \\(\\theta\\) will see their credibility decrease. Time for a third toss. This time we obtain a tail. Now the credibility of \\(\\theta = 1\\) drops to 0, and the relative credibilities of the smaller values of \\(\\theta\\) will increase and of the larger values of \\(\\theta\\) will decrease. Finally, we flip one more tail. As expected, the posterior is now symmetric with the two central values of \\(\\theta\\) having the larger credibility. We can keep playing this game as long as we like. Each coin toss provides a bit more information with which to update the posterior, which becomes our new prior for subsequent data. The BernGrid() function in the CalvinBayes package makes it easy to generate plots similar to the ones above. 1 BernGrid(&quot;HHTTTHTTT&quot;, # the data steps = TRUE, # show each step p = c(0, 0.2, 0.4, 0.6, 0.8, 1)) # possible probabilities ## Converting data to 1, 1, 0, 0, 0, 1, 0, 0, 0 2.2.1 Freedom of choice In practice, we are usually not given a small number of possible values for the probability (of obtaining heads in our example, but it could be any probability). Instead, the probability could be any value between 0 and 1. But we can do Bayesian updating in essentially the same way. Instead of a bar chart, we will use a line graph (called a density plot) to show how the credibility depends on the paramter value. BernGrid(&quot;HHTTTHTTT&quot;, # the data steps = TRUE) # show each step ## Converting data to 1, 1, 0, 0, 0, 1, 0, 0, 0 2.3 Distribtions The (prior and posterior) distributions in the previous plots were calculated numerically using a Bayesian update rule that we will soon learn. Density functions have the properties that * they are never negative, and * the total area under the curve is 1. Where the density curve is taller, values are more likely. So in the last posterior credibility above, we see that values near 1/3 are the most credible while values below 0.015 or above 0.065 are not very credible. In particular, we still can’t discount the possibility that we are dealing with a fair coin since 0.5 lies well within the most credible central portion of the plot. We will also encounter densities with names like “normal”, “beta”, and “t”. The gf_dist() function from ggformula can be used to plot distributions. We just need to provide R’s version of the name for the family and any required parameter values. 2.3.1 Beta distributions The curves in our coins example above look a lot like beta distributions. In fact, we will eventually learn that they are beta distributions, and that each new observed coin toss increases either shape1 or shape2 by 1. gf_dist(&quot;beta&quot;, shape1 = 1, shape2 = 1, color = &quot;gray50&quot;) %&gt;% gf_dist(&quot;beta&quot;, shape1 = 2, shape2 = 1, color = &quot;red&quot;) %&gt;% gf_dist(&quot;beta&quot;, shape1 = 3, shape2 = 1, color = &quot;orange&quot;) %&gt;% gf_dist(&quot;beta&quot;, shape1 = 3, shape2 = 2, color = &quot;forestgreen&quot;) %&gt;% gf_dist(&quot;beta&quot;, shape1 = 3, shape2 = 3, color = &quot;navy&quot;) 2.3.2 Normal distributions Another important family of distributions is the normal family. These are bell-shaped, symmetric distributions centered at the mean (\\(\\mu\\)). A second paramter, the standard deviation (\\(\\sigma\\)) quantifies how spread out the distribution is. To plot a normal distribution with mean 10 and standard deviation 1 or 2, we use gf_dist(&quot;norm&quot;, mean = 10, sd = 1, color = &quot;steelblue&quot;) %&gt;% gf_dist(&quot;norm&quot;, mean = 10, sd = 2, color = &quot;red&quot;) The red curve is “twice as spread out” as the blue one. We can also draw random samples from distributions. Random samples will not exactly follow the shape of the distribution they were drawn from, so it takes some experience to get calibrated to know when things are “close enough” to consider a proposed distribution to be believable, and when they are “different enough” to be skeptical. Generating some random data and comparing to the theoretical distribution can help us calibrate. In the example below, we generate 25 random samples of size 100 and compare their (density) histograms to the theoretical Norm(10, 2) distribuiton. expand.grid() produces a data frame with two columns containing every combination of the numbers 1 through 100 with the numbers 1 through 25, for a total of 2500 rows. mutate() is used to add a new variable to the data frame. Rdata &lt;- expand.grid( rep = 1:100, sample = 1:25) %&gt;% mutate( x = rnorm(2500, mean = 10, sd = 2) ) head(Rdata) rep sample x 1 1 11.171 2 1 11.419 3 1 9.781 4 1 9.093 5 1 11.212 6 1 6.364 gf_dhistogram( ~ x | sample, data = Rdata, color = &quot;gray30&quot;, alpha = 0.5) %&gt;% gf_dist(&quot;norm&quot;, mean = 10, sd = 2, color = &quot;red&quot;) We will see many other uses of these functions. See the next chapter for in introduction to R functions that will be useful. 2.4 Example 2: Height vs Weight The coins example above is overly simple compared to typical applications. Before getting to the nuts and bolts of doing bayesian data analysis, let’s look at a somewhat more realistic example. Suppose we want to model the relationship beteween weight and height in 40-year-old Americans. 2.4.1 Data Here’s a scatter plot of some data from the NHANES study that we will use for this example. (Note: this is not the same data set used in the book. The data here come from the NHANES::NHANES data set.) 2.4.2 Describing a model for the relationship between height and weight A plausible model is that weight is linearly related to height. We will make this model a bit more precise by defining the model parameters and distributions involved. Typically statisticians use Greek letters to represent parameters. This model has three parameters (\\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\)) and makes two claims The average weight of people with height \\(x\\) is \\(\\beta_0 + \\beta_1 x\\) (some linear function of \\(x\\)). We can express this as \\[ \\mu_{Y|x} = E(Y \\mid x) = \\beta_0 + \\beta_1 x \\] or \\[ \\mu_{\\mbox{weight}|\\mbox{height}} = E(\\mbox{weight} \\mid \\mbox{height}) = \\beta_0 + \\beta_1 \\cdot \\mbox{height} \\] The \\(Y\\) and \\(x\\) notation is useful for general formulas; for specific problems (especially in R code), it is usually better to use descriptive names for the variables. The capital \\(Y\\) indicates that it has a distribution. \\(x\\) is lower case because we are imagining a specific value there. So for each value of \\(x\\), the there is a distribution of \\(Y\\)’s. But not everyone is average. The model used here assumes that the distributions of the heights of people with a given weight are symmetrically distributed around the average weight for that height and that the distribution is normal (bell-shaped). The parameter \\(\\sigma\\) is called the standard deviation and measures the amount of variability. If \\(\\sigma\\) is small, then most people’s weights are very close to the average for their height. If \\(\\sigma\\) is larger, then there is more variability in weights for people who have the same height. We express this as \\[\\begin{align} y \\mid x \\sim {\\sf Norm}(\\mu_{y|x}, \\sigma) \\end{align}\\] Notice the \\(\\sim\\) in this expression. It is read “is distrubted as” and describes the distribution (shape) of some quantity. Putting this all together, and being a little bit sloppy we might write it this way: \\[\\begin{align} Y &amp;\\sim {\\sf Norm}(\\mu, \\sigma) \\\\ \\mu &amp; \\sim \\beta_0 + \\beta_1 x \\end{align}\\] In this style the dependence of \\(y\\) on \\(x\\) is implicit (via \\(\\mu\\)’s dependence on \\(x\\)) and we save writing \\(\\mid x\\) in a few places. 2.4.3 Prior A prior distribution describes what is known/believed about the parameters before we use the information from our data. This could be informed by previous data, or it may be a fairly uniformative prior that considers many values of the parameter to be credible. For this example, we use very flat broad priors (centered at 0 for the \\(\\beta\\)’s and extending from 0 to a very large number of \\(\\sigma\\). (We know that \\(\\sigma &gt; 0\\), so our prior should reflect that knowledge.) 2.4.4 Posterior The posterior distribution is calculated by combining the information about the model (via the likelihood function) with the prior. The posterior will provide updated distibutions for \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\). These distributions will be narrow if our data give a strong evidence about their values and wider if even after considering the data, there is still considerable uncertainty about the paramter values. For now we won’t worry about how the posterior distribution is computed, but we can inspect it visually. (It is called Post in the R code below.) For example, if we are primarily interested in the slope (how much heavier are people on average for each inch they are taller?), we can plot the posterior distribution of \\(\\beta_1\\) or calcuate its mean, or the region containing the central 95% of the distribution. Such a region is called a highest density interval (HDI) (sometimes called the highest posterior density interval (HPDI), to emphasize that we are looking at a posterior distribution, but an HDI can be computed for other distributions as well). gf_density( ~ b_height, data = Post, alpha = 0.5) mean(~ b_height, data = Post) ## [1] 7.223 hdi(Post, pars = &quot;b_height&quot;) par lo hi prob b_height 5.551 9.045 0.95 mcmc_areas(as.mcmc(Post), pars = &quot;b_height&quot;, prob = 0.95) Although we don’t get a very precise estimate of \\(\\beta_1\\) from this model/data combination, we can be quite confident that taller people are indeed heavier (on average), somewhere between 5 and 10 pounds heavier per inch taller. Another interesting plot shows lines overlaid on the scatter plot. Each line represents a plausible (according to the posterior distribution) combination of slope and intercept. 100 such lines are included in the plot below. 2.4.5 Posterior Predictive Check Notice that only a few of the dots are covered by the blue lines. That’s because the blue lines represent plausible average weights. But the model takes into account that some people may be quite a bit heavier or lighter than average. A posterior predictive check is a way of checking that the data look like they could have been plausibly generated by our model. We can generate a simulated weight for a given height by randomly selecting values of \\(\\beta_0\\), \\(\\beta_1\\), \\(\\sigma\\) so that the more credible values are more likely to be selected, and using the normal distribution to generate a difference between an individual weight and the average weight (as determined by the paramters \\(\\beta_0\\) and \\(\\beta_1\\). For example, here are the first two rows of our posterior distribution: Post %&gt;% head(2) b_Intercept b_height sigma lp__ -217.0 6.028 40.11 -784.5 -422.1 9.030 48.28 -786.5 To simulate a weight for a height of 65 inches based on the fist row, we could take a random draw from a \\({\\sf Norm}(-217 + 6.028 \\cdot 65, 40.11)\\) distribution. We can do a similar thing for the second row. Post %&gt;% head(2) %&gt;% mutate(pred_y = rnorm(2, mean = b_Intercept + b_height * 65, sd = sigma)) b_Intercept b_height sigma lp__ pred_y -217.0 6.028 40.11 -784.5 185.3 -422.1 9.030 48.28 -786.5 219.4 Those values are quite different. This is because credible values of \\(\\sigma\\) are quite large – an indication that individuals will vary quite substantially from the average weight for their height. gf_density( ~ sigma, data = Post) We should not be surprised to see some (~ 5%) of people who are 85 pounds above or below the average weight for their height. If we do this many times for several height values and plot the central 95% of the weights, we get a plot that looks like this: PPC &lt;- expand.grid( height = seq(56, 76, by = 1), rep = 1:nrow(Post) ) %&gt;% mutate( b_Intercept = Post$b_Intercept[rep], b_height = Post$b_height[rep], sigma = Post$sigma[rep], weight = b_Intercept + b_height * height + rnorm(n = n(), 0, sigma) ) %&gt;% group_by(height) %&gt;% summarise( mean = mean(weight), lo = quantile(weight, prob = 0.025), hi = quantile(weight, prob = 0.975) ) gf_point(weight ~ height, data = NHANES40, shape = 1) %&gt;% gf_pointrange(mean + lo + hi ~ height, data = PPC, alpha = 0.7, color = &quot;steelblue&quot;) Now we see that indeed, most (but not all) of the data points fall within a range that the model believes is credible. If this were not the case, it would be evidence that our model is not well aligned with the data and might lead us to explore other models. Notice that by taking many different credible values of the parameters (including \\(\\sigma\\)), we are taking into account both our uncertainty about the parameter values and the variability that the model describes in the population (even for given parameter values). 2.5 Where do we go from here? Now that we have seen an overview of Bayesian inference at work, you probably have lots of questions. Of time we will improve our answers to each of them. How do we create models? One of the nice things about Bayesian inference is that it is so flexible. That allows us to create all sorts of models. We will begin with models of a proportion (and how that proportion might depend on other variables) because these are the simplest to understand. Then we will move on other important examples. (The back half of our book is a smorgasbord of example situations.) How do we select priors? We will begin with fairly “uninformative” priors that say very little, and we will expriment with different priors to see what affect the choice of prior has on our analysis. Gradually we will learn more about prior selection. How do we update the prior based on data to get the posterior? Here we will learn several approaches, most of them computational. (There are only a limited number of examples where the prior can be computed analytically.) We will start with computational methods that are simple to implement and relatively easy to understand, but are too inefficient to use on large or complex problems. Eventually we will learn how to use two important algorithms (JAGS and Stan) to describe and fit Bayesian models. How do we tell whether the algorithm that generated the posterior worked well? The computainal algorithms that compute posterior distributions can fail. No one algorithm works best on every problem, and sometimes we need to describe our model differently to help the computer. We will learn some diagnostics to help us detect when there may be problems with our computations. What can we do with the posterior once we have it? After all the work of building a model, selectig a prior, fitting the model to obtain a posterior, and convincing ourselves that no disasters have happened along the way, what can we do with the posterior? We will use it both to diagnose the model itself and to see what the model has to say. 2.6 Exercises Consider Figure 2.6 on page 29 of DBDA2E. Two of the data points fall above the vertical bars. Does this mean that the model does not describe the data well? Briefly explain your answer. Run the following examples in R. Compare the plots produced and comment the big idea(s) illusrated by this comparison. library(CalvinBayes) BernGrid(&quot;H&quot;, resolution = 4, prior = triangle::dtriangle) BernGrid(&quot;H&quot;, resolution = 10, prior = triangle::dtriangle) BernGrid(&quot;H&quot;, prior = 1, resolution = 100, geom = geom_col) BernGrid(&quot;H&quot;, resolution = 100, prior = function(p) abs(p - 0.5) &gt; 0.48, geom = geom_col) Run the following examples in R. Compare the plots produced and comment the big idea(s) illusrated by this comparison. library(CalvinBayes) BernGrid(&quot;TTHT&quot;, prior = triangle::dtriangle) BernGrid(&quot;TTHT&quot;, prior = function(x) triangle::dtriangle(x)^0.1) BernGrid(&quot;TTHT&quot;, prior = function(x) triangle::dtriangle(x)^10) Run the following examples in R. Compare the plots produced and comment the big idea(s) illusrated by this comparison. library(CalvinBayes) dfoo &lt;- function(p) { 0.02 * dunif(p) + 0.49 * triangle::dtriangle(p, 0.1, 0.2) + 0.49 * triangle::dtriangle(p, 0.8, 0.9) } BernGrid(c(rep(0,13), rep(1,14)), prior = triangle::dtriangle) BernGrid(c(rep(0,13), rep(1,14)), resolution = 1000, prior = dfoo) Run the following examples in R. Compare the plots produced and comment the big idea(s) illusrated by this comparison. library(CalvinBayes) dfoo &lt;- function(p) { 0.02 * dunif(p) + 0.49 * triangle::dtriangle(p, 0.1, 0.2) + 0.49 * triangle::dtriangle(p, 0.8, 0.9) } BernGrid(c(rep(0, 3), rep(1, 3)), prior = dfoo) BernGrid(c(rep(0, 3), rep(1, 3)), prior = dfoo) BernGrid(c(rep(0, 10), rep(1, 10)), prior = dfoo) BernGrid(c(rep(0, 30), rep(1, 30)), prior = dfoo) BernGrid(c(rep(0, 100), rep(1, 100)), prior = dfoo) Run the following examples in R and compare them to the ones in the previous exercise. What do you observe? library(CalvinBayes) dfoo &lt;- function(p) { 0.02 * dunif(p) + 0.49 * triangle::dtriangle(p, 0.1, 0.2) + 0.49 * triangle::dtriangle(p, 0.8, 0.9) } BernGrid(c(rep(0, 3), rep(1, 4)), prior = dfoo) BernGrid(c(rep(0, 4), rep(1, 3)), prior = dfoo) BernGrid(c(rep(0, 10), rep(1, 11)), prior = dfoo) BernGrid(c(rep(0, 11), rep(1, 10)), prior = dfoo) BernGrid(c(rep(0, 30), rep(1, 31)), prior = dfoo) BernGrid(c(rep(0, 31), rep(1, 30)), prior = dfoo) 2.7 Footnotes Kruschke likes to write his integrals in a different order: \\(\\int dx \\; f(x)\\) instead of \\(\\int f(x) \\; dx\\). Either order means the same thing.↩ "],
["some-useful-bits-of-r.html", "3 Some Useful Bits of R 3.1 You Gotta Have Style 3.2 Vectors, Lists, and Data Frames 3.3 Plotting with ggformula 3.4 Creating data with expand.grid() 3.5 Transforming and summarizing data dplyr and tidyr 3.6 Writing Functions 3.7 Exercises 3.8 Footnotes", " 3 Some Useful Bits of R 3.1 You Gotta Have Style Good programming style is incredibly important. It makes your code easier to read and edit. That leads to fewer errors. Here is a brief style guide you are expected to follow for all code in this course: For more detailes see http://adv-r.had.co.nz/Style.html, on which this is based. No long lines. Lines of code should have at most 80 characters. Programming lines should not wrap, you should choose the line breaks yourself. Choose them in natural places. In R markdown, long codes lines don’t wrap, they flow off the page, so the end isn’t visible. (And it makes it obvious that you didn’t look at your own print out.) # Don&#39;t ever use really long lines. Not even in comments. They spill off the page and make people wonder what they are missing. Use your spacebar. There is a reason it is the largest key on the keyboard. Use it often. Spaces after commas (always). Spaces around operators (always). Spaces after the comment symbol # (always). Use other spaces judiciously to align similar code chunks to make things easier to read or compare. x&lt;-c(1,2,4)+5 # BAD BAD BAD x &lt;- c(1, 2, 4) + 5 # Ah :^) But don’t go crazy with the spacebar. There are a few places you should not use spaces: after open parentheses or before closed parentheses between function names and the parentheses that follow Indent to show the structure of your code. Use 2 spaces to indent (to keep things from drifting right too quickly). Fortunately, this is really easy. Highlight your code and hit &lt;CTRL&gt;-i (PC) or &lt;command&gt;-i (Mac). If the indention looks odd to you, you have most likely messed up commas, quotes, parentheses, or curly braces. Choose names wisely and consistently. Naming things is hard, but take a moment to choose good names, and go back and change them if you come up with a better name later. Here are some helpful hints: Very short names should only be used for a very short time (a couple lines of code). Else we tend to forget what they meant. Avoid names like x, f, etc. unless the use is brief and mimics some common mathematical formula. Break long names visually. Common ways to do this are with a dot (.), an underscore _, or camelCase. There are R coders who prefer all three, but don’t mix and match for similar kinds of things, that just makes it harder to remember what to do the next time. good_name &lt;- 10 good.name &lt;- 10 goodName &lt;- 10 # not alignment via extra space in this line really_terrible.ideaToDo &lt;- -5 The trend in R is toward using underscore (_) and I recommend it. Older code often used dot (.). CamelCase is the least common in R. Recommendation: capitalize data frame names; use lower case for variables inside data frames. This is not a common convention in R, but it can really help to keep things straight. I’ll do this in the data sets I create, but when we use other data sets, they may not follow this convention. Avoid using names that are already in use by R. This can be hard to avoid when you are starting out because you don’t know what all is defined. Here are a few things to avoid. T # abbreviation for TRUE F # abbreviation for FALSE c # used to concetenate vectors and lists df # density function for f distributions dt # density function for t distributions Use comments (#), but use them for the right thing. Comments can be used to clarify names, point out subtlties in code, etc. They should not be used for your analysis or discussion of results. Don’t comment things that are obvious without comment. Comments should add value. x &lt;- 4 # set x to 4 &lt;----------------------- no need for this comment x &lt;- 4 # b/c there are four grade levels in the study &lt;------- useful Excpetions should be exceptional. No style guide works perfectly in all situations. Ocassionally you may need to violate the style guide. But these instances should be rare and should have a good reason. They should not arise form your sloppiness or laziness. 3.1.1 An additional note about homwork When you do homework, I want to see your code and the results (and your discussion of those results). Writing in R Markdown makes this all easy to do. But make sure that I can see all the necessary things to evaluate what you are doing. You have access to your code and can investigate variables, etc. But make sure I can see what’s going one in the document. This often means displaying intermediate results. Once common way to do this is with a semi-colon: x &lt;- 57 * 23; x ## [1] 1311 3.2 Vectors, Lists, and Data Frames 3.2.1 Vectors In R, a vector is a homogeneous ordered collection (indexing starts at 1). By homogeneous, we mean that each element is he same kind of thing. Short vectors can be created using c(): x &lt;- c(1, 3, 5) x ## [1] 1 3 5 Evenly spaced sequences can be created using seq(): x &lt;- seq(0, 100, by = 5); x ## [1] 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 ## [18] 85 90 95 100 y &lt;- seq(0, 1, length.out = 11); y ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0:10 # short cut for consecutive integers ## [1] 0 1 2 3 4 5 6 7 8 9 10 Repeated values can be created with rep(): rep(5, 3) ## [1] 5 5 5 rep(c(1, 2, 3), each = 2) ## [1] 1 1 2 2 3 3 rep(c(1, 2, 3), times = 2) ## [1] 1 2 3 1 2 3 rep(c(1, 2, 3), times = c(1, 2, 3)) ## [1] 1 2 2 3 3 3 rep(c(1, 2, 3), each = c(1, 2, 3)) # Ack! see warning message. ## Warning in rep(c(1, 2, 3), each = c(1, 2, 3)): first element used of &#39;each&#39; ## argument ## [1] 1 2 3 When a function acts on a vector, there are several things that could happen. One result can be computed from the entire vector. x &lt;- c(1, 3, 6, 10) length(x) ## [1] 4 mean(x) ## [1] 5 The function may be applied to each element of the vector and a vector of restults returned. (Such functions are called vectorized.) log(x) ## [1] 0.000 1.099 1.792 2.303 2 * x ## [1] 2 6 12 20 x^2 ## [1] 1 9 36 100 The first element of the vector may be used and the others ignored. (Less common but dangerous – be on the lookout. See example above.) Items in a vector can be accessed using []: x &lt;- seq(10, 20, by = 2) x[2] ## [1] 12 x[10] # NA indicates a missing value ## [1] NA x[10] &lt;- 4 x # missing values filled in to make room! ## [1] 10 12 14 16 18 20 NA NA NA 4 is.na(x) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE In addition to using integer indices, there are two other ways to access elements of a vector: names and logicals. If the items in a vector are named, names are displayed when a vector is displayed, and names can be used to access elements. x &lt;- c(a = 5, b = 3, c = 12, 17, 1) x ## a b c ## 5 3 12 17 1 names(x) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;&quot; &quot;&quot; x[&quot;b&quot;] ## b ## 3 Logicals (TRUE and FALSE) are very interesting in R. In indexing, they tell us which items to keep and which to discard. x &lt;- (1:10)^2 x &lt; 50 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE x[x &lt; 50] ## [1] 1 4 9 16 25 36 49 x[c(TRUE, FALSE)] # T/F recycled to length 10 ## [1] 1 9 25 49 81 which(x &lt; 50) ## [1] 1 2 3 4 5 6 7 3.2.2 Lists Lists are a lot like vectors, but Create a list with list(). The elements can be different kinds of things (including other lists) Use [[ ]] to access elements. You can also use $ to access named elements If you use [] you will get a list back not an element. # a messy list L &lt;- list(5, list(1, 2, 3), x = TRUE, y = list(5, a = 3, 7)) L ## [[1]] ## [1] 5 ## ## [[2]] ## [[2]][[1]] ## [1] 1 ## ## [[2]][[2]] ## [1] 2 ## ## [[2]][[3]] ## [1] 3 ## ## ## $x ## [1] TRUE ## ## $y ## $y[[1]] ## [1] 5 ## ## $y$a ## [1] 3 ## ## $y[[3]] ## [1] 7 L[[1]] ## [1] 5 L[[2]] ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 L[1] ## [[1]] ## [1] 5 L$y ## [[1]] ## [1] 5 ## ## $a ## [1] 3 ## ## [[3]] ## [1] 7 L[[&quot;x&quot;]] ## [1] TRUE L[1:3] ## [[1]] ## [1] 5 ## ## [[2]] ## [[2]][[1]] ## [1] 1 ## ## [[2]][[2]] ## [1] 2 ## ## [[2]][[3]] ## [1] 3 ## ## ## $x ## [1] TRUE glimpse(L) ## List of 4 ## $ : num 5 ## $ :List of 3 ## ..$ : num 1 ## ..$ : num 2 ## ..$ : num 3 ## $ x: logi TRUE ## $ y:List of 3 ## ..$ : num 5 ## ..$ a: num 3 ## ..$ : num 7 3.2.3 Data frames for rectangular data Rectangular data is organized in rows and columns (much like an excel spreadsheet). These rows and columns have a particular meaning: Each row represents one observational unit. Observational units go by many others names depending whether they are people, or inanimate objects, our events, etc. Examples include case, subject, item, etc. Regardless, the observational units are the things about which we collect information, and each one gets its own row in rectangular data. Each column represents a variable – one thing that is “measured” and recorded (at least in principle, some measurements might be missing) for each observational unit. Example In a study of nutritional habbits of college students, our observational units are the college students in the study. Each student gets her own row in the data frame. The variables might include things like an ID number (or name), sex, height, weight, whether the student lives on campus or off, what type of meal plan they have at the dining hall, etc., etc. Each of these is recorded in a separate column. Data frames are the standard way to store rectangular data in R. Usually variables (elements of the list) are vectors, but this isn’t required, sometimes you will see list variables in data frames. Each element (ie variable) must have the same length (to keep things rectangular). Here, for example, are the first few rows of a dataset called KidsFeet: library(mosaicData) # Load package to make KidsFeet data available head(KidsFeet) # first few rows name birthmonth birthyear length width sex biggerfoot domhand David 5 88 24.4 8.4 B L R Lars 10 87 25.4 8.8 B L L Zach 12 87 24.5 9.7 B R R Josh 1 88 25.2 9.8 B L R Lang 2 88 25.1 8.9 B L R Scotty 3 88 25.7 9.7 B R R 3.2.3.1 Accessing via [ ] We can access rows, columns, or individual elements of a data frame using [ ]. This is the more usual way to do things. KidsFeet[, &quot;length&quot;] ## [1] 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 ## [15] 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 ## [29] 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 KidsFeet[, 4] ## [1] 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 ## [15] 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 ## [29] 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 KidsFeet[3, ] name birthmonth birthyear length width sex biggerfoot domhand 3 Zach 12 87 24.5 9.7 B R R KidsFeet[3, 4] ## [1] 24.5 KidsFeet[3, 4, drop = FALSE] # keep it a data frame length 3 24.5 KidsFeet[1:3, 2:3] birthmonth birthyear 5 88 10 87 12 87 By default, Accessing a row returns a 1-row data frame. Accessing a column returns a vector (at least for vector columns) Accessing a element returns that element (technically a vector with one element in it). 3.2.3.2 Accessing columns via $ We can also access individual varaiables using the $ operator: KidsFeet$length ## [1] 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 ## [15] 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 ## [29] 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 KidsFeet$length[2] ## [1] 25.4 KidsFeet[2, &quot;length&quot;] ## [1] 25.4 KidsFeet[2, &quot;length&quot;, drop = FALSE] # keep it a data frame length 2 25.4 As we will see, there are are other tools that will help us avoid needing to us $ or [ ] for access columns in a data frame. This is espcially nice when we are working with several variables all coming from the same data frame. 3.2.3.3 Accessing by number is dangerous Generally speaking, it is safer to access things by name than by number when that is an option. It is easy to miscalculate the row or column number you need, and if rows or columns are added to or deleted from a data frame, the numbering can change. 3.2.3.4 Implementation Data frames are implemented in R as a special type (technically, class) of list. The elements of the list are the columns in the data frame. Each column must have the same length (so that our data frame has coherent rows). Most often the columns are vectors, but this isn’t required. This explains why $ works the way it does – we are just accessing one item in a list. It also means that we can use [[ ]] to access a column: KidsFeet[[&quot;length&quot;]] ## [1] 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 ## [15] 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 ## [29] 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 KidsFeet[[4]] ## [1] 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 ## [15] 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 ## [29] 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 KidsFeet[4] length 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 3.2.4 Other types of data Some types of data do not work well in a rectangular arrangment of a data frame, and there are many other ways to store data. In R, other types of data commonly get stored in a list of some sort. 3.3 Plotting with ggformula R has several plotting systems. Base graphics is the oldest. lattice and ggplot2 are both built on a system called grid graphics. ggformula is built on ggplot2 to make it easier to use and to bring in some of the advantages of lattice. You can find out about more about ggformula at https://projectmosaic.github.io/ggformula/news/index.html. 3.4 Creating data with expand.grid() We will frequently have need of synthetic data that includes all combinations of some variable values. expand.grid() does this for us: expand.grid( a = 1:3, b = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)) a b 1 A 2 A 3 A 1 B 2 B 3 B 1 C 2 C 3 C 1 D 2 D 3 D 3.5 Transforming and summarizing data dplyr and tidyr See the tutorial at http://rsconnect.calvin.edu/wrangling-jmm2019 or https://rpruim.shinyapps.io/wrangling-jmm2019 3.6 Writing Functions 3.6.1 Why write functions? There are two main reasons for writing functions. You may want to use a tool that requires a function as input. To use integrate(), for example, you must provide the integrand as a function. To make your own work easier. Functions make it easier to reuse code or to break larger tasks into smaller parts. 3.6.2 Function parts Functions consist of several parts. Most importantly An argument list. A list of named inputs to the function. These may have default values (or not). There is also a special argument ... which gathers up any other arguments provided by the user. Many R functions make use of ... args(ilogit) # one argument, called x, no default value ## function (x) ## NULL The body. This is the code the tells R to do when the function is executed. body(ilogit) ## { ## exp(x)/(1 + exp(x)) ## } An environment where code is executed. Each function has its own “scratch pad” where it can do work without interfering with global computations. But environments in R are nested, so it is possible to reach outside of this narrow environment to access other things (and possible to change them). For the most part we won’t worry about this, but if you use a variable not defined in your function but defined elsewhere, you may see unexpecte results. If you type the name of a function witout parenthesis, you will see all three parts listed: ilogit ## function (x) ## { ## exp(x)/(1 + exp(x)) ## } ## &lt;bytecode: 0x7f9b1d4c42d8&gt; ## &lt;environment: namespace:mosaicCore&gt; 3.6.3 The function() function has its function To write a function we use the function() function to specify the arguments and the body. (R will assign an environment for us.) The general outline is my_function_name &lt;- function(arg1 = default1, arg2 = default2, arg3, arg4, ...) { # stuff for my function to do } We may include as many named arguments as we like, and some or all or none of them may have default values. The results of the last line of the function are returned. If we like, we can also use the return() function to make it clear what is being returned when. Let’s write a function that adds. (Redundant, but a useful illustration.) foo &lt;- function(x, y = 5) { x + y # or return(x + y) } foo(3, 5) ## [1] 8 foo(3) ## [1] 8 foo(2, x = 3) # Note: this makes y = 2 ## [1] 5 foo(x = 1:3, y = 100) ## [1] 101 102 103 foo(x = 1:3, y = c(100, 200, 300)) # vectorized! ## [1] 101 202 303 foo(x = 1:3, y = c(100, 200)) # You have been warned! ## Warning in x + y: longer object length is not a multiple of shorter object ## length ## [1] 101 202 103 Here is a more useful example. Suppose we want to integrate \\(f(x) = x^2 (2-x)\\) on the interval from 0 to 2. Since this is such a simple function, if we are not going to reuse it, we don’t need to bother naming it, we can just create the function inside our call to integrate(). integrate(function(x) { x^2 * (2-x) }, 0, 2) ## 1.333 with absolute error &lt; 1.5e-14 3.7 Exercises Create a function in R that converts Fahrenheit temperatures to Celsius temperatures. [Hint: \\(C = (F-32) \\cdot 5/9\\).] What you turn in should show the code that defines your function. some test cases that show that your function is working. (Show that -40, 32, 98.6, and 212 convert to -40, 0, 37, and 212.) Note: you should be able to test all these cases by calling the function only once. Use c(-40, 32, 98.6, 212) as the input. The problem uses the KidsFeet data set from the mosaicData package. The hints are suggested functions that might be of use. How many kids are represented in the data set. [Hint: nrow() or dim()] Which of the variables are factors? [Hint: glimpse()] Add a new variable called foot_ratio that is equal to length divided by width. [Hint: mutate()] Add a new variable called biggerfoot2 that has values &quot;dom&quot; (if domhand and biggerfoot are the same) and &quot;nondom&quot; (if domhand and biggerfoot are different). [Hint: mutate(), ==, ifelse()] Create new data set called Boys that contains only the boys. [Hint: filter(), ==] What is the name of the boy with the largest foot_ratio? Show how to find this programmatically, don’t just scan through the whole data set yourself. [Hint: max() or arrange()] 3.8 Footnotes "],
["probability.html", "4 Probability 4.1 Some terminology 4.2 Distributions in R 4.3 Joint, marginal, and conditional distributions 4.4 Exercises 4.5 Footnotes", " 4 Probability knitr::opts_chunk$set( fig.width = 6, fig.height = 2.5 ) library(ggformula) library(dplyr) library(mosaic) theme_set(theme_bw()) 4.1 Some terminology Probability is about quantifying the relative chances of various possible outcomes of a random process. As a very simple example (used to illustrate the terminology below), considering rolling a single 6-sided die. sample space: The set of all possible outcomess of a random process. [{1, 2, 3, 4, 5, 6}] event: a set of outcomes (subset of sample space) [E = {2, 4, 6} is the event that we obtain an even number] probability: a number between 0 and 1 assigned to an event (really a function that assigns numbers to each event). We write this P(E). [P(E) = 1/2 where E = {1, 2, 3}] random variable: a random process that produces a number. [So rolling a die can be considered a random variable.] probability distribution: a description of all possible outcomes and their probabilities. For rolling a die we might do this with a table like this: 1 2 3 4 5 6 1/6 1/6 1/6 1/6 1/6 1/6 support (of a random variable): the set of possible values of a random variable. This is very similar to the sample space. probabilty mass function (pmf): a fuction (often denoted with \\(p\\) or \\(f\\)) that takes possible values of a discrete random variable as input and returns the probability of that outcome. If \\(S\\) is the support of the random variable, then \\[ \\sum_{x \\in S} p(x) = 1 \\] and any function with this property is a pmf. Probabilites of events are obtained by adding the probabilities of all outcomes in the event: \\[ \\operatorname{Pr}(E) = \\sum_{x \\in E} p(x) \\] * pmfs can be represented in a table (like the one above) or graphically with a probability histogram or lollipop plot like the ones below. [These are not for the 6-sided die, as we can tell because the probabilities are not the same for each input; the die rolling exmaple would make very boring plots.] Histograms are generally presented on the density scale so the total area of the histogram is 1. (In this example, the bin widths are 1, so this is the same as being on the probability scale.) probabilty density function (pdf): a function (often denoted with \\(p\\) or \\(f\\)) that takes the possible values of continuous random variable as input and returns the probability density. If \\(S\\) is the support of the random variable, then 2 \\[ \\int_{x \\in S} f(x) \\; dx = 1 \\] and any function with this property is a pmf. Probabilities are obtained by integrating (visualized by the area under the density curve): \\[ \\operatorname{Pr}(a \\le X \\le b) = \\int_a^b f(x) \\; dx \\] kernel function: If \\(\\int_{x \\in S} f(x) \\; dx = k\\) for some real number \\(k\\), then \\(f\\) is a kernel function. We can obtain the pdf from the kernel by dividing by \\(k\\). cumulative distribution function (cdf): a function (often denoted with a capital \\(F\\)) that takes a possible value of a random variable as input and returns the probabilty of obtaining a value less than or equal to the input: \\[ F_X(x) = \\operatorname{Pr}(X \\le x) \\] cdfs can be defined for both discrete and continuous random variables. a family of distributions: is a collection of distributions which share common features but are distinguished by different parameter values. For example, we could have the family of distributions of fair dice random variables. The parameter would tell us how many sides the die has. Statisticians call this family the discrte uniform distributions because all the probabilities are equal (1/6 for 6-sided die, 1/10 for a \\(D_10\\), etc.). We will get to know several important families of distributions, among them the binomial, beta, normal, and t families will be especially useful. You may already be familiar with some or all of these. We will also use distributions that have no name and are only described by a pmf or pdf, or perhaps only by a large number of random samples from which we attempt to estimate the pmf or pdf. 4.2 Distributions in R pmfs, pdfs, and cdfs are available in R for many important families of distributions. You just need to know a few things: each family has a standard abbreviation in R pmf and pdf functions begin with the letter d followed by the family abbreviation cdf functions begin with the letter p follwed by the family abbreviation the inverse of the cdf function is called a quantile function, it starts with the letter q functions beginning with r can generate random samples from a distribution help for any of these functions will tell you what R calls the parameters of the family. gf_dist() can be used to make various plots of distributions. 4.2.1 Example: Normal distributions As an exmple, let’s look the family of normal distributions. If you type dnorm( and then hit TAB or if you type args(dnorm) you can see the arguments for this function. args(dnorm) ## function (x, mean = 0, sd = 1, log = FALSE) ## NULL From this we see that the parameters are called mean and sd and have default value of 0 and 1. These values will be used if we don’t specify somethng else. As with many of the pmf and pdf functions, there is also an option to get back the log of the pmf or pdf by setting log = TRUE. This turns out to be computationally much more efficient in many contexts, as we will see. Let’s begin with some pictures of a normal distribution with mean 10 and standard deviation 1: gf_dist(&quot;norm&quot;, mean = 10, sd = 2, title = &quot;pdf for Norm(10, 2)&quot;) gf_dist(&quot;norm&quot;, mean = 10, sd = 2, kind = &quot;cdf&quot;, title = &quot;cdf for Norm(10, 2)&quot;) Now some exercises. Assume \\(X \\sim {\\sf Norm}(10, 2)\\). What is \\(\\operatorname{Pr}(X \\le 5)\\)? We can see by inspection that it is less that 0.5. pnorm() will give us the value we are after; xpnorm() will provide more verbose output and a plot as well. pnorm(5, mean = 10, sd = 2) ## [1] 0.00621 xpnorm(5, mean = 10, sd = 2) ## ## If X ~ N(10, 2), then ## P(X &lt;= 5) = P(Z &lt;= -2.5) = 0.00621 ## P(X &gt; 5) = P(Z &gt; -2.5) = 0.9938 ## ## [1] 0.00621 What is \\(\\operatorname{Pr}(5 \\le X \\le 10)\\)? pnorm(10, mean = 10, sd = 2) - pnorm(5, mean = 10, sd = 2) ## [1] 0.4938 How tall is the density function at it’s peak? Normal distributions are symmetric about their means, so we need the value of the pdf at 10. dnorm(10, mean = 10, sd = 2) ## [1] 0.1995 What is the mean of a Norm(10, 2) distribution? Ignoring for the moment that we know the answer is 10, we can compute it. Notice the use of dnorm() in the computation. integrate(function(x) x * dnorm(x, mean = 10, sd = 2), -Inf, Inf) ## 10 with absolute error &lt; 0.0011 What is the variance of a Norm(10, 2) distribution? Again, we know the answer is the square of the standard deviation, so 4. But let’s get R to compute it in a way that would work for other distributions as well. integrate(function(x) (x - 10)^2 * dnorm(x, mean = 10, sd = 2), -Inf, Inf) ## 4 with absolute error &lt; 7.1e-05 Simulate a data set with 50 values drawn from a \\({\\sf Norm}(10, 2)\\) distribution and make a histogram of the results and overlay the normal pdf for comparison. x &lt;- rnorm(50, mean = 10, sd = 2) # be sure to use a density histogram so it is on the same scale as the pdf! gf_dhistogram(~ x, bins = 10) %&gt;% gf_dist(&quot;norm&quot;, mean = 10, sd = 2, color = &quot;red&quot;) 4.2.2 Simulating running proportions library(ggformula) library(dplyr) theme_set(theme_bw()) Flips &lt;- tibble( n = 1:500, flip = rbinom(500, 1, 0.5), running_count = cumsum(flip), running_prop = running_count / n ) gf_line( running_prop ~ n, data = Flips, color = &quot;skyblue&quot;, ylim = c(0, 1.0), xlab = &quot;Flip Number&quot;, ylab = &quot;Proportion Heads&quot;, main = &quot;Running Proportion of Heads&quot;) %&gt;% gf_hline(yintercept = 0.5, linetype = &quot;dotted&quot;) 4.3 Joint, marginal, and conditional distributions Sometimes (most of the time, actually) we are interested joint distributions. A joint distribution is the distribution of multiple random variables that result from the same random process. For example, we might roll a pair of dice and obtain two numbers (one for each die). Or we might collect a random sample of people and record the height for each of them. Or me might randomly select one person, but record multiple facts (height and weight, for example). All of these situations are covered by joint distributions.3 4.3.1 Example: Hair and eye color Kruschke illustrates joint distributions with an example of hair and eye color recorded for a number of people. 4 That table below has the proportions for each hair/eye color combination. For example, Hair/Eyes Blue Green Hazel Brown Black 0.034 0.115 0.008 0.025 Blond 0.159 0.012 0.027 0.017 Brown 0.142 0.201 0.049 0.091 Red 0.029 0.044 0.024 0.024 Each value in the table indicates the proportion of people that have a particular hair color and a particular eye color. So the upper left cell says that 3.4% of people have black hair and blue eyes (in this particular sample – the proportions will vary a lot depending on the population of interest). We will denote this as \\[ \\operatorname{Pr}(\\mathrm{Hair} = \\mathrm{black}, \\mathrm{Eyes} = \\mathrm{blue}) = 0.034 \\;. \\] or more succinctly as \\[ p(\\mathrm{black}, \\mathrm{blue}) = 0.034 \\;. \\] This type of probability is called a joint probability because it tells about the probability of both things happening. Use the table above to do the following. What is \\(p(\\mathrm{brown}, \\mathrm{green})\\) and what does that number mean? Add the proportion across each row and down each column. (Record them to the right and along the bottom of the table.) For example, in the first row we get \\[ 0.034 + 0.115 + 0.008 + 0.025 = 0.182 \\;. \\] Explain why \\(p(\\mathrm{black}) = 0.182\\) is good notation for this number. Up to round-off error, the total of all the proportions should be 1. Check that this is true. What proportion of people with black hair have blue eyes? This is called a conditional probability. We denote it as \\(\\operatorname{Pr}(\\mathrm{Eyes} = \\mathrm{blue} \\mid \\mathrm{Hair} = \\mathrm{black})\\). or \\(p(\\mathrm{blue} \\mid \\mathrm{black})\\). Compute some other conditional probabilities. \\(p(\\mathrm{black} \\mid \\mathrm{blue})\\). \\(p(\\mathrm{blue} \\mid \\mathrm{blond})\\). \\(p(\\mathrm{blond} \\mid \\mathrm{blue})\\). \\(p(\\mathrm{brown} \\mid \\mathrm{hazel})\\). \\(p(\\mathrm{hazel} \\mid \\mathrm{brown})\\). There are 32 such conditional probabilities that we can compute from this table. Which is largest? Which is smallest? Write a general formula for computing the conditional probability \\(p(c \\mid r)\\) from the \\(p(r,c)\\) values. (\\(r\\) and \\(c\\) are to remind you of rows and columns.) Write a general formula for computing the conditional probability \\(p(r \\mid c)\\) from the \\(p(r,c)\\) values. If we have continuous random variables, we can do a similar thing. Instead of working with probability, we will work with a pdf. Instead of sums, we will have integrals. Write a general formula for computing each of the following if \\(p(x,y)\\) is a continuous joing pdf. \\(p_X(x) = p(x) =\\) \\(p_Y(y) = p(y) =\\) \\(p_{Y\\mid X}(y\\mid x) = p(y \\mid x) =\\) \\(p_{X\\mid Y}(y\\mid x) = (x \\mid y) =\\) We can expression both versions of conditional probability using a word equation. Fill in the missing numerator and denominator \\[ \\mathrm{conditional} = \\frac{\\phantom{joint}}{\\phantom{marginal}} \\] 4.3.2 Independence If \\(p(x \\mid y) = p(x)\\) (conditional = marginal) for all combinations of \\(x\\) and \\(y\\), we say that \\(X\\) and \\(Y\\) are independent. Use the definitions above to express indendence another way. Are hair and eye color independent in our example? True or False. If we randomly select a card from a standard deck (52 cards, 13 denominations, 4 suits), are suit and denomination independent? Create a table for two independent random variables \\(X\\) and \\(Y\\), each of which takes on only 3 possible values. Now create a table for a different pair \\(X\\) and \\(Y\\) that are not independent but have the same marginal probabilities as in the previous exercise. 4.4 Exercises Suppose a random variable has the pdf \\(p(x) = 6x (1-x)\\) on the interval \\([0,1]\\). (That means it is 0 outside of that interval.) Use function() to create a function in R that is equivlent to p(x). Use gf_function() to plot the function on the interval \\([0, 1]\\). Integrate by hand to show that the total area under the pdf is 1 (as it should be for any pdf). Now have R compute that same integral (using integrate()). What is the largest value of \\(p(x)\\)? At what value of \\(x\\) does it occur? Is it a problem that this value is larger than 1? Hint: differentiation might be useful. Recall that \\(\\operatorname{E}(X) = \\int x f(x) \\;dx\\) for a continuous random variable with pdf \\(f\\) and \\(\\operatorname{E}(X) = \\sum x f(x) \\;dx\\) for a discrete random variable with pmf \\(f\\). (The integral or sum is over the support of the random variable.) Compute the expected value for the following random variables. \\(A\\) is discrete with pmf \\(f(x) = x/10\\) for \\(x \\in \\{1, 2, 3, 4\\}\\). \\(B\\) is continuous with kernel \\(f(x) = x^2(1-x)\\) on \\([0, 1]\\). Hint: first figure out what the pdf is. Compute the variance and standard deviation of each of the distributions in the previous problem. In Bayesian inference, we will often need to come up with a distribution that matches certain features that correspond to our knowledge or intuition about a situation. Find a normal distribution with a mean of 10 such that half of the distribution is within 3 of 10 (ie, between 7 and 13). Hint: use qnorm() to determine how many standard deviations are between 10 and 7. School children were surveyed regarding their favorite foods. Of the total sample, 20% were 1st graders, 20% were 6th graders, and 60% were 11th graders. For each grade, the following table shows the proportion of respondents that chose each of three foods as their favorite. From that information, construct a table of joint probabilities of grade and favorite food. Are grade and favorite food independent? Explain how you ascertained the answer. grade Ice cream Fruit French fries 1st 0.1 0.1 0.6 6th 0.3 0.6 0.3 11th 0.6 0.3 0.1 4.5 Footnotes Kruschke likes to write his integrals in a different order: \\(\\int dx \\; f(x)\\) instead of \\(\\int f(x) \\; dx\\). Either order means the same thing.↩ Kruschke calls these 2-way distributions, but there can be more than variables involved.↩ The datasets package has a version of this data with a third variable: sex. It is also stored in a different format (as a 3d table rather than as a data frame).↩ "],
["bayes-rule-and-the-grid-method.html", "5 Bayes’ Rule and the Grid Method 5.1 Estimating the bias in a coin using the Grid Method 5.2 Exercises", " 5 Bayes’ Rule and the Grid Method 5.1 Estimating the bias in a coin using the Grid Method 5.1.1 Creating a Grid library(purrr) ## ## Attaching package: &#39;purrr&#39; ## The following object is masked from &#39;package:mosaic&#39;: ## ## cross x &lt;- 1; n &lt;- 4 CoinsGrid &lt;- expand.grid( theta = seq(0, 1, by = 0.001) ) %&gt;% mutate( prior = pmin(theta, 1 - theta), # higher if farther from edges prior = prior / sum(prior), # normalize likelihood = map_dbl(theta, ~ dbinom(x = x, size = n, .x)), posterior = prior * likelihood, posterior = posterior / sum(posterior) # normalize ) 5.1.2 Plots from the grid gf_line(prior ~ theta, data = CoinsGrid) gf_line(likelihood ~ theta, data = CoinsGrid) gf_line(posterior ~ theta, data = CoinsGrid) gf_area(prior ~ theta, data = CoinsGrid, alpha = 0.5) gf_area(likelihood ~ theta, data = CoinsGrid, alpha = 0.5) gf_area(posterior ~ theta, data = CoinsGrid, alpha = 0.5) 5.1.3 HDI from the grid Let’s write a function to compute the Highest Density Interval (of the posterior for theta) based on our grid. Since different grids may use different names for the parameter(s) and for the posterior, we’ll write our function in a way that will let us specify those names if we need to, but use posterior and theta by default. And for good measure, we’ll calculate the posterior mode as well. The basic idea (after standardizing the grid) is to sort the grid by the posterior. The mode will be at the end of the list, and the “bottom 95%” will be the HDI (or some other percent if we choose a different level). This method works as long as the posterior is unimodal, increasing to the mode from either side. HDI &lt;- function(formula = posterior ~ theta, grid, level = 0.95) { # Create a standardized version of the grid model.frame(formula, data = grid) %&gt;% # turn formula into data frame setNames(c(&quot;posterior&quot;, &quot;theta&quot;)) %&gt;% # standardrize names mutate( posterior = posterior / sum(posterior) # normalize posterior ) %&gt;% arrange(posterior) %&gt;% # sort by posterior mutate( cum_posterior = cumsum(posterior) # cumulative posterior ) %&gt;% filter( cum_posterior &gt;= 1 - level, # keep highest cum_posterior ) %&gt;% summarise( # summarise what&#39;s left lo = min(theta), hi = max(theta), height = min(posterior), level = level, mode_height = last(posterior), mode = last(theta), ) } HDICoins &lt;- HDI(posterior ~ theta, grid = CoinsGrid) HDICoins lo hi height level mode_height mode 0.098 0.681 5e-04 0.95 0.0024 0.4 With this information in hand, we can add a representation of the 95% HDI to our plot. gf_line(posterior ~ theta, data = CoinsGrid) %&gt;% gf_hline(yintercept = ~height, data = HDICoins, color = &quot;red&quot;, alpha = 0.5) %&gt;% gf_pointrangeh(height ~ mode + lo + hi, data = HDICoins, color = &quot;red&quot;, size = 1) %&gt;% gf_labs(caption = &quot;posterior mode and 95% HPI indicated in red&quot;) 5.1.4 Automating the grid Note: This function is a bit different from CalvinBayes::BernGrid(). MyBernGrid &lt;- function( x, n, prior = dunif, res = 1001, ...) { Grid &lt;- expand.grid( theta = seq(0, 1, length.out = res) ) %&gt;% mutate( prior = prior(theta, ...), prior = prior / sum(prior), likelihood = dbinom(x, n, theta), likelihood = likelihood / sum(likelihood), posterior = prior * likelihood, posterior = posterior / sum(posterior) ) H &lt;- HDI(grid = Grid) gf_line(prior ~ theta, data = Grid, color = ~&quot;prior&quot;, size = 1.15, alpha = 0.8) %&gt;% gf_line(likelihood ~ theta, data = Grid, color = ~&quot;likelihood&quot;, size = 1.15, alpha = 0.7) %&gt;% gf_line(posterior ~ theta, data = Grid, color = ~&quot;posterior&quot;, size = 1.15, alpha = 0.6) %&gt;% gf_pointrangeh( height ~ mode + lo + hi, data = H, color = &quot;red&quot;, size = 1) %&gt;% gf_labs(title = &quot;Prior/Likelihood/Posterior&quot;, subtitle = paste(&quot;Data: n =&quot;, n, &quot;, x =&quot;, x)) %&gt;% gf_refine( scale_color_manual( values = c( &quot;prior&quot; = &quot;forestgreen&quot;, &quot;likelihood&quot; = &quot;blue&quot;, &quot;posterior&quot; = &quot;red&quot;), breaks = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;) )) %&gt;% print() invisible(Grid) # return the Grid, but don&#39;t show it } This function let’s us quickly explore several scenarios and compare the results. How does changing the prior affect the posterior? How does changing the data affect the posterior? library(triangle) MyBernGrid(1, 4, prior = dtriangle, a = 0, b = 1, c = 0.5) MyBernGrid(1, 4, prior = dunif) MyBernGrid(10, 40, prior = dtriangle, a = 0, b = 1, c = 0.5) MyBernGrid(10, 40, prior = dunif) MyBernGrid(1, 4, prior = dtriangle, a = 0, b = 1, c = 0.8) MyBernGrid(10, 40, prior = dtriangle, a = 0, b = 1, c = 0.8) MyBernGrid(10, 40, prior = dbeta, shape1 = 25, shape2 = 12) 5.2 Exercises Consider again the disease and diagnostic test of the previous two exercises. Suppose that a person selected at random from the population gets the test and it comes back negative. Compute the probability that the person has the disease. The person then gets re-tested, and on the second test the result is positive. Compute the probability that the person has the disease. How does the result compare with your answer to Exercise 5.1? "],
["inferring-a-binomial-probability-via-exact-mathematical-analysis.html", "6 Inferring a Binomial Probability via Exact Mathematical Analysis", " 6 Inferring a Binomial Probability via Exact Mathematical Analysis "],
["markov-chain-monte-carlo-mcmc.html", "7 Markov Chain Monte Carlo (MCMC)", " 7 Markov Chain Monte Carlo (MCMC) "],
["jags-just-another-gibbs-sampler.html", "8 JAGS – Just Another Gibbs Sampler 8.1 What JAGS is 8.2 A Complete Example: estimating a proportion 8.3 Example 2: comparing two proportions", " 8 JAGS – Just Another Gibbs Sampler This chapter focuses on a very simple model – one for which JAGS is overkill. This allows us to get familiar with JAGS and the various tools to investigate JAGS models in a simple setting before moving on to more intersting models soon. 8.1 What JAGS is JAGS (Just Another Gibbs Sampler) is an implementation of an MCMC algorithm called Gibbs sampling to sample the posterior distribution of a Bayesian model. We will interact with JAGS from within R using the following packages: R2jags – interface between R and JAGS coda – general tools for analysing and graphing MCMC algorithms bayesplot – a number of useful plots using ggplot2 CalvinBayes – includes some of the functions from Kruschke’s text 8.2 A Complete Example: estimating a proportion 8.2.1 The Model 8.2.2 Load Data The data sets provided as csv files by Kruschke also live in the CalvinBayes package, so you can read this file with library(CalvinBayes) data(&quot;z15N50&quot;) glimpse(z15N50) ## Observations: 50 ## Variables: 1 ## $ y &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,… We see that the data are coded as 50 0’s and 1’s in a variable named y. (You should use better names when creating your own data sets.) 8.2.3 Specify the model We can specify the model by creating a text file containing the JAGS description of the model or by creating a special kind of function. The avoids creating temporary files and keeps tidy in our R markdown documents. The main part of the model description is the same in either style, but notice that the using the function style, we do not need to include model{ ... } in our description. bern_model &lt;- function() { for (i in 1:N) { # each response is Bernoulli with fixed parameter theta y[i] ~ dbern(theta) } theta ~ dbeta(1, 1) # prior for theta } 8.2.4 Run the model R2jags::jags() can be used to run our JAGS model. We need to specify three things: (1) the model we are using (as defined above), (2) the data we are using, (3) the parameters we want saved in the posterior sampling. (theta is the only parameter in this model, but in larger models, we might choose to save only some of the parameters). There are some additional, optional things we might want to control as well. More on those later. For now, let’s fit the model using the default values for everything else. # Load the R2jags package library(R2jags) # Make the same &quot;random&quot; choices each time this is run. # This makes the Rmd file stable so you can comment on specific results. set.seed(123) # Fit the model bern_jags &lt;- jags( data = list(y = z15N50$y, N = nrow(z15N50)), model.file = bern_model, parameters.to.save = c(&quot;theta&quot;) ) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 1 ## Total graph size: 53 ## ## Initializing model Let’s take a quick look at what we have. bern_jags ## Inference for Bugs model at &quot;/var/folders/py/txwd26jx5rq83f4nn0f5fmmm0000gn/T//RtmpCNreoD/model723c4d13f1ba.txt&quot;, fit using jags, ## 3 chains, each with 2000 iterations (first 1000 discarded) ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.308 0.064 0.191 0.261 0.306 0.351 0.435 1.001 3000 ## deviance 62.089 1.395 61.087 61.186 61.571 62.459 65.770 1.001 3000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 1.0 and DIC = 63.1 ## DIC is an estimate of expected predictive error (lower deviance is better). We see that the average value of theta in our posterior distribution is 0.3075. The values of Rhat and n.eff give a quick check that nothing disasterous seems to have happened when we fit this simple model. (Rhat should be very nearly 1 if the MCMC algorithm has converged. Because n.eff is close to \\(3000 = 3 \\cdot 1000\\), this model is sampling the posterior very effeciently.) We can plot the posterior distribution. library(CalvinBayes) head(posterior(bern_jags)) deviance theta 61.85 0.2455 61.13 0.3129 61.13 0.2860 61.13 0.3133 61.83 0.3577 62.85 0.2193 gf_dhistogram(~theta, data = posterior(bern_jags)) %&gt;% gf_dens(~theta, data = posterior(bern_jags)) 8.2.5 Using coda The coda package provides output analysis and diagnostics for MCMC algorithms. In order to use it, we must convert our JAGS object into something coda recognizes. We do with with the as.mcmc() function. bern_mcmc &lt;- as.mcmc(bern_jags) plot(bern_mcmc) Note: Kruschke uses rjags without R2jags, so he does this step using rjags::coda.samples() instead of as.mcmc(). Both functions result in the same thing – posterior samples in a format that coda expects, but they have different starting points. 8.2.6 Using bayesplot The mcmc object we extracted with as.mcmc() can be used by the utilities in the bayesplot(). Here, for example is the bayesplot plot of the posterior distribution for theta. By default, a vertical line segment is drawn at the median of the posterior distribution. library(bayesplot) mcmc_areas( bern_mcmc, pars = c(&quot;theta&quot;), # make a plot for the theta parameter prob = 0.90) # shade the central 90% One advantage of bayesplot is that the plots use the ggplot2 system and so interoperate well with ggformula. mcmc_trace(bern_mcmc, pars = &quot;theta&quot;) # spearate the chains using facets and modify the color scheme mcmc_trace(bern_mcmc, pars = &quot;theta&quot;) %&gt;% gf_facet_grid(Chain ~ .) %&gt;% gf_refine(scale_color_viridis_d()) ## Scale for &#39;colour&#39; is already present. Adding another scale for ## &#39;colour&#39;, which will replace the existing scale. We will encounter additional plots from bayesplot as we go along. 8.2.7 Using Kruschke’s functions I have put (modified versions of) some of functions from Kruschke’s book into the CalvinBayes package so that you don’t have to source his files to use them. 8.2.7.1 diagMCMC() diagMCMC(bern_mcmc, par = &quot;theta&quot;) 8.2.7.2 plotPost() plotPost(bern_mcmc[, &quot;theta&quot;], main = &quot;theta&quot;, xlab = expression(theta)) ESS mean median mode hdiMass hdiLow hdiHigh compVal pGtCompVal ROPElow ROPEhigh pLtROPE pInROPE pGtROPE theta 3000 0.3075 0.3056 0.2982 0.95 0.1896 0.4342 NA NA NA NA NA NA NA plotPost(bern_mcmc[, &quot;theta&quot;], main = &quot;theta&quot;, xlab = expression(theta), cenTend = &quot;median&quot;, compVal = 0.5, ROPE = c(0.45, 0.55), credMass = 0.90) ESS mean median mode hdiMass hdiLow hdiHigh compVal pGtCompVal ROPElow ROPEhigh pLtROPE pInROPE pGtROPE theta 3000 0.3075 0.3056 0.2982 0.9 0.1983 0.4072 0.5 0.0033 0.45 0.55 0.9867 0.013 3e-04 8.2.8 Optional arguments to jags() 8.2.8.1 Number and size of chains Sometimes we want to use more or longer chains (or fewer or shorter chains if we are doing a quick preliminary check before running longer chains later). jags() has three arguments for this: n.chains: number of chains n.iter: number of iterations per chain n.burning: number of burn in steps per chain set.seed(76543) bern_jags2 &lt;- jags( data = list(y = z15N50$y, N = nrow(z15N50)), model.file = bern_model, parameters.to.save = c(&quot;theta&quot;), n.chains = 4, n.iter = 5000, n.burnin = 1000, ) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 1 ## Total graph size: 53 ## ## Initializing model 8.2.8.2 Starting point for chains We can also control the starting point for the chains. Starting different chains and quite different parameter values can help * verify that the MCMC algorthm is not overly sensitive to where we are starting from, and * ensure that the MCMC algorithm has explored the posterior distribution sufficiently. On the other hand, if we start a chain too far from the peak of the posterior distribution, the chain may have trouble converging. We can provide either specific starting points for each chain or a function that generates random starting points. gf_dist(&quot;beta&quot;, shape1 = 3, shape2 = 3) set.seed(2345) bern_jags3 &lt;- jags( data = list(y = z15N50$y, N = nrow(z15N50)), model.file = bern_model, parameters.to.save = c(&quot;theta&quot;), n.chains = 4, n.iter = 5000, n.burnin = 1000, # start each chain &quot;near&quot; 0.5 inits = function() list(theta = rbeta(1, 3, 3)) ) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 1 ## Total graph size: 53 ## ## Initializing model bern_jags4 &lt;- jags( data = list(y = z15N50$y, N = nrow(z15N50)), model.file = bern_model, parameters.to.save = c(&quot;theta&quot;), n.chains = 3, n.iter = 550, n.burnin = 500, # choose specific starting point for each chain inits = list( list(theta = 0.5), list(theta = 0.3), list(theta = 0.7) ) ) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 1 ## Total graph size: 53 ## ## Initializing model mcmc_trace(as.mcmc(bern_jags4), pars = &quot;theta&quot;) 8.2.8.3 Running chains in parallel Although this model runs very quickly, others models may take considerably longer. We can use jags.parallel() in place of jags() to take advantage of multiple cores to run more than one chain at a time. jags.seed can be used to set the seed for the parallel random number generator used. (Note: set.seed() does not work when using jags.parallel() and jags.seed has no effect when using jags().) library(R2jags) bern_jags3 &lt;- jags.parallel( data = list(y = z15N50$y, N = nrow(z15N50)), model.file = bern_model, parameters.to.save = c(&quot;theta&quot;), n.chains = 4, n.iter = 5000, n.burnin = 1000, jags.seed = 12345 ) 8.3 Example 2: comparing two proportions 8.3.1 The data Suppose we want to compare Reginald and Tony’s abilities to hit a target (with a dart, perhaps). For each attempt, we record two pieces of information: the person making the attampt (the subject) and whether the attempt succeeded (0 or 1). library(mosaic) head(z6N8z2N7) y s 1 Reginald 0 Reginald 1 Reginald 1 Reginald 1 Reginald 1 Reginald # Let&#39;s do some renaming Target &lt;- z6N8z2N7 %&gt;% rename(hit = y, subject = s) df_stats(hit ~ subject, data = Target, props, attempts = length) subject prop_0 prop_1 attempts Reginald 0.2500 0.7500 8 Tony 0.7143 0.2857 7 8.3.2 The model Now our model is that each person has his own success rate – we have **two \\(\\theta\\)’s, one for Reginald and one for Tony. We express this as hit ~ dbern(theta[subject[i]]) where subject[i] tells us which subject the ith observation was for. 8.3.3 Describing the model to JAGS bern2_model &lt;- function() { for (i in 1:Nobs) { # each response is Bernoulli with the appropriate theta hit[i] ~ dbern(theta[subject[i]]) } for (s in 1:Nsub) { theta[s] ~ dbeta(2, 2) # prior for each theta } } JAGS will also need access to four pieces of information from our data set: a vector of hit values a vector of subject values – coded as integers 1 and 2 (so that subject[i] makes sense to JAGS. (In general, JAGS is much less fluid in handling data than R is, so we often need to do some manual data conversion for JAGSS.) Nobs – the total number of observations Nsub – the number of subjects We will prepare these as a list. TargetList &lt;- list( Nobs = nrow(Target), Nsub = 2, hit = Target$hit, subject = as.numeric(as.factor(Target$subject)) ) TargetList ## $Nobs ## [1] 15 ## ## $Nsub ## [1] 2 ## ## $hit ## [1] 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 ## ## $subject ## [1] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 8.3.4 Fitting the model bern2_jags &lt;- jags( data = TargetList, model.file = bern2_model, parameters.to.save = &quot;theta&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 15 ## Unobserved stochastic nodes: 2 ## Total graph size: 35 ## ## Initializing model 8.3.5 Inspecting the results bern2_mcmc &lt;- as.mcmc(bern2_jags) diagMCMC(bern2_mcmc) # bayesplot plots mcmc_acf(bern2_mcmc) mcmc_acf_bar(bern2_mcmc) mcmc_pairs(bern2_mcmc, pars = c(&quot;theta[1]&quot;, &quot;theta[2]&quot;)) mcmc_combo(bern2_mcmc) mcmc_combo(bern2_mcmc, combo = c(&quot;dens&quot;, &quot;dens_overlay&quot;, &quot;trace&quot;, &quot;scatter&quot;), pars = c(&quot;theta[1]&quot;, &quot;theta[2]&quot;)) Here is a list of mcmc_ functions available: apropos(&quot;^mcmc_&quot;) ## [1] &quot;mcmc_acf&quot; &quot;mcmc_acf_bar&quot; ## [3] &quot;mcmc_areas&quot; &quot;mcmc_areas_data&quot; ## [5] &quot;mcmc_areas_ridges&quot; &quot;mcmc_areas_ridges_data&quot; ## [7] &quot;mcmc_combo&quot; &quot;mcmc_dens&quot; ## [9] &quot;mcmc_dens_chains&quot; &quot;mcmc_dens_chains_data&quot; ## [11] &quot;mcmc_dens_overlay&quot; &quot;mcmc_hex&quot; ## [13] &quot;mcmc_hist&quot; &quot;mcmc_hist_by_chain&quot; ## [15] &quot;mcmc_intervals&quot; &quot;mcmc_intervals_data&quot; ## [17] &quot;mcmc_neff&quot; &quot;mcmc_neff_data&quot; ## [19] &quot;mcmc_neff_hist&quot; &quot;mcmc_nuts_acceptance&quot; ## [21] &quot;mcmc_nuts_divergence&quot; &quot;mcmc_nuts_energy&quot; ## [23] &quot;mcmc_nuts_stepsize&quot; &quot;mcmc_nuts_treedepth&quot; ## [25] &quot;mcmc_pairs&quot; &quot;mcmc_parcoord&quot; ## [27] &quot;mcmc_parcoord_data&quot; &quot;mcmc_recover_hist&quot; ## [29] &quot;mcmc_recover_intervals&quot; &quot;mcmc_recover_scatter&quot; ## [31] &quot;mcmc_rhat&quot; &quot;mcmc_rhat_data&quot; ## [33] &quot;mcmc_rhat_hist&quot; &quot;mcmc_scatter&quot; ## [35] &quot;mcmc_trace&quot; &quot;mcmc_trace_highlight&quot; ## [37] &quot;mcmc_violin&quot; The functions ending in _data() return the data used to make the corresponding plot. This can be useful if you want to display that same infromation in a different way or if you just want to inspect the data to make sure you understand the plot. 8.3.6 Differnce in proportions If we are primarily interested in the difference between Reginald and Tony, we can plot the difference in their theta values. head(posterior(bern2_jags)) deviance theta.1 theta.2 18.54 0.5708 0.2464 23.64 0.3772 0.5269 18.69 0.6122 0.4316 18.98 0.5956 0.1552 19.26 0.5277 0.2124 19.27 0.5204 0.3450 gf_density( ~(theta.1 - theta.2), data = posterior(bern2_jags)) 8.3.7 Sampling from the prior To sample from the prior, we must do the following: remove the response variable from our data list change Nobs to 0 set DIC = FALSE in the call to jags(). This will run the model witout any data, which means the posterior will be the same as the prior. # make a copy of our data list TargetList0 &lt;- list( Nobs = 0, Nsub = 2, subject = as.numeric(as.factor(Target$subject)) ) bern2_jags0 &lt;- jags( data = TargetList0, model.file = bern2_model, parameters.to.save = c(&quot;theta&quot;), DIC = FALSE) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 2 ## Total graph size: 20 ## ## Initializing model 8.3.7.1 Note about : in JAGS and in R From the JAGS documentation: &gt; The sequence operator : can only produce increasing sequences. If n &lt; m then m:n produces a vector of length zero and when this is used in a for loop index expression the contents of loop inside the curly brackets are skipped. Note that this behaviour is different from the sequence operator in R, where m:n will produce a decreasing sequence if n &lt; m. So in our JAGS model, 1:0 correctly represents no data (and no trips through the for loop). 8.3.7.2 What good is it to generate samples from the prior? Our model set priors for \\(\\theta_1\\) and \\(\\theta_2\\), but this implies a distribution for \\(\\theta_1 - \\theta_2\\), and we might like to see what that distribution looks like. gf_density( ~(theta.1 - theta.2), data = posterior(bern2_jags0)) "],
["heierarchical-models.html", "9 Heierarchical Models 9.1 One coin from one mint 9.2 Multiple coins from one mint 9.3 Multiple coins from multiple mints", " 9 Heierarchical Models 9.1 One coin from one mint 9.2 Multiple coins from one mint 9.3 Multiple coins from multiple mints "]
]
