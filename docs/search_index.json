[
["multiple-metric-predictors.html", "18 Multiple Metric Predictors 18.1 SAT 18.2 Exercises", " 18 Multiple Metric Predictors 18.1 SAT 18.1.1 SAT vs expenditure Does spending more on education result in higher SAT scores? Data from 1999 (published in a paper by Gruber) can be used to explore this question. Among other things, the data includes average total SAT score (on a 400-1600 scale) and the amount of money spent on education (in 1000s of dollars per student) in each state. As a first attempt, we could fit a linear model (sat ~ expend). Using centering, the core of the model looks like this: for (i in 1:length(y)) { y[i] ~ dt(mu[i], 1/sigma^2, nu) mu[i] &lt;- alpha0 + alpha1 * (x[i] - mean(x)) } alpha1 measures how much better SAT performance is for each $1000 spent on education in a state. To fit the model, we need priors on our four parameters: nu: We can use our usual shifted exponential. sigma: {Unif}(?, ?) alpha0: {Norm}(?, ?) alpha1: {Norm}(0, ?) The question marks depend on the scale of our variables. If we build those into our model, and provide the answers as part of our data, we can use the same model for multiple data sets, even if they are at different scales. sat_model &lt;- function() { for (i in 1:length(y)) { y[i] ~ dt(mu[i], 1/sigma^2, nu) mu[i] &lt;- alpha0 + alpha1 * (x[i] - mean(x)) } nuMinusOne ~ dexp(1/29.0) nu &lt;- nuMinusOne + 1 alpha0 ~ dnorm(alpha0mean, 1 / alpha0sd^2) alpha1 ~ dnorm(0, 1 / alpha1sd^2) sigma ~ dunif(sigma_lo, sigma_hi * 1000) log10nu &lt;- log(nu) / log(10) # log10(nu) beta0 &lt;- alpha0 - mean(x) * alpha1 # true intercept } So how do we fill in the question marks for this data set? sigma: {Unif}(?,?) This quantifies the amount of variation from state to state among states that have the same per student expenditure. The scale of the SAT ranges from 400 to 1600. Statewide averages will not be near the extremes of this scale. A 6-order of maginitude window around 1 gives {Unif}(0.001, 1000), both ends of which are plenty far from what we think is reasonable. alpha0: {Norm}(?, ?) alpha0 measures the average SAT score for states that spend an average amount. Since average SATs are around 1000, something like {Norm}(1000, 100) seems reasable. alpha1: {Norm}(0, ?) This is the trickiest one. The slope of a regression line can’t be much more than \\(\\frac{SD_y}{SD_x}\\), so we can either estimate that ratio or compute it from our data to guide our choice of prior. library(R2jags) sat_jags &lt;- jags( model = sat_model, data = list( y = SAT$sat, x = SAT$expend, alpha0mean = 1000, # SAT scores are roughly 500 + 500 alpha0sd = 100, # broad prior on scale of 400 - 1600 alpha1sd = 4 * sd(SAT$sat) / sd(SAT$expend), sigma_lo = 0.001, # 3 o.m. less than 1 sigma_hi = 1000 # 3 o.m. greater than 1 ), parameters.to.save = c(&quot;nu&quot;, &quot;log10nu&quot;, &quot;alpha0&quot;, &quot;beta0&quot;, &quot;alpha1&quot;, &quot;sigma&quot;), n.iter = 4000, n.burnin = 1000, n.chains = 3 ) ## module glm loaded sat_jags ## Inference for Bugs model at &quot;/var/folders/py/txwd26jx5rq83f4nn0f5fmmm0000gn/T//RtmpIDZHjk/model6e140e93c1.txt&quot;, fit using jags, ## 3 chains, each with 4000 iterations (first 1000 discarded), n.thin = 3 ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## alpha0 966.30 10.477 945.217 959.654 966.396 973.052 987.086 1.001 3000 ## alpha1 -21.40 7.370 -36.165 -26.181 -21.389 -16.622 -6.772 1.001 2700 ## beta0 1092.68 45.112 1003.147 1063.878 1092.556 1121.854 1182.939 1.001 2100 ## log10nu 1.51 0.328 0.823 1.295 1.524 1.744 2.105 1.001 3000 ## nu 42.31 32.618 6.659 19.731 33.425 55.419 127.370 1.001 3000 ## sigma 69.95 7.860 56.467 64.621 69.186 74.728 87.222 1.001 3000 ## deviance 568.53 2.742 565.260 566.512 567.872 569.868 575.345 1.002 2700 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 3.8 and DIC = 572.3 ## DIC is an estimate of expected predictive error (lower deviance is better). diag_mcmc(as.mcmc(sat_jags)) mcmc_combo(as.mcmc(sat_jags)) Our primary interest is alpha1. summary_df(sat_jags) %&gt;% filter(param == &quot;alpha1&quot;) param mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff alpha1 -21.4 7.37 -36.16 -26.18 -21.39 -16.62 -6.772 1.001 2700 plot_post(posterior(sat_jags)$alpha1, xlab = &quot;alpha1&quot;, ROPE = c(-5, 5)) ## $posterior ## ESS mean median mode ## var1 3000 -21.4 -21.39 -21.16 ## ## $hdi ## prob lo hi ## 1 0.95 -36.24 -6.892 ## ## $ROPE ## lo hi P(&lt; ROPE) P(in ROPE) P(&gt; ROPE) ## 1 -5 5 0.986 0.01367 0.0003333 hdi(sat_jags, pars = &quot;alpha1&quot;, prob = 0.95) par lo hi prob chain alpha1 -37.02 -7.286 0.95 1 alpha1 -36.42 -8.540 0.95 2 alpha1 -35.29 -5.200 0.95 3 This seems odd: Nearly all the credible values for alpha1 are negative? Can we really raise SAT scores by cutting funding to schools? Maybe we should look at the raw data with our model overlaid. gf_point(sat ~ expend, data = SAT) %&gt;% gf_abline(slope = ~ alpha1, intercept = ~ beta0, data = posterior(sat_jags) %&gt;% sample_n(2000), alpha = 0.01, color = &quot;steelblue&quot;) That’s a lot of scatter, and the negative trend is heavily influenced by the 4 states that spend the most (and have relatively low SAT scores). We could do a bit more with this model, for exapmle we could * fit without those 4 states to see how much they are driving the negative trend; * do some PPC to see if the model is reasonable. But instead will will explore another model, one that has two predictors. 18.1.2 SAT vs expenditure and percent taking the test We have some additional data about each state. Let’s fit a model with two predictors: expend and frac. SAT %&gt;% head(4) state expend ratio salary frac verbal math sat Alabama 4.405 17.2 31.14 8 491 538 1029 Alaska 8.963 17.6 47.95 47 445 489 934 Arizona 4.778 19.3 32.17 27 448 496 944 Arkansas 4.459 17.1 28.93 6 482 523 1005 Here’s our model for (robust) multiple linear regression: Coding it in JAGS requires adding in the additional predictor: sat_model2 &lt;- function() { for (i in 1:length(y)) { y[i] ~ dt(mu[i], 1/sigma^2, nu) mu[i] &lt;- alpha0 + alpha1 * (x1[i] - mean(x1)) + alpha2 * (x2[i] - mean(x2)) } nuMinusOne ~ dexp(1/29.0) nu &lt;- nuMinusOne + 1 alpha0 ~ dnorm(alpha0mean, 1 / alpha0sd^2) alpha1 ~ dnorm(0, 1 / alpha1sd^2) alpha2 ~ dnorm(0, 1 / alpha2sd^2) sigma ~ dunif(sigma_lo, sigma_hi * 1000) beta0 &lt;- alpha0 - mean(x1) * alpha1 - mean(x2) * alpha2 log10nu &lt;- log(nu) / log(10) } library(R2jags) sat2_jags &lt;- jags( model = sat_model2, data = list( y = SAT$sat, x1 = SAT$expend, x2 = SAT$frac, alpha0mean = 1000, # SAT scores are roughly 500 + 500 alpha0sd = 100, # broad prior on scale of 400 - 1600 alpha1sd = 4 * sd(SAT$sat) / sd(SAT$expend), alpha2sd = 4 * sd(SAT$sat) / sd(SAT$frac), sigma_lo = 0.001, sigma_hi = 1000 ), parameters.to.save = c(&quot;log10nu&quot;, &quot;alpha0&quot;, &quot;alpha1&quot;, &quot;alpha2&quot;, &quot;beta0&quot;,&quot;sigma&quot;), n.iter = 4000, n.burnin = 1000, n.chains = 3 ) sat2_jags ## Inference for Bugs model at &quot;/var/folders/py/txwd26jx5rq83f4nn0f5fmmm0000gn/T//RtmpIDZHjk/model6e1e0e7029.txt&quot;, fit using jags, ## 3 chains, each with 4000 iterations (first 1000 discarded), n.thin = 3 ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## alpha0 965.801 4.754 956.565 962.669 965.735 968.869 975.316 1.001 3000 ## alpha1 12.906 4.345 4.138 10.061 12.994 15.709 21.563 1.001 3000 ## alpha2 -2.885 0.225 -3.318 -3.039 -2.885 -2.734 -2.441 1.001 2000 ## beta0 991.250 22.164 947.709 976.934 990.995 1006.116 1035.153 1.001 3000 ## log10nu 1.379 0.367 0.661 1.118 1.393 1.642 2.048 1.001 2600 ## sigma 31.586 3.807 24.563 28.947 31.409 34.047 39.761 1.001 3000 ## deviance 491.060 3.010 487.234 488.830 490.421 492.588 498.706 1.001 2100 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 4.5 and DIC = 495.6 ## DIC is an estimate of expected predictive error (lower deviance is better). diag_mcmc(as.mcmc(sat2_jags)) mcmc_combo(as.mcmc(sat2_jags)) summary_df(sat2_jags) %&gt;% filter(param == &quot;alpha1&quot;) param mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff alpha1 12.91 4.345 4.138 10.06 12.99 15.71 21.56 1.001 3000 plot_post(posterior(sat2_jags)$alpha1, xlab = &quot;alpha1&quot;, ROPE = c(-5, 5)) ## $posterior ## ESS mean median mode ## var1 2662 12.91 12.99 13.58 ## ## $hdi ## prob lo hi ## 1 0.95 3.742 21.12 ## ## $ROPE ## lo hi P(&lt; ROPE) P(in ROPE) P(&gt; ROPE) ## 1 -5 5 0 0.03733 0.9627 hdi(sat2_jags, pars = &quot;alpha1&quot;, prob = 0.95) par lo hi prob chain alpha1 4.138 21.60 0.95 1 alpha1 4.354 21.12 0.95 2 alpha1 4.704 22.34 0.95 3 summary_df(sat2_jags) %&gt;% filter(param == &quot;alpha2&quot;) param mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff alpha2 -2.885 0.225 -3.317 -3.039 -2.885 -2.734 -2.441 1.002 2000 plot_post(posterior(sat2_jags)$alpha2, xlab = &quot;alpha2&quot;) ## $posterior ## ESS mean median mode ## var1 2504 -2.885 -2.885 -2.88 ## ## $hdi ## prob lo hi ## 1 0.95 -3.329 -2.465 hdi(sat2_jags, pars = &quot;alpha2&quot;, prob = 0.95) par lo hi prob chain alpha2 -3.355 -2.484 0.95 1 alpha2 -3.329 -2.490 0.95 2 alpha2 -3.327 -2.465 0.95 3 gf_point(sat ~ expend, data = SAT) %&gt;% gf_abline(slope = ~ alpha1, intercept = ~ beta0, data = posterior(sat2_jags) %&gt;% sample_n(2000), alpha = 0.01, color = &quot;steelblue&quot;) gf_point(expend ~ frac, data = SAT) 18.1.3 Multiple predictors in pictures 18.1.3.1 If the predictors are uncorrelated 18.1.3.2 Correlated predictors 18.1.3.3 SAT model 18.2 Exercises Fit a model that predicts student-teacher ratio (ratio) from ependiture (expend). Is spending a good predictor of student-teacher ratio? Fit a model that predicts SAT scores from student-teacher ratio (ratio) and the fraction of students who take the SAT (frac). How does this model compare with the model that uses expend and ratio as predictors? "]
]
