[
["count-response.html", "24 Count Response 24.1 Hair and eye color data 24.2 Are hair and eye color independent? 24.3 Poisson model", " 24 Count Response 24.1 Hair and eye color data HairEyeColor %&gt;% tidyr::spread(Eye, Count) Hair Blue Brown Green Hazel Black 20 68 5 15 Blond 94 7 16 10 Brown 84 119 29 54 Red 17 26 14 14 gf_tile(Count ~ Hair + Eye, data = HairEyeColor) %&gt;% gf_refine(scale_fill_viridis_c(option = &quot;C&quot;)) 24.2 Are hair and eye color independent? You probably suspect not. We expect blue eyes to be more common among blond-haired people than among black-haired people, perhaps. How do we fit a model to test if our intuition is correct using the data above? If the rows and columns of the table were independent, then for each row \\(r\\) and column \\(c\\) the probability of being in a row \\(r\\) and column \\(c\\) would be the product of the probabilities of being in row \\(r\\) and of being in column \\(c\\): \\[ \\begin{align*} \\frac{\\mu_{r c}}{N} &amp;= \\frac{y_{r\\cdot}}{N} \\cdot \\frac{y_{\\cdot c}}{N} \\\\ \\mu_{r c} &amp;= \\frac{1}{N} \\cdot y_{r\\cdot} \\cdot y_{\\cdot c} \\\\ \\log(\\mu_{r c}) &amp;= \\underbrace{\\log(\\frac{1}{N})}_{\\alpha_0}\\cdot1 + \\underbrace{\\log(y_{r\\cdot})}_{\\alpha_{r \\cdot}}\\cdot1 + \\underbrace{\\log(y_{\\cdot c})}_{\\alpha_{\\cdot c}}\\cdot1 \\\\ \\log(\\mu) &amp;= \\alpha_0 + \\sum_{r = 1}^R \\alpha_{r\\cdot} [\\![ \\mathrm{in\\ row\\ } r ]\\!] + \\sum_{c = 1}^C \\alpha_{\\cdot c} [\\![ \\mathrm{in\\ column\\ } c ]\\!] \\\\ \\log(\\mu) &amp;= \\underbrace{(\\alpha_0 + \\alpha_{1\\cdot} + \\alpha_{\\cdot 1})}_{\\beta_0} + \\sum_{r = 2}^R \\alpha_{r\\cdot} [\\![ \\mathrm{in\\ row\\ } r ]\\!] + \\sum_{c=2}^C \\alpha_{\\cdot c} [\\![ \\mathrm{in\\ column\\ } c ]\\!] \\\\ \\log(\\mu) &amp;= \\beta_0 + \\sum_{r = 2}^R \\beta_{r\\cdot} [\\![ \\mathrm{in\\ row\\ } r ]\\!] + \\sum_{c=2}^C \\beta_{\\cdot c} [\\![ \\mathrm{in\\ column\\ } c ]\\!] \\end{align*} \\] This looks exactly like our additive linear model (on the log scale) and so the common name for this model is the log linear model. If the rows and columns are not independent, then we will have non-zero interaction terms indicating how far things are from independent. We know how to add in interaction terms, so we are good to go there. All that remains is to come up with a good distribution that turns a mean \\(\\mu\\) into a count. We don’t expect the cell count $y_{rc} to be exactly \\(\\mu_{rc}\\) (especially when \\(\\mu_{rc}\\) in not an integer!). But values close to \\(\\mu_{rc}\\) should be more likely than values farther away from \\(\\mu_{rc}\\). A Poisson distribution has exactly these properties and makes a good model for the noise in this situation. Poisson distributions have one parameter (often denoted \\(\\lambda\\)) satisfying \\[ \\begin{align*} \\mathrm{mean} &amp;= \\lambda \\\\ \\mathrm{variance} &amp;= \\lambda \\\\ \\mathrm{standard\\ deviation} &amp;= \\sqrt{\\lambda} \\\\ \\end{align*} \\] Here are several examples of Poisson distributions. Notice that \\(\\lambda\\) need not be an integer, but all of the values produced by a Poisson random process are integers. gf_dist(&quot;pois&quot;, lambda = 1.8) gf_dist(&quot;pois&quot;, lambda = 5.8) gf_dist(&quot;pois&quot;, lambda = 25.8) gf_dist(&quot;pois&quot;, lambda = 254.8) The Poisson distributions become more and more symmetric as \\(\\lambda\\) increases. In fact, they become very nearly a normal distribution.1 24.3 Poisson model The discussion above gives us enough information to create the appropriate model in R using brm(). color_brm &lt;- brm(Count ~ Hair * Eye, data = HairEyeColor, family = poisson(link = log)) ## Compiling the C++ model ## Start sampling color_brm ## Family: poisson ## Links: mu = log ## Formula: Count ~ Hair * Eye ## Data: HairEyeColor (Number of observations: 16) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 2.97 0.23 2.51 3.39 1054 1.00 ## HairBlond 1.57 0.25 1.09 2.06 1174 1.00 ## HairBrown 1.46 0.25 0.96 1.96 1087 1.00 ## HairRed -0.17 0.34 -0.86 0.47 1361 1.00 ## EyeBrown 1.25 0.26 0.75 1.76 1122 1.00 ## EyeGreen -1.45 0.53 -2.56 -0.51 1388 1.00 ## EyeHazel -0.29 0.35 -0.99 0.38 1211 1.00 ## HairBlond:EyeBrown -3.91 0.48 -4.89 -3.01 1729 1.00 ## HairBrown:EyeBrown -0.89 0.30 -1.49 -0.31 1208 1.00 ## HairRed:EyeBrown -0.81 0.41 -1.62 -0.00 1493 1.00 ## HairBlond:EyeGreen -0.35 0.59 -1.44 0.86 1592 1.00 ## HairBrown:EyeGreen 0.37 0.57 -0.68 1.56 1459 1.00 ## HairRed:EyeGreen 1.25 0.65 0.03 2.58 1565 1.00 ## HairBlond:EyeHazel -2.00 0.48 -2.93 -1.09 1705 1.00 ## HairBrown:EyeHazel -0.15 0.40 -0.90 0.64 1317 1.00 ## HairRed:EyeHazel 0.09 0.51 -0.89 1.09 1449 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our main question is whether any of the interaction terms are credibly different from 0. That would indicate a cell that has more or fewer observations than we would expect if rows and columns were independent. We can construct contrasts to look at particular ways in which independence might fail. color2_brm &lt;- brm(Count ~ Hair + Eye, data = HairEyeColor, family = poisson(link = log)) ## Compiling the C++ model ## Start sampling The model with interaction has higher estimated elpd than the model without interaction terms, an indication that there are credible interaction effects. loo_compare(waic(color_brm), waic(color2_brm)) elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic color_brm 0.0 0.00 -53.98 1.856 8.097 0.1355 108.0 3.713 color2_brm -108.1 48.45 -162.10 48.633 68.357 28.5582 324.2 97.265 (LOO gives a similar result, but requires starting from scratch for several observations, so it is slower.) As an example, let’s test whether blond-haired people are more likely to have blue eyes than black-haired people. We don’t want to compare counts, however, since the number of blond-haired and black-haired people is not equal. Differences on a log scale are ratios on the natural scale. So we might compare \\[ \\begin{align*} \\log(\\mu_{\\mathrm{blond,\\ blue}}) - \\log(\\mu_{\\mathrm{blond,\\ not\\ blue}}) &amp;= \\log\\left( \\frac{\\mu_{\\mathrm{blond,\\ blue}}} {\\mu_{\\mathrm{blond,\\ not\\ blue}}}\\right) \\end{align*} \\] with \\[ \\begin{align*} \\log(\\mu_{\\mathrm{black,\\ blue}}) - \\log(\\mu_{\\mathrm{black,\\ not\\ blue}}) &amp;= \\log\\left( \\frac{\\mu_{\\mathrm{black,\\ blue}}} {\\mu_{\\mathrm{black,\\ not\\ blue}}}\\right) \\end{align*} \\] If those two quantities are equal, then the log odds, hence odds, hence probability of having blue eyes is the same in both groups. Let’s build the corresponding contrast and find out. Since the intercept coefficient shows up in every term (and then cancels out), we can drop it from our contrast to save some typing. Similarly in the blond difference b_HairBlond drops out and in the black-haired difference b_HairBlack drops out. Things are further simplified because blue eyes and black hair are the reference groups (because they come alphabetially first). Post &lt;- posterior(color_brm) names(Post) ## [1] &quot;b_Intercept&quot; &quot;b_HairBlond&quot; &quot;b_HairBrown&quot; &quot;b_HairRed&quot; ## [5] &quot;b_EyeBrown&quot; &quot;b_EyeGreen&quot; &quot;b_EyeHazel&quot; &quot;b_HairBlond:EyeBrown&quot; ## [9] &quot;b_HairBrown:EyeBrown&quot; &quot;b_HairRed:EyeBrown&quot; &quot;b_HairBlond:EyeGreen&quot; &quot;b_HairBrown:EyeGreen&quot; ## [13] &quot;b_HairRed:EyeGreen&quot; &quot;b_HairBlond:EyeHazel&quot; &quot;b_HairBrown:EyeHazel&quot; &quot;b_HairRed:EyeHazel&quot; ## [17] &quot;lp__&quot; Post &lt;- Post %&gt;% mutate( contrast = 0 - (b_EyeBrown + `b_HairBlond:EyeBrown` + b_EyeGreen + `b_HairBlond:EyeGreen` + b_EyeHazel + `b_HairBlond:EyeHazel`) / 3 + - 0 + (b_EyeBrown + b_EyeGreen + b_EyeHazel) / 3 ) hdi(Post, pars = ~ contrast) par lo hi prob contrast 1.376 2.782 0.95 plot_post(Post$contrast) ## $posterior ## ESS mean median mode ## var1 1358 2.085 2.084 2.141 ## ## $hdi ## prob lo hi ## 1 0.95 1.376 2.782 As expected, the posterior distribution for this contrast is shifted well away from 0, an indication that the proportion of blond-haired people with blue eyes is credibly higher than the proportion of black-haired people with blue eyes. The log odds ratio is about 2 (posterior HDI suggests somewhere betweeen 1.4 and 2.7). and the odds ratio can be obtained by exponentiation. hdi(Post, pars = ~ exp(contrast)) par lo hi prob exp(contrast) 3.359 14.97 0.95 plot_post(exp(Post$contrast)) ## $posterior ## ESS mean median mode ## var1 1283 8.602 8.036 7.007 ## ## $hdi ## prob lo hi ## 1 0.95 3.359 14.97 Unfortunately, we can’t convert the odds ratio directly into a relative risk. \\[ \\begin{align*} \\mathrm{odds\\ ratio} &amp;= \\frac{p_1 / (1-p_1)}{p_2 / (1-p_2)} \\\\ &amp;= \\frac{p_1}{1-p_1} \\cdot \\frac{1-p_2}{p_2} \\\\ &amp;= \\frac{p_1}{p_2}\\cdot \\frac{1-p_2}{1-p_1} \\\\ &amp;= \\mathrm{relative\\ risk} \\cdot \\frac{1-p_2}{1-p_1} \\\\ \\end{align*} \\] Relative risk and odds ratio are numerically close when \\(\\frac{1-p_2}{1-p_1}\\) is close to 1, which happens when \\(p_1\\) and \\(p_2\\) are both quite small. Take Stat 343 to find out why.↩ "]
]
