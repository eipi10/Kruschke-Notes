[
["index.html", "(Re)Doing Bayesain Data Analysis Introduction", " (Re)Doing Bayesain Data Analysis R Pruim 2019-01-07 Introduction This “book” is a companion to Kruschke’s Doing Bayesian Data Analysis. The main reasons for this companion are to use a different style of R code that includes: use of modern packages like tidyverse, R2jags, bayesplot, and ggformula; adherence to a different style guide; less reliance on manually editing scripts and more use of resusable code available in packages; a workflow that takes advantage of RStudio and RMarkdown. This is a work in progress. Please accept my apologies in advance for errors, inconsistencies lack of complete coverage But feel free to post issues on github if you spot things that require attention or care to make suggestions for improvement. I’ll be teaching from this book in Spring 2019, so I expect rapid development during those months. "],
["credibility-models-and-parameters.html", "Chapter 1 Credibility, Models, and Parameters 1.1 Bayesian Inference is Reallocation of Credibility Across Possibilities 1.2 Possibilities Are Parameter Values in Descriptive Models 1.3 The Steps of Bayesian Data Analysis", " Chapter 1 Credibility, Models, and Parameters 1.1 Bayesian Inference is Reallocation of Credibility Across Possibilities 1.2 Possibilities Are Parameter Values in Descriptive Models 1.3 The Steps of Bayesian Data Analysis In general, Bayesian analysis of data follows these steps: Identify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors? Define a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis. Specify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists. Use Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step). Check that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model. "],
["some-useful-bits-of-r.html", "Chapter 2 Some Useful Bits of R 2.1 Vectors, Lists, and Data Frames 2.2 Plotting with ggformula 2.3 Creating data with expand.grid() 2.4 Transforming and summarizing data dplyr 2.5 Wide Data vs Long Data and other uses of tidyr", " Chapter 2 Some Useful Bits of R 2.1 Vectors, Lists, and Data Frames 2.1.1 Vectors 2.1.2 Lists 2.1.3 Data frames for rectangular data Rectangular data is organized in rows and columns (much like an excel spreadsheet). These rows and columns have a particular meaning: Each row represents one observational unit. Observational units go by many others names depending whether they are people, or inanimate objects, our events, etc. Examples include case, subject, item, etc. Regardless, the observational units are the things about which we collect information, and each one gets its own row in rectangular data. Each column represents a variable – one thing that is “measured” and recorded (at least in principle, some measurements might be missing) for each observational unit. Example In a study of nutritional habbits of college students, our observational units are the college students in the study. Each student gets her own row in the data frame. The variables might include things like an ID number (or name), sex, height, weight, whether the student lives on campus or off, what type of meal plan they have at the dining hall, etc., etc. Each of these is recorded in a separate column. Data frames are the standard way to store rectangular data in R. Here, for example, are the first few rows of a dataset called KidsFeet: library(mosaicData) # Load package to make KidsFeet data available head(KidsFeet) # first few rows name birthmonth birthyear length width sex biggerfoot domhand David 5 88 24.4 8.4 B L R Lars 10 87 25.4 8.8 B L L Zach 12 87 24.5 9.7 B R R Josh 1 88 25.2 9.8 B L R Lang 2 88 25.1 8.9 B L R Scotty 3 88 25.7 9.7 B R R 2.1.3.1 Accessing via [ ] We can access rows, columns, or individual elements of a dat frame using [ ]: KidsFeet[, &quot;length&quot;] ## [1] 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 ## [15] 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 ## [29] 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 KidsFeet[, 4] ## [1] 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 ## [15] 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 ## [29] 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 KidsFeet[3, ] name birthmonth birthyear length width sex biggerfoot domhand 3 Zach 12 87 24.5 9.7 B R R KidsFeet[3, 4] ## [1] 24.5 By default, Accessing a row returns a 1-row data frame. Accessing a column returns a vector (at least for vector columns) Accessing a element returns that element (technically a vector with one element in it). 2.1.3.2 Accessing columns via $ We can also access individual varaiables using the $ operator: KidsFeet$length ## [1] 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 ## [15] 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 ## [29] 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 As we will see, there are are other tools that will help us avoid needing to us $ or [ ] for access columns in a data frame. This is espcially nice when we are working with several variables all coming from the same data frame. 2.1.3.3 Accessing by number is dangerous Generally speaking, it is safer to access things by name than by number. It is easy to miscalculate the row or column number you need, and if rows or columns are added to or deleted from a data frame, the numbering can change. 2.1.3.4 Implementation Data frames are implemented in R as a special type (technically, class) of list. The elements of the list are the columns in the data frame. Each column must have the same length (so that our data frame has coherent rows). Most often the columns are vectors, but this isn’t required. This explains why $ works the way it does – we are just accessing one item in a list. It also means that we can use [[ ]] to access a column: KidsFeet[[&quot;length&quot;]] ## [1] 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 ## [15] 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 ## [29] 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 KidsFeet[[4]] ## [1] 24.4 25.4 24.5 25.2 25.1 25.7 26.1 23.0 23.6 22.9 27.5 24.8 26.1 27.0 ## [15] 26.0 23.7 24.0 24.7 26.7 25.5 24.0 24.4 24.0 24.5 24.2 27.1 26.1 25.5 ## [29] 24.2 23.9 24.0 22.5 24.5 23.6 24.7 22.9 26.0 21.6 24.6 2.1.4 Other types of data Some types of data do not work well in a rectangular arrangment of a data frame, and there are many other ways to store data. In R, other types of data commonly get stored in a list of some sort. 2.2 Plotting with ggformula 2.3 Creating data with expand.grid() We will frequently have need of synthetic data that includes all combinations of some variable values. expand.grid() does this for us: expand.grid( a = 1:3, b = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)) a b 1 A 2 A 3 A 1 B 2 B 3 B 1 C 2 C 3 C 1 D 2 D 3 D 2.4 Transforming and summarizing data dplyr 2.5 Wide Data vs Long Data and other uses of tidyr "],
["what-is-this-thing-called-probability-4.html", "Chapter 3 What is this thing called probability? {4} 3.1 The Set of All Possible Events 3.2 Probability: Outside or Inside the Head 3.3 Probability Distributions 3.4 Two-Way Distributions", " Chapter 3 What is this thing called probability? {4} 3.1 The Set of All Possible Events 3.1.1 Simulating library(ggformula) library(dplyr) theme_set(theme_bw()) # Flip a coin N times and compute the running proportion of heads at each flip. # Generate a random sample of N flips (heads = 1, tails = 0): flipSequence &lt;- function(size = 500, p = 0.5) { sample(c(0, 1), prob = c(1 - p, p), size = size, replace = TRUE) } A &lt;- data_frame( n = 1:500, flip = flipSequence(500), running_count = cumsum(flip), running_prop = running_count / n ) ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. gf_line( running_prop ~ n, data = A, color = &quot;skyblue&quot;, ylim = c(0, 1.0), xlab = &quot;Flip Number&quot;, ylab = &quot;Proportion Heads&quot;, main = &quot;Running Proportion of Heads&quot;) %&gt;% gf_hline(yintercept = 0.5, linetype = &quot;dotted&quot;) # # Plot a dotted horizontal reference line: # abline( h = pHeads , lty = &quot;dotted&quot; ) # # Display the beginning of the flip sequence: # flipLetters &lt;- paste( c(&quot;T&quot;,&quot;H&quot;)[flipSequence[1:10]+1] , collapse = &quot;&quot; ) # displayString &lt;- paste0( &quot;Flip Sequence = &quot; , flipLetters , &quot;...&quot; ) # text( N , .9 , displayString , adj = c(1,0.5) , cex = 1.3 ) # # Display the relative frequency at the end of the sequence. # text( N , .8 , paste(&quot;End Proportion =&quot;,runProp[N]) , adj = c(1,0.5) , cex = 1.3 ) 3.1.2 Deriving 3.2 Probability: Outside or Inside the Head 3.2.1 Calibrating subjective belief by preferences 3.2.2 Describing subjective belief maathematically 3.3 Probability Distributions 3.3.1 Discrete: Probability mass 3.3.2 Continuous: Density 3.4 Two-Way Distributions 3.4.1 Conditional probability 3.4.2 Independence of attributes "],
["bayes-rule-and-the-grid-method.html", "Chapter 4 Bayes’ Rule and the Grid Method 4.1 Estimating the bias in a coin using the Grid Method", " Chapter 4 Bayes’ Rule and the Grid Method 4.1 Estimating the bias in a coin using the Grid Method 4.1.1 Creating a Grid library(purrr) ## ## Attaching package: &#39;purrr&#39; ## The following object is masked from &#39;package:mosaic&#39;: ## ## cross x &lt;- 1; n &lt;- 4 CoinsGrid &lt;- expand.grid( theta = seq(0, 1, by = 0.001) ) %&gt;% mutate( prior = pmin(theta, 1 - theta), # higher if farther from edges prior = prior / sum(prior), # normalize likelihood = map_dbl(theta, ~ dbinom(x = x, size = n, .x)), posterior = prior * likelihood, posterior = posterior / sum(posterior) # normalize ) 4.1.2 Plots from the grid gf_line(prior ~ theta, data = CoinsGrid) gf_line(likelihood ~ theta, data = CoinsGrid) gf_line(posterior ~ theta, data = CoinsGrid) gf_area(prior ~ theta, data = CoinsGrid, alpha = 0.5) gf_area(likelihood ~ theta, data = CoinsGrid, alpha = 0.5) gf_area(posterior ~ theta, data = CoinsGrid, alpha = 0.5) 4.1.3 HDI from the grid Let’s write a function to compute the Highest Density Interval (of the posterior for theta) based on our grid. Since different grids may use different names for the parameter(s) and for the posterior, we’ll write our function in a way that will let us specify those names if we need to, but use posterior and theta by default. And for good measure, we’ll calculate the posterior mode as well. The basic idea (after standardizing the grid) is to sort the grid by the posterior. The mode will be at the end of the list, and the “bottom 95%” will be the HDI (or some other percent if we choose a different level). This method works as long as the posterior is unimodal, increasing to the mode from either side. HDI &lt;- function(formula = posterior ~ theta, grid, level = 0.95) { # Create a standardized version of the grid model.frame(formula, data = grid) %&gt;% # turn formula into data frame setNames(c(&quot;posterior&quot;, &quot;theta&quot;)) %&gt;% # standardrize names mutate( posterior = posterior / sum(posterior) # normalize posterior ) %&gt;% arrange(posterior) %&gt;% # sort by posterior mutate( cum_posterior = cumsum(posterior) # cumulative posterior ) %&gt;% filter( cum_posterior &gt;= 1 - level, # keep highest cum_posterior ) %&gt;% summarise( # summarise what&#39;s left lo = min(theta), hi = max(theta), height = min(posterior), level = level, mode_height = last(posterior), mode = last(theta), ) } HDICoins &lt;- HDI(posterior ~ theta, grid = CoinsGrid) HDICoins lo hi height level mode_height mode 0.098 0.681 0.0004833 0.95 0.0023698 0.4 With this information in hand, we can add a representation of the 95% HDI to our plot. gf_line(posterior ~ theta, data = CoinsGrid) %&gt;% gf_hline(yintercept = ~height, data = HDICoins, color = &quot;red&quot;, alpha = 0.5) %&gt;% gf_pointrangeh(height ~ mode + lo + hi, data = HDICoins, color = &quot;red&quot;, size = 1) %&gt;% gf_labs(caption = &quot;posterior mode and 95% HPI indicated in red&quot;) 4.1.4 Automating the grid BernGrid &lt;- function( x, n, prior = dunif, res = 1001, ...) { Grid &lt;- expand.grid( theta = seq(0, 1, length.out = res) ) %&gt;% mutate( prior = prior(theta, ...), prior = prior / sum(prior), likelihood = dbinom(x, n, theta), likelihood = likelihood / sum(likelihood), posterior = prior * likelihood, posterior = posterior / sum(posterior) ) H &lt;- HDI(grid = Grid) gf_line(prior ~ theta, data = Grid, color = ~&quot;prior&quot;, size = 1.15, alpha = 0.8) %&gt;% gf_line(likelihood ~ theta, data = Grid, color = ~&quot;likelihood&quot;, size = 1.15, alpha = 0.7) %&gt;% gf_line(posterior ~ theta, data = Grid, color = ~&quot;posterior&quot;, size = 1.15, alpha = 0.6) %&gt;% gf_pointrangeh( height ~ mode + lo + hi, data = H, color = &quot;red&quot;, size = 1) %&gt;% gf_labs(title = &quot;Prior/Likelihood/Posterior&quot;, subtitle = paste(&quot;Data: n =&quot;, n, &quot;, x =&quot;, x)) %&gt;% gf_refine( scale_color_manual( values = c( &quot;prior&quot; = &quot;forestgreen&quot;, &quot;likelihood&quot; = &quot;blue&quot;, &quot;posterior&quot; = &quot;red&quot;), breaks = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;) )) %&gt;% print() invisible(Grid) # return the Grid, but don&#39;t show it } This function let’s us quickly explore several scenarios and compare the results. How does changing the prior affect the posterior? How does changing the data affect the posterior? library(triangle) BernGrid(1, 4, prior = dtriangle, a = 0, b = 1, c = 0.5) BernGrid(1, 4, prior = dunif) BernGrid(10, 40, prior = dtriangle, a = 0, b = 1, c = 0.5) BernGrid(10, 40, prior = dunif) BernGrid(1, 4, prior = dtriangle, a = 0, b = 1, c = 0.8) BernGrid(10, 40, prior = dtriangle, a = 0, b = 1, c = 0.8) BernGrid(10, 40, prior = dbeta, shape1 = 25, shape2 = 12) "],
["jags-just-another-gibbs-sampler.html", "Chapter 5 JAGS – Just Another Gibbs Sampler 5.1 What JAGS is 5.2 A Complete Example: estimating a proportion 5.3 Example 2: comparing two proportions", " Chapter 5 JAGS – Just Another Gibbs Sampler This chapter focuses on a very simple model – one for which JAGS is overkill. This allows us to get familiar with JAGS and the various tools to investigate JAGS models in a simple setting before moving on to more intersting models soon. 5.1 What JAGS is JAGS (Just Another Gibbs Sampler) is an implementation of an MCMC algorithm called Gibbs sampling to sample the posterior distribution of a Bayesian model. We will interact with JAGS from within R using the following packages: R2jags – interface between R and JAGS coda – general tools for analysing and graphing MCMC algorithms bayesplot – a number of useful plots using ggplot2 CalvinBayes – includes some of the functions from Kruschke’s text 5.2 A Complete Example: estimating a proportion 5.2.1 The Model 5.2.2 Load Data The data sets provided as csv files by Kruschke also live in the CalvinBayes package, so you can read this file with library(CalvinBayes) data(&quot;z15N50&quot;) glimpse(z15N50) ## Observations: 50 ## Variables: 1 ## $ y &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,… We see that the data are coded as 50 0’s and 1’s in a variable named y. (You should use better names when creating your own data sets.) 5.2.3 Specify the model We can specify the model by creating a text file containing the JAGS description of the model or by creating a special kind of function. The avoids creating temporary files and keeps tidy in our R markdown documents. The main part of the model description is the same in either style, but notice that the using the function style, we do not need to include model{ ... } in our description. bern_model &lt;- function() { for (i in 1:N) { # each response is Bernoulli with fixed parameter theta y[i] ~ dbern(theta) } theta ~ dbeta(1, 1) # prior for theta } 5.2.4 Run the model R2jags::jags() can be used to run our JAGS model. We need to specify three things: (1) the model we are using (as defined above), (2) the data we are using, (3) the parameters we want saved in the posterior sampling. (theta is the only parameter in this model, but in larger models, we might choose to save only some of the parameters). There are some additional, optional things we might want to control as well. More on those later. For now, let’s fit the model using the default values for everything else. # Load the R2jags package library(R2jags) # Make the same &quot;random&quot; choices each time this is run. # This makes the Rmd file stable so you can comment on specific results. set.seed(123) # Fit the model bern_jags &lt;- jags( data = list(y = z15N50$y, N = nrow(z15N50)), model.file = bern_model, parameters.to.save = c(&quot;theta&quot;) ) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 1 ## Total graph size: 53 ## ## Initializing model ## ## | | | 0% | |** | 4% | |**** | 8% | |****** | 12% | |******** | 16% | |********** | 20% | |************ | 24% | |************** | 28% | |**************** | 32% | |****************** | 36% | |******************** | 40% | |********************** | 44% | |************************ | 48% | |************************** | 52% | |**************************** | 56% | |****************************** | 60% | |******************************** | 64% | |********************************** | 68% | |************************************ | 72% | |************************************** | 76% | |**************************************** | 80% | |****************************************** | 84% | |******************************************** | 88% | |********************************************** | 92% | |************************************************ | 96% | |**************************************************| 100% Let’s take a quick look at what we have. bern_jags ## Inference for Bugs model at &quot;/var/folders/py/txwd26jx5rq83f4nn0f5fmmm0000gn/T//RtmpGt3T1Z/model17ca96a93b09.txt&quot;, fit using jags, ## 3 chains, each with 2000 iterations (first 1000 discarded) ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## theta 0.308 0.064 0.191 0.261 0.306 0.351 0.435 1.001 3000 ## deviance 62.089 1.395 61.087 61.186 61.571 62.459 65.770 1.001 3000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 1.0 and DIC = 63.1 ## DIC is an estimate of expected predictive error (lower deviance is better). We see that the average value of theta in our posterior distribution is 0.3075345. The values of Rhat and n.eff give a quick check that nothing disasterous seems to have happened when we fit this simple model. (Rhat should be very nearly 1 if the MCMC algorithm has converged. Because n.eff is close to \\(3000 = 3 \\cdot 1000\\), this model is sampling the posterior very effeciently.) We can plot the posterior distribution. library(CalvinBayes) head(posterior(bern_jags)) deviance theta 61.85316 0.2454862 61.12541 0.3128974 61.13378 0.2860278 61.12806 0.3133316 61.82968 0.3576552 62.84865 0.2193062 gf_dhistogram(~theta, data = posterior(bern_jags)) %&gt;% gf_dens(~theta, data = posterior(bern_jags)) 5.2.5 Using coda The coda package provides output analysis and diagnostics for MCMC algorithms. In order to use it, we must convert our JAGS object into something coda recognizes. We do with with the as.mcmc() function. bern_mcmc &lt;- as.mcmc(bern_jags) plot(bern_mcmc) Note: Kruschke uses rjags without R2jags, so he does this step using rjags::coda.samples() instead of as.mcmc(). Both functions result in the same thing – posterior samples in a format that coda expects, but they have different starting points. 5.2.6 Using bayesplot The mcmc object we extracted with as.mcmc() can be used by the utilities in the bayesplot(). Here, for example is the bayesplot plot of the posterior distribution for theta. By default, a vertical line segment is drawn at the median of the posterior distribution. library(bayesplot) mcmc_areas( bern_mcmc, pars = c(&quot;theta&quot;), # make a plot for the theta parameter prob = 0.90) # shade the central 90% One advantage of bayesplot is that the plots use the ggplot2 system and so interoperate well with ggformula. mcmc_trace(bern_mcmc, pars = &quot;theta&quot;) # spearate the chains using facets and modify the color scheme mcmc_trace(bern_mcmc, pars = &quot;theta&quot;) %&gt;% gf_facet_grid(Chain ~ .) %&gt;% gf_refine(scale_color_viridis_d()) ## Scale for &#39;colour&#39; is already present. Adding another scale for ## &#39;colour&#39;, which will replace the existing scale. We will encounter additional plots from bayesplot as we go along. 5.2.7 Using Kruschke’s functions I have put (modified versions of) some of functions from Kruschke’s book into the CalvinBayes package so that you don’t have to source his files to use them. 5.2.7.1 diagMCMC() diagMCMC(bern_mcmc, par = &quot;theta&quot;) 5.2.7.2 plotPost() plotPost(bern_mcmc[, &quot;theta&quot;], main = &quot;theta&quot;, xlab = expression(theta)) ESS mean median mode hdiMass hdiLow hdiHigh compVal pGtCompVal ROPElow ROPEhigh pLtROPE pInROPE pGtROPE theta 3000 0.3075345 0.3056372 0.2982405 0.95 0.1895683 0.4342434 NA NA NA NA NA NA NA plotPost(bern_mcmc[, &quot;theta&quot;], main = &quot;theta&quot;, xlab = expression(theta), cenTend = &quot;median&quot;, compVal = 0.5, ROPE = c(0.45, 0.55), credMass = 0.90) ESS mean median mode hdiMass hdiLow hdiHigh compVal pGtCompVal ROPElow ROPEhigh pLtROPE pInROPE pGtROPE theta 3000 0.3075345 0.3056372 0.2982405 0.9 0.1983403 0.4072414 0.5 0.0033333 0.45 0.55 0.9866667 0.013 0.0003333 5.2.8 Optional arguments to jags() 5.2.8.1 Number and size of chains Sometimes we want to use more or longer chains (or fewer or shorter chains if we are doing a quick preliminary check before running longer chains later). jags() has three arguments for this: n.chains: number of chains n.iter: number of iterations per chain n.burning: number of burn in steps per chain set.seed(76543) bern_jags2 &lt;- jags( data = list(y = z15N50$y, N = nrow(z15N50)), model.file = bern_model, parameters.to.save = c(&quot;theta&quot;), n.chains = 4, n.iter = 5000, n.burnin = 1000, ) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 1 ## Total graph size: 53 ## ## Initializing model ## ## | | | 0% | |* | 2% | |** | 5% | |**** | 8% | |***** | 10% | |****** | 12% | |******** | 15% | |********* | 18% | |********** | 20% | |*********** | 22% | |************ | 25% | |************** | 28% | |*************** | 30% | |**************** | 32% | |****************** | 35% | |******************* | 38% | |******************** | 40% | |********************* | 42% | |********************** | 45% | |************************ | 48% | |************************* | 50% | |************************** | 52% | |**************************** | 55% | |***************************** | 58% | |****************************** | 60% | |******************************* | 62% | |******************************** | 65% | |********************************** | 68% | |*********************************** | 70% | |************************************ | 72% | |************************************** | 75% | |*************************************** | 78% | |**************************************** | 80% | |***************************************** | 82% | |****************************************** | 85% | |******************************************** | 88% | |********************************************* | 90% | |********************************************** | 92% | |************************************************ | 95% | |************************************************* | 98% | |**************************************************| 100% 5.2.8.2 Starting point for chains We can also control the starting point for the chains. Starting different chains and quite different parameter values can help * verify that the MCMC algorthm is not overly sensitive to where we are starting from, and * ensure that the MCMC algorithm has explored the posterior distribution sufficiently. On the other hand, if we start a chain too far from the peak of the posterior distribution, the chain may have trouble converging. We can provide either specific starting points for each chain or a function that generates random starting points. gf_dist(&quot;beta&quot;, shape1 = 3, shape2 = 3) set.seed(2345) bern_jags3 &lt;- jags( data = list(y = z15N50$y, N = nrow(z15N50)), model.file = bern_model, parameters.to.save = c(&quot;theta&quot;), n.chains = 4, n.iter = 5000, n.burnin = 1000, # start each chain &quot;near&quot; 0.5 inits = function() list(theta = rbeta(1, 3, 3)) ) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 1 ## Total graph size: 53 ## ## Initializing model ## ## | | | 0% | |* | 2% | |** | 5% | |**** | 8% | |***** | 10% | |****** | 12% | |******** | 15% | |********* | 18% | |********** | 20% | |*********** | 22% | |************ | 25% | |************** | 28% | |*************** | 30% | |**************** | 32% | |****************** | 35% | |******************* | 38% | |******************** | 40% | |********************* | 42% | |********************** | 45% | |************************ | 48% | |************************* | 50% | |************************** | 52% | |**************************** | 55% | |***************************** | 58% | |****************************** | 60% | |******************************* | 62% | |******************************** | 65% | |********************************** | 68% | |*********************************** | 70% | |************************************ | 72% | |************************************** | 75% | |*************************************** | 78% | |**************************************** | 80% | |***************************************** | 82% | |****************************************** | 85% | |******************************************** | 88% | |********************************************* | 90% | |********************************************** | 92% | |************************************************ | 95% | |************************************************* | 98% | |**************************************************| 100% bern_jags4 &lt;- jags( data = list(y = z15N50$y, N = nrow(z15N50)), model.file = bern_model, parameters.to.save = c(&quot;theta&quot;), n.chains = 3, n.iter = 550, n.burnin = 500, # choose specific starting point for each chain inits = list( list(theta = 0.5), list(theta = 0.3), list(theta = 0.7) ) ) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 1 ## Total graph size: 53 ## ## Initializing model mcmc_trace(as.mcmc(bern_jags4), pars = &quot;theta&quot;) 5.2.8.3 Running chains in parallel Although this model runs very quickly, others models may take considerably longer. We can use jags.parallel() in place of jags() to take advantage of multiple cores to run more than one chain at a time. jags.seed can be used to set the seed for the parallel random number generator used. (Note: set.seed() does not work when using jags.parallel() and jags.seed has no effect when using jags().) library(R2jags) bern_jags3 &lt;- jags.parallel( data = list(y = z15N50$y, N = nrow(z15N50)), model.file = bern_model, parameters.to.save = c(&quot;theta&quot;), n.chains = 4, n.iter = 5000, n.burnin = 1000, jags.seed = 12345 ) 5.3 Example 2: comparing two proportions 5.3.1 The data Suppose we want to compare Reginald and Tony’s abilities to hit a target (with a dart, perhaps). For each attempt, we record two pieces of information: the person making the attampt (the subject) and whether the attempt succeeded (0 or 1). library(mosaic) head(z6N8z2N7) y s 1 Reginald 0 Reginald 1 Reginald 1 Reginald 1 Reginald 1 Reginald # Let&#39;s do some renaming Target &lt;- z6N8z2N7 %&gt;% rename(hit = y, subject = s) df_stats(hit ~ subject, data = Target, props, attempts = length) subject prop_0 prop_1 attempts Reginald 0.2500000 0.7500000 8 Tony 0.7142857 0.2857143 7 5.3.2 The model Now our model is that each person has his own success rate – we have **two \\(\\theta\\)’s, one for Reginald and one for Tony. We express this as hit ~ dbern(theta[subject[i]]) where subject[i] tells us which subject the ith observation was for. 5.3.3 Describing the model to JAGS bern2_model &lt;- function() { for (i in 1:Nobs) { # each response is Bernoulli with the appropriate theta hit[i] ~ dbern(theta[subject[i]]) } for (s in 1:Nsub) { theta[s] ~ dbeta(2, 2) # prior for each theta } } JAGS will also need access to four pieces of information from our data set: a vector of hit values a vector of subject values – coded as integers 1 and 2 (so that subject[i] makes sense to JAGS. (In general, JAGS is much less fluid in handling data than R is, so we often need to do some manual data conversion for JAGSS.) Nobs – the total number of observations Nsub – the number of subjects We will prepare these as a list. TargetList &lt;- list( Nobs = nrow(Target), Nsub = 2, hit = Target$hit, subject = as.numeric(as.factor(Target$subject)) ) TargetList ## $Nobs ## [1] 15 ## ## $Nsub ## [1] 2 ## ## $hit ## [1] 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 ## ## $subject ## [1] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 5.3.4 Fitting the model bern2_jags &lt;- jags( data = TargetList, model.file = bern2_model, parameters.to.save = &quot;theta&quot;) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 15 ## Unobserved stochastic nodes: 2 ## Total graph size: 35 ## ## Initializing model ## ## | | | 0% | |** | 4% | |**** | 8% | |****** | 12% | |******** | 16% | |********** | 20% | |************ | 24% | |************** | 28% | |**************** | 32% | |****************** | 36% | |******************** | 40% | |********************** | 44% | |************************ | 48% | |************************** | 52% | |**************************** | 56% | |****************************** | 60% | |******************************** | 64% | |********************************** | 68% | |************************************ | 72% | |************************************** | 76% | |**************************************** | 80% | |****************************************** | 84% | |******************************************** | 88% | |********************************************** | 92% | |************************************************ | 96% | |**************************************************| 100% 5.3.5 Inspecting the results bern2_mcmc &lt;- as.mcmc(bern2_jags) diagMCMC(bern2_mcmc) # bayesplot plots mcmc_acf(bern2_mcmc) mcmc_acf_bar(bern2_mcmc) mcmc_pairs(bern2_mcmc, pars = c(&quot;theta[1]&quot;, &quot;theta[2]&quot;)) mcmc_combo(bern2_mcmc) mcmc_combo(bern2_mcmc, combo = c(&quot;dens&quot;, &quot;dens_overlay&quot;, &quot;trace&quot;, &quot;scatter&quot;), pars = c(&quot;theta[1]&quot;, &quot;theta[2]&quot;)) Here is a list of mcmc_ functions available: apropos(&quot;^mcmc_&quot;) ## [1] &quot;mcmc_acf&quot; &quot;mcmc_acf_bar&quot; ## [3] &quot;mcmc_areas&quot; &quot;mcmc_areas_data&quot; ## [5] &quot;mcmc_areas_ridges&quot; &quot;mcmc_areas_ridges_data&quot; ## [7] &quot;mcmc_combo&quot; &quot;mcmc_dens&quot; ## [9] &quot;mcmc_dens_chains&quot; &quot;mcmc_dens_chains_data&quot; ## [11] &quot;mcmc_dens_overlay&quot; &quot;mcmc_hex&quot; ## [13] &quot;mcmc_hist&quot; &quot;mcmc_hist_by_chain&quot; ## [15] &quot;mcmc_intervals&quot; &quot;mcmc_intervals_data&quot; ## [17] &quot;mcmc_neff&quot; &quot;mcmc_neff_data&quot; ## [19] &quot;mcmc_neff_hist&quot; &quot;mcmc_nuts_acceptance&quot; ## [21] &quot;mcmc_nuts_divergence&quot; &quot;mcmc_nuts_energy&quot; ## [23] &quot;mcmc_nuts_stepsize&quot; &quot;mcmc_nuts_treedepth&quot; ## [25] &quot;mcmc_pairs&quot; &quot;mcmc_parcoord&quot; ## [27] &quot;mcmc_parcoord_data&quot; &quot;mcmc_recover_hist&quot; ## [29] &quot;mcmc_recover_intervals&quot; &quot;mcmc_recover_scatter&quot; ## [31] &quot;mcmc_rhat&quot; &quot;mcmc_rhat_data&quot; ## [33] &quot;mcmc_rhat_hist&quot; &quot;mcmc_scatter&quot; ## [35] &quot;mcmc_trace&quot; &quot;mcmc_trace_highlight&quot; ## [37] &quot;mcmc_violin&quot; The functions ending in _data() return the data used to make the corresponding plot. This can be useful if you want to display that same infromation in a different way or if you just want to inspect the data to make sure you understand the plot. 5.3.6 Differnce in proportions If we are primarily interested in the difference between Reginald and Tony, we can plot the difference in their theta values. head(posterior(bern2_jags)) deviance theta.1 theta.2 18.54436 0.5707752 0.2463970 23.64073 0.3772347 0.5269200 18.68776 0.6122157 0.4316202 18.97816 0.5955774 0.1552301 19.25650 0.5276570 0.2124313 19.26544 0.5203864 0.3450249 gf_density( ~(theta.1 - theta.2), data = posterior(bern2_jags)) 5.3.7 Sampling from the prior To sample from the prior, we must do the following: remove the response variable from our data list change Nobs to 0 set DIC = FALSE in the call to jags(). This will run the model witout any data, which means the posterior will be the same as the prior. # make a copy of our data list TargetList0 &lt;- list( Nobs = 0, Nsub = 2, subject = as.numeric(as.factor(Target$subject)) ) bern2_jags0 &lt;- jags( data = TargetList0, model.file = bern2_model, parameters.to.save = c(&quot;theta&quot;), DIC = FALSE) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 2 ## Total graph size: 20 ## ## Initializing model ## ## | | | 0% | |** | 4% | |**** | 8% | |****** | 12% | |******** | 16% | |********** | 20% | |************ | 24% | |************** | 28% | |**************** | 32% | |****************** | 36% | |******************** | 40% | |********************** | 44% | |************************ | 48% | |************************** | 52% | |**************************** | 56% | |****************************** | 60% | |******************************** | 64% | |********************************** | 68% | |************************************ | 72% | |************************************** | 76% | |**************************************** | 80% | |****************************************** | 84% | |******************************************** | 88% | |********************************************** | 92% | |************************************************ | 96% | |**************************************************| 100% 5.3.7.1 Note about : in JAGS and in R From the JAGS documentation: &gt; The sequence operator : can only produce increasing sequences. If n &lt; m then m:n produces a vector of length zero and when this is used in a for loop index expression the contents of loop inside the curly brackets are skipped. Note that this behaviour is different from the sequence operator in R, where m:n will produce a decreasing sequence if n &lt; m. So in our JAGS model, 1:0 correctly represents no data (and no trips through the for loop). 5.3.7.2 What good is it to generate samples from the prior? Our model set priors for \\(\\theta_1\\) and \\(\\theta_2\\), but this implies a distribution for \\(\\theta_1 - \\theta_2\\), and we might like to see what that distribution looks like. gf_density( ~(theta.1 - theta.2), data = posterior(bern2_jags0)) "],
["heierarchical-models.html", "Chapter 6 Heierarchical Models 6.1 One coin from one mint 6.2 Multiple coins from one mint 6.3 Multiple coins from multiple mints", " Chapter 6 Heierarchical Models 6.1 One coin from one mint 6.2 Multiple coins from one mint 6.3 Multiple coins from multiple mints "]
]
