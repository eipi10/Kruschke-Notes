<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>20 Multiple Nominal Predictors | (Re)Doing Bayesain Data Analysis</title>
  <meta name="description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="20 Multiple Nominal Predictors | (Re)Doing Bayesain Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="20 Multiple Nominal Predictors | (Re)Doing Bayesain Data Analysis" />
  
  <meta name="twitter:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

<meta name="author" content="R Pruim">


<meta name="date" content="2019-04-28">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="nominal-predictors.html">
<link rel="next" href="dichotymous-response.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">(Re)Doing Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What’s in These Notes</a></li>
<li class="part"><span><b>I The Basics: Models, Probability, Bayes, and R</b></span></li>
<li class="chapter" data-level="2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html"><i class="fa fa-check"></i><b>2</b> Credibility, Models, and Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#the-steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.1</b> The Steps of Bayesian Data Analysis</a><ul>
<li class="chapter" data-level="2.1.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-code"><i class="fa fa-check"></i><b>2.1.1</b> R code</a></li>
<li class="chapter" data-level="2.1.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-packages"><i class="fa fa-check"></i><b>2.1.2</b> R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-1-which-coin-is-it"><i class="fa fa-check"></i><b>2.2</b> Example 1: Which coin is it?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#freedom-of-choice"><i class="fa fa-check"></i><b>2.2.1</b> Freedom of choice</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#distributions"><i class="fa fa-check"></i><b>2.3</b> Distributions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#beta-distributions"><i class="fa fa-check"></i><b>2.3.1</b> Beta distributions</a></li>
<li class="chapter" data-level="2.3.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#normal-distributions"><i class="fa fa-check"></i><b>2.3.2</b> Normal distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-2-height-vs-weight"><i class="fa fa-check"></i><b>2.4</b> Example 2: Height vs Weight</a><ul>
<li class="chapter" data-level="2.4.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#data"><i class="fa fa-check"></i><b>2.4.1</b> Data</a></li>
<li class="chapter" data-level="2.4.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#describing-a-model-for-the-relationship-between-height-and-weight"><i class="fa fa-check"></i><b>2.4.2</b> Describing a model for the relationship between height and weight</a></li>
<li class="chapter" data-level="2.4.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#prior"><i class="fa fa-check"></i><b>2.4.3</b> Prior</a></li>
<li class="chapter" data-level="2.4.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior"><i class="fa fa-check"></i><b>2.4.4</b> Posterior</a></li>
<li class="chapter" data-level="2.4.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior-predictive-check"><i class="fa fa-check"></i><b>2.4.5</b> Posterior Predictive Check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#where-do-we-go-from-here"><i class="fa fa-check"></i><b>2.5</b> Where do we go from here?</a></li>
<li class="chapter" data-level="2.6" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#ch02-exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#footnotes"><i class="fa fa-check"></i><b>2.7</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html"><i class="fa fa-check"></i><b>3</b> Some Useful Bits of R</a><ul>
<li class="chapter" data-level="3.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#style-guide"><i class="fa fa-check"></i><b>3.1</b> You Gotta Have Style</a><ul>
<li class="chapter" data-level="3.1.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#an-additional-note-about-homework"><i class="fa fa-check"></i><b>3.1.1</b> An additional note about homework</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors-lists-and-data-frames"><i class="fa fa-check"></i><b>3.2</b> Vectors, Lists, and Data Frames</a><ul>
<li class="chapter" data-level="3.2.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors"><i class="fa fa-check"></i><b>3.2.1</b> Vectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#lists"><i class="fa fa-check"></i><b>3.2.2</b> Lists</a></li>
<li class="chapter" data-level="3.2.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#data-frames-for-rectangular-data"><i class="fa fa-check"></i><b>3.2.3</b> Data frames for rectangular data</a></li>
<li class="chapter" data-level="3.2.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#other-types-of-data"><i class="fa fa-check"></i><b>3.2.4</b> Other types of data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#plotting-with-ggformula"><i class="fa fa-check"></i><b>3.3</b> Plotting with ggformula</a></li>
<li class="chapter" data-level="3.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#creating-data-with-expand.grid"><i class="fa fa-check"></i><b>3.4</b> Creating data with expand.grid()</a></li>
<li class="chapter" data-level="3.5" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#transforming-and-summarizing-data-dplyr-and-tidyr"><i class="fa fa-check"></i><b>3.5</b> Transforming and summarizing data dplyr and tidyr</a></li>
<li class="chapter" data-level="3.6" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#writing-functions"><i class="fa fa-check"></i><b>3.6</b> Writing Functions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#why-write-functions"><i class="fa fa-check"></i><b>3.6.1</b> Why write functions?</a></li>
<li class="chapter" data-level="3.6.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#function-parts"><i class="fa fa-check"></i><b>3.6.2</b> Function parts</a></li>
<li class="chapter" data-level="3.6.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#the-function-function-has-its-function"><i class="fa fa-check"></i><b>3.6.3</b> The function() function has its function</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#some-common-error-messages"><i class="fa fa-check"></i><b>3.7</b> Some common error messages</a><ul>
<li class="chapter" data-level="3.7.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#object-not-found"><i class="fa fa-check"></i><b>3.7.1</b> object not found</a></li>
<li class="chapter" data-level="3.7.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#package-inputenc-error-unicode-char-not-set-up-for-use-with-latex."><i class="fa fa-check"></i><b>3.7.2</b> Package inputenc Error: Unicode char not set up for use with LaTeX.</a></li>
<li class="chapter" data-level="3.7.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#any-message-mentioning-yaml"><i class="fa fa-check"></i><b>3.7.3</b> Any message mentioning yaml</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#ch03-exercises"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#footnotes-1"><i class="fa fa-check"></i><b>3.9</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#some-terminology"><i class="fa fa-check"></i><b>4.1</b> Some terminology</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>4.2</b> Distributions in R</a><ul>
<li class="chapter" data-level="4.2.1" data-path="probability.html"><a href="probability.html#example-normal-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Example: Normal distributions</a></li>
<li class="chapter" data-level="4.2.2" data-path="probability.html"><a href="probability.html#simulating-running-proportions"><i class="fa fa-check"></i><b>4.2.2</b> Simulating running proportions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#joint-marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>4.3</b> Joint, marginal, and conditional distributions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="probability.html"><a href="probability.html#example-hair-and-eye-color"><i class="fa fa-check"></i><b>4.3.1</b> Example: Hair and eye color</a></li>
<li class="chapter" data-level="4.3.2" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>4.3.2</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#ch04-exercises"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#footnotes-2"><i class="fa fa-check"></i><b>4.5</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html"><i class="fa fa-check"></i><b>5</b> Bayes’ Rule and the Grid Method</a><ul>
<li class="chapter" data-level="5.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#the-big-bayesian-idea"><i class="fa fa-check"></i><b>5.1</b> The Big Bayesian Idea</a><ul>
<li class="chapter" data-level="5.1.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#likelihood"><i class="fa fa-check"></i><b>5.1.1</b> Likelihood</a></li>
<li class="chapter" data-level="5.1.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#when-bayes-is-easy"><i class="fa fa-check"></i><b>5.1.2</b> When Bayes is easy</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#estimating-the-bias-in-a-coin-using-the-grid-method"><i class="fa fa-check"></i><b>5.2</b> Estimating the bias in a coin using the Grid Method</a><ul>
<li class="chapter" data-level="5.2.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#creating-a-grid"><i class="fa fa-check"></i><b>5.2.1</b> Creating a Grid</a></li>
<li class="chapter" data-level="5.2.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#hdi-from-the-grid"><i class="fa fa-check"></i><b>5.2.2</b> HDI from the grid</a></li>
<li class="chapter" data-level="5.2.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#automating-the-grid"><i class="fa fa-check"></i><b>5.2.3</b> Automating the grid</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#working-on-the-log-scale"><i class="fa fa-check"></i><b>5.3</b> Working on the log scale</a></li>
<li class="chapter" data-level="5.4" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#discrete-params"><i class="fa fa-check"></i><b>5.4</b> Discrete Parameters</a></li>
<li class="chapter" data-level="5.5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#ch05-exercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
<li class="chapter" data-level="5.6" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#footnotes-3"><i class="fa fa-check"></i><b>5.6</b> Footnotes</a></li>
</ul></li>
<li class="part"><span><b>II Inferring a Binomial Probability</b></span></li>
<li class="chapter" data-level="6" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><i class="fa fa-check"></i><b>6</b> Inferring a Binomial Probability via Exact Mathematical Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-distributions-1"><i class="fa fa-check"></i><b>6.1</b> Beta distributions</a></li>
<li class="chapter" data-level="6.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-and-bayes"><i class="fa fa-check"></i><b>6.2</b> Beta and Bayes</a><ul>
<li class="chapter" data-level="6.2.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#the-bernoulli-likelihood-function"><i class="fa fa-check"></i><b>6.2.1</b> The Bernoulli likelihood function</a></li>
<li class="chapter" data-level="6.2.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#a-convenient-prior"><i class="fa fa-check"></i><b>6.2.2</b> A convenient prior</a></li>
<li class="chapter" data-level="6.2.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#pros-and-cons-of-conjugate-priors"><i class="fa fa-check"></i><b>6.2.3</b> Pros and Cons of conjugate priors</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#getting-to-know-the-beta-distributions"><i class="fa fa-check"></i><b>6.3</b> Getting to know the Beta distributions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#important-facts"><i class="fa fa-check"></i><b>6.3.1</b> Important facts</a></li>
<li class="chapter" data-level="6.3.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#alternative-parameterizations-of-beta-distributions"><i class="fa fa-check"></i><b>6.3.2</b> Alternative parameterizations of Beta distributions</a></li>
<li class="chapter" data-level="6.3.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta_params"><i class="fa fa-check"></i><b>6.3.3</b> beta_params()</a></li>
<li class="chapter" data-level="6.3.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#automating-bayesian-updates-for-a-proportion-beta-prior"><i class="fa fa-check"></i><b>6.3.4</b> Automating Bayesian updates for a proportion (beta prior)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#what-if-the-prior-isnt-a-beta-distribution"><i class="fa fa-check"></i><b>6.4</b> What if the prior isn’t a beta distribution?</a></li>
<li class="chapter" data-level="6.5" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#ch06-exercises"><i class="fa fa-check"></i><b>6.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html"><i class="fa fa-check"></i><b>7</b> Markov Chain Monte Carlo (MCMC)</a><ul>
<li class="chapter" data-level="7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#king-markov-and-adviser-metropolis"><i class="fa fa-check"></i><b>7.1</b> King Markov and Adviser Metropolis</a></li>
<li class="chapter" data-level="7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#quick-intro-to-markov-chains"><i class="fa fa-check"></i><b>7.2</b> Quick Intro to Markov Chains</a><ul>
<li class="chapter" data-level="7.2.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#more-info-please"><i class="fa fa-check"></i><b>7.2.1</b> More info, please</a></li>
<li class="chapter" data-level="7.2.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#definition"><i class="fa fa-check"></i><b>7.2.2</b> Definition</a></li>
<li class="chapter" data-level="7.2.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#time-homogeneous-markov-chains"><i class="fa fa-check"></i><b>7.2.3</b> Time-Homogeneous Markov Chains</a></li>
<li class="chapter" data-level="7.2.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#matrix-representation"><i class="fa fa-check"></i><b>7.2.4</b> Matrix representation</a></li>
<li class="chapter" data-level="7.2.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#regular-markov-chains"><i class="fa fa-check"></i><b>7.2.5</b> Regular Markov Chains</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#back-to-king-markov"><i class="fa fa-check"></i><b>7.3</b> Back to King Markov</a></li>
<li class="chapter" data-level="7.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#how-well-does-the-metropolis-algorithm-work"><i class="fa fa-check"></i><b>7.4</b> How well does the Metropolis Algorithm work?</a><ul>
<li class="chapter" data-level="7.4.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-to-any-island"><i class="fa fa-check"></i><b>7.4.1</b> Jumping to any island</a></li>
<li class="chapter" data-level="7.4.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-only-to-neighbor-islands"><i class="fa fa-check"></i><b>7.4.2</b> Jumping only to neighbor islands</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#markov-chains-and-posterior-sampling"><i class="fa fa-check"></i><b>7.5</b> Markov Chains and Posterior Sampling</a><ul>
<li class="chapter" data-level="7.5.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#example-1-estimating-a-proportion"><i class="fa fa-check"></i><b>7.5.1</b> Example 1: Estimating a proportion</a></li>
<li class="chapter" data-level="7.5.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#example-2-estimating-mean-and-variance"><i class="fa fa-check"></i><b>7.5.2</b> Example 2: Estimating mean and variance</a></li>
<li class="chapter" data-level="7.5.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#issues-with-metropolis-algorithm"><i class="fa fa-check"></i><b>7.5.3</b> Issues with Metropolis Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#two-coins"><i class="fa fa-check"></i><b>7.6</b> Two coins</a><ul>
<li class="chapter" data-level="7.6.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#the-model"><i class="fa fa-check"></i><b>7.6.1</b> The model</a></li>
<li class="chapter" data-level="7.6.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#exact-analysis"><i class="fa fa-check"></i><b>7.6.2</b> Exact analysis</a></li>
<li class="chapter" data-level="7.6.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#metropolis"><i class="fa fa-check"></i><b>7.6.3</b> Metropolis</a></li>
<li class="chapter" data-level="7.6.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#gibbs-sampling"><i class="fa fa-check"></i><b>7.6.4</b> Gibbs sampling</a></li>
<li class="chapter" data-level="7.6.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#advantages-and-disadvantages-of-gibbs-vs-metropolis"><i class="fa fa-check"></i><b>7.6.5</b> Advantages and Disadvantages of Gibbs vs Metropolis</a></li>
<li class="chapter" data-level="7.6.6" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#so-what-do-we-learn-about-the-coins"><i class="fa fa-check"></i><b>7.6.6</b> So what do we learn about the coins?</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#mcmc-posterior-sampling-big-picture"><i class="fa fa-check"></i><b>7.7</b> MCMC posterior sampling: Big picture</a><ul>
<li class="chapter" data-level="7.7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#mcmc-markov-chain-monte-carlo"><i class="fa fa-check"></i><b>7.7.1</b> MCMC = Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="7.7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#posterior-sampling-random-walk-through-the-posterior"><i class="fa fa-check"></i><b>7.7.2</b> Posterior sampling: Random walk through the posterior</a></li>
<li class="chapter" data-level="7.7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#where-do-we-go-from-here-1"><i class="fa fa-check"></i><b>7.7.3</b> Where do we go from here?</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#ch07-exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html"><i class="fa fa-check"></i><b>8</b> JAGS – Just Another Gibbs Sampler</a><ul>
<li class="chapter" data-level="8.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#what-jags-is"><i class="fa fa-check"></i><b>8.1</b> What JAGS is</a><ul>
<li class="chapter" data-level="8.1.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#jags-documentation"><i class="fa fa-check"></i><b>8.1.1</b> JAGS documentation</a></li>
<li class="chapter" data-level="8.1.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#updating-c-and-clang"><i class="fa fa-check"></i><b>8.1.2</b> Updating C and CLANG</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-1-estimating-a-proportion-1"><i class="fa fa-check"></i><b>8.2</b> Example 1: estimating a proportion</a><ul>
<li class="chapter" data-level="8.2.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-1"><i class="fa fa-check"></i><b>8.2.1</b> The Model</a></li>
<li class="chapter" data-level="8.2.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#load-data"><i class="fa fa-check"></i><b>8.2.2</b> Load Data</a></li>
<li class="chapter" data-level="8.2.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#specify-the-model"><i class="fa fa-check"></i><b>8.2.3</b> Specify the model</a></li>
<li class="chapter" data-level="8.2.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#run-the-model"><i class="fa fa-check"></i><b>8.2.4</b> Run the model</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#extracting-information-from-a-jags-run"><i class="fa fa-check"></i><b>8.3</b> Extracting information from a JAGS run</a><ul>
<li class="chapter" data-level="8.3.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#posterior-1"><i class="fa fa-check"></i><b>8.3.1</b> posterior()</a></li>
<li class="chapter" data-level="8.3.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#side-note-posterior-sampling-and-the-grid-method"><i class="fa fa-check"></i><b>8.3.2</b> Side note: posterior sampling and the grid method</a></li>
<li class="chapter" data-level="8.3.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-coda"><i class="fa fa-check"></i><b>8.3.3</b> Using coda</a></li>
<li class="chapter" data-level="8.3.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-bayesplot"><i class="fa fa-check"></i><b>8.3.4</b> Using bayesplot</a></li>
<li class="chapter" data-level="8.3.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-kruschkes-functions"><i class="fa fa-check"></i><b>8.3.5</b> Using Kruschke’s functions</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#optional-arguments-to-jags"><i class="fa fa-check"></i><b>8.4</b> Optional arguments to jags()</a><ul>
<li class="chapter" data-level="8.4.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#number-and-size-of-chains"><i class="fa fa-check"></i><b>8.4.1</b> Number and size of chains</a></li>
<li class="chapter" data-level="8.4.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#starting-point-for-chains"><i class="fa fa-check"></i><b>8.4.2</b> Starting point for chains</a></li>
<li class="chapter" data-level="8.4.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#running-chains-in-parallel"><i class="fa fa-check"></i><b>8.4.3</b> Running chains in parallel</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-2-comparing-two-proportions"><i class="fa fa-check"></i><b>8.5</b> Example 2: comparing two proportions</a><ul>
<li class="chapter" data-level="8.5.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-data"><i class="fa fa-check"></i><b>8.5.1</b> The data</a></li>
<li class="chapter" data-level="8.5.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-2"><i class="fa fa-check"></i><b>8.5.2</b> The model</a></li>
<li class="chapter" data-level="8.5.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#describing-the-model-to-jags"><i class="fa fa-check"></i><b>8.5.3</b> Describing the model to JAGS</a></li>
<li class="chapter" data-level="8.5.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#fitting-the-model"><i class="fa fa-check"></i><b>8.5.4</b> Fitting the model</a></li>
<li class="chapter" data-level="8.5.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#inspecting-the-results"><i class="fa fa-check"></i><b>8.5.5</b> Inspecting the results</a></li>
<li class="chapter" data-level="8.5.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#difference-in-proportions"><i class="fa fa-check"></i><b>8.5.6</b> Difference in proportions</a></li>
<li class="chapter" data-level="8.5.7" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#sampling-from-the-prior"><i class="fa fa-check"></i><b>8.5.7</b> Sampling from the prior</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#ch08-exercises"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="heierarchical-models.html"><a href="heierarchical-models.html"><i class="fa fa-check"></i><b>9</b> Heierarchical Models</a><ul>
<li class="chapter" data-level="9.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#gamma-distributions"><i class="fa fa-check"></i><b>9.1</b> Gamma Distributions</a></li>
<li class="chapter" data-level="9.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#one-coin-from-one-mint"><i class="fa fa-check"></i><b>9.2</b> One coin from one mint</a></li>
<li class="chapter" data-level="9.3" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-one-mint"><i class="fa fa-check"></i><b>9.3</b> Multiple coins from one mint</a></li>
<li class="chapter" data-level="9.4" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-multiple-mints"><i class="fa fa-check"></i><b>9.4</b> Multiple coins from multiple mints</a></li>
<li class="chapter" data-level="9.5" data-path="heierarchical-models.html"><a href="heierarchical-models.html#therapeutic-touch"><i class="fa fa-check"></i><b>9.5</b> Therapeutic Touch</a><ul>
<li class="chapter" data-level="9.5.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#abstract"><i class="fa fa-check"></i><b>9.5.1</b> Abstract</a></li>
<li class="chapter" data-level="9.5.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#data-1"><i class="fa fa-check"></i><b>9.5.2</b> Data</a></li>
<li class="chapter" data-level="9.5.3" data-path="heierarchical-models.html"><a href="heierarchical-models.html#a-heierarchical-model"><i class="fa fa-check"></i><b>9.5.3</b> A heierarchical model</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="heierarchical-models.html"><a href="heierarchical-models.html#other-parameterizations-we-might-have-tried"><i class="fa fa-check"></i><b>9.6</b> Other parameterizations we might have tried</a><ul>
<li class="chapter" data-level="9.6.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#shape-parameters-for-beta"><i class="fa fa-check"></i><b>9.6.1</b> Shape parameters for Beta</a></li>
<li class="chapter" data-level="9.6.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#mean-instead-of-mode"><i class="fa fa-check"></i><b>9.6.2</b> Mean instead of mode</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="heierarchical-models.html"><a href="heierarchical-models.html#shrinkage"><i class="fa fa-check"></i><b>9.7</b> Shrinkage</a></li>
<li class="chapter" data-level="9.8" data-path="heierarchical-models.html"><a href="heierarchical-models.html#example-baseball-batting-average"><i class="fa fa-check"></i><b>9.8</b> Example: Baseball Batting Average</a></li>
<li class="chapter" data-level="9.9" data-path="heierarchical-models.html"><a href="heierarchical-models.html#ch09-exercises"><i class="fa fa-check"></i><b>9.9</b> Exerciess</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>10</b> (Model Comparison)</a></li>
<li class="chapter" data-level="11" data-path="nhst.html"><a href="nhst.html"><i class="fa fa-check"></i><b>11</b> (NHST)</a></li>
<li class="chapter" data-level="12" data-path="point-null-hypotheses.html"><a href="point-null-hypotheses.html"><i class="fa fa-check"></i><b>12</b> (Point Null Hypotheses)</a></li>
<li class="chapter" data-level="13" data-path="goals-power-sample-size.html"><a href="goals-power-sample-size.html"><i class="fa fa-check"></i><b>13</b> (Goals, Power, Sample Size)</a></li>
<li class="chapter" data-level="14" data-path="stan.html"><a href="stan.html"><i class="fa fa-check"></i><b>14</b> Stan</a><ul>
<li class="chapter" data-level="14.1" data-path="stan.html"><a href="stan.html#why-stan-might-work-better"><i class="fa fa-check"></i><b>14.1</b> Why Stan might work better</a></li>
<li class="chapter" data-level="14.2" data-path="stan.html"><a href="stan.html#describing-a-model-to-stan"><i class="fa fa-check"></i><b>14.2</b> Describing a model to Stan</a></li>
<li class="chapter" data-level="14.3" data-path="stan.html"><a href="stan.html#samping-from-the-prior"><i class="fa fa-check"></i><b>14.3</b> Samping from the prior</a></li>
<li class="chapter" data-level="14.4" data-path="stan.html"><a href="stan.html#ch14-exercises"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="glm-overview.html"><a href="glm-overview.html"><i class="fa fa-check"></i><b>15</b> GLM Overview</a><ul>
<li class="chapter" data-level="15.1" data-path="glm-overview.html"><a href="glm-overview.html#data-consists-of-observations-of-variables"><i class="fa fa-check"></i><b>15.1</b> Data consists of observations of variables</a><ul>
<li class="chapter" data-level="15.1.1" data-path="glm-overview.html"><a href="glm-overview.html#variable-roles"><i class="fa fa-check"></i><b>15.1.1</b> Variable Roles</a></li>
<li class="chapter" data-level="15.1.2" data-path="glm-overview.html"><a href="glm-overview.html#types-of-variables"><i class="fa fa-check"></i><b>15.1.2</b> Types of Variables</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="glm-overview.html"><a href="glm-overview.html#glm-framework"><i class="fa fa-check"></i><b>15.2</b> GLM Framework</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html"><i class="fa fa-check"></i><b>16</b> Estimating One and Two Means</a><ul>
<li class="chapter" data-level="16.1" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#basic-model-for-two-means"><i class="fa fa-check"></i><b>16.1</b> Basic Model for Two Means</a><ul>
<li class="chapter" data-level="16.1.1" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#data-2"><i class="fa fa-check"></i><b>16.1.1</b> Data</a></li>
<li class="chapter" data-level="16.1.2" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#model"><i class="fa fa-check"></i><b>16.1.2</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#an-old-sleep-study"><i class="fa fa-check"></i><b>16.2</b> An Old Sleep Study</a><ul>
<li class="chapter" data-level="16.2.1" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#data-3"><i class="fa fa-check"></i><b>16.2.1</b> Data</a></li>
<li class="chapter" data-level="16.2.2" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#model-1"><i class="fa fa-check"></i><b>16.2.2</b> Model</a></li>
<li class="chapter" data-level="16.2.3" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#separate-standard-deviations-for-each-group"><i class="fa fa-check"></i><b>16.2.3</b> Separate standard deviations for each group</a></li>
<li class="chapter" data-level="16.2.4" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#comparison-to-t-test"><i class="fa fa-check"></i><b>16.2.4</b> Comparison to t-test</a></li>
<li class="chapter" data-level="16.2.5" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#rope-region-of-practical-equivalence"><i class="fa fa-check"></i><b>16.2.5</b> ROPE (Region of Practical Equivalence)</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#variations-on-the-theme"><i class="fa fa-check"></i><b>16.3</b> Variations on the theme</a><ul>
<li class="chapter" data-level="16.3.1" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#other-distributions-for-the-response"><i class="fa fa-check"></i><b>16.3.1</b> Other distributions for the response</a></li>
<li class="chapter" data-level="16.3.2" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#other-priors-for-sigma-or-tau"><i class="fa fa-check"></i><b>16.3.2</b> Other Priors for <span class="math inline">\(\sigma\)</span> (or <span class="math inline">\(\tau\)</span>)</a></li>
<li class="chapter" data-level="16.3.3" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#paired-comparisons"><i class="fa fa-check"></i><b>16.3.3</b> Paired Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#how-many-chains-how-long"><i class="fa fa-check"></i><b>16.4</b> How many chains? How long?</a><ul>
<li class="chapter" data-level="16.4.1" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#why-multiple-chains"><i class="fa fa-check"></i><b>16.4.1</b> Why multiple chains?</a></li>
<li class="chapter" data-level="16.4.2" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#what-large-n.eff-does-and-doesnt-do-for-us"><i class="fa fa-check"></i><b>16.4.2</b> What large n.eff does and doesn’t do for us</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#looking-at-likelihood"><i class="fa fa-check"></i><b>16.5</b> Looking at Likelihood</a></li>
<li class="chapter" data-level="16.6" data-path="estimating-one-and-two-means.html"><a href="estimating-one-and-two-means.html#ch16-exercises"><i class="fa fa-check"></i><b>16.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>17</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="17.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-deluxe-basic-model"><i class="fa fa-check"></i><b>17.1</b> The deluxe basic model</a><ul>
<li class="chapter" data-level="17.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#likelihood-1"><i class="fa fa-check"></i><b>17.1.1</b> Likelihood</a></li>
<li class="chapter" data-level="17.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#priors"><i class="fa fa-check"></i><b>17.1.2</b> Priors</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#example-galtons-data"><i class="fa fa-check"></i><b>17.2</b> Example: Galton’s Data</a><ul>
<li class="chapter" data-level="17.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#describing-the-model-to-jags-1"><i class="fa fa-check"></i><b>17.2.1</b> Describing the model to JAGS</a></li>
<li class="chapter" data-level="17.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#problems-and-how-to-fix-them"><i class="fa fa-check"></i><b>17.2.2</b> Problems and how to fix them</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#centering-and-standardizing"><i class="fa fa-check"></i><b>17.3</b> Centering and Standardizing</a><ul>
<li class="chapter" data-level="17.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#beta_0-and-beta_1-are-still-correlated"><i class="fa fa-check"></i><b>17.3.1</b> <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are still correlated</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#weve-fit-a-model-now-what"><i class="fa fa-check"></i><b>17.4</b> We’ve fit a model, now what?</a><ul>
<li class="chapter" data-level="17.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#estimate-parameters"><i class="fa fa-check"></i><b>17.4.1</b> Estimate parameters</a></li>
<li class="chapter" data-level="17.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#make-predictions"><i class="fa fa-check"></i><b>17.4.2</b> Make predictions</a></li>
<li class="chapter" data-level="17.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>17.4.3</b> Posterior Predictive Checks</a></li>
<li class="chapter" data-level="17.4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#posterior-predictive-checks-with-bayesplot"><i class="fa fa-check"></i><b>17.4.4</b> Posterior predictive checks with bayesplot</a></li>
<li class="chapter" data-level="17.4.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#ppc-with-custom-data"><i class="fa fa-check"></i><b>17.4.5</b> PPC with custom data</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fitting-models-with-stan"><i class="fa fa-check"></i><b>17.5</b> Fitting models with Stan</a></li>
<li class="chapter" data-level="17.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#two-intercepts-model"><i class="fa fa-check"></i><b>17.6</b> Two Intercepts model</a></li>
<li class="chapter" data-level="17.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#ch17-exercises"><i class="fa fa-check"></i><b>17.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html"><i class="fa fa-check"></i><b>18</b> Multiple Metric Predictors</a><ul>
<li class="chapter" data-level="18.1" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#sat"><i class="fa fa-check"></i><b>18.1</b> SAT</a><ul>
<li class="chapter" data-level="18.1.1" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#sat-vs-expenditure"><i class="fa fa-check"></i><b>18.1.1</b> SAT vs expenditure</a></li>
<li class="chapter" data-level="18.1.2" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#sat-vs-expenditure-and-percent-taking-the-test"><i class="fa fa-check"></i><b>18.1.2</b> SAT vs expenditure and percent taking the test</a></li>
<li class="chapter" data-level="18.1.3" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#whats-wrong-with-this-picture"><i class="fa fa-check"></i><b>18.1.3</b> What’s wrong with this picture?</a></li>
<li class="chapter" data-level="18.1.4" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#multiple-predictors-in-pictures"><i class="fa fa-check"></i><b>18.1.4</b> Multiple predictors in pictures</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#interaction"><i class="fa fa-check"></i><b>18.2</b> Interaction</a><ul>
<li class="chapter" data-level="18.2.1" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#sat-with-interaction-term"><i class="fa fa-check"></i><b>18.2.1</b> SAT with interaction term</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#fitting-a-linear-model-with-brms"><i class="fa fa-check"></i><b>18.3</b> Fitting a linear model with brms</a><ul>
<li class="chapter" data-level="18.3.1" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#adjusting-the-model-with-brm"><i class="fa fa-check"></i><b>18.3.1</b> Adjusting the model with brm()</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#interpretting-a-model-with-an-interaction-term"><i class="fa fa-check"></i><b>18.4</b> Interpretting a model with an interaction term</a><ul>
<li class="chapter" data-level="18.4.1" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#thinking-about-the-noise"><i class="fa fa-check"></i><b>18.4.1</b> Thinking about the noise</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="multiple-metric-predictors.html"><a href="multiple-metric-predictors.html#ch18-exercises"><i class="fa fa-check"></i><b>18.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="nominal-predictors.html"><a href="nominal-predictors.html"><i class="fa fa-check"></i><b>19</b> Nominal Predictors</a><ul>
<li class="chapter" data-level="19.1" data-path="nominal-predictors.html"><a href="nominal-predictors.html#fruit-flies-study"><i class="fa fa-check"></i><b>19.1</b> Fruit flies Study</a></li>
<li class="chapter" data-level="19.2" data-path="nominal-predictors.html"><a href="nominal-predictors.html#model-1-out-of-the-box"><i class="fa fa-check"></i><b>19.2</b> Model 1: Out-of-the-box</a></li>
<li class="chapter" data-level="19.3" data-path="nominal-predictors.html"><a href="nominal-predictors.html#model-2-custom-priors"><i class="fa fa-check"></i><b>19.3</b> Model 2: Custom Priors</a></li>
<li class="chapter" data-level="19.4" data-path="nominal-predictors.html"><a href="nominal-predictors.html#models-3-and-4-alternate-parameterizations"><i class="fa fa-check"></i><b>19.4</b> Models 3 and 4: alternate parameterizations</a></li>
<li class="chapter" data-level="19.5" data-path="nominal-predictors.html"><a href="nominal-predictors.html#comparing-groups"><i class="fa fa-check"></i><b>19.5</b> Comparing groups</a><ul>
<li class="chapter" data-level="19.5.1" data-path="nominal-predictors.html"><a href="nominal-predictors.html#comparing-to-the-intercept-group"><i class="fa fa-check"></i><b>19.5.1</b> Comparing to the “intercept group”</a></li>
<li class="chapter" data-level="19.5.2" data-path="nominal-predictors.html"><a href="nominal-predictors.html#comparing-others-pairs-of-groups"><i class="fa fa-check"></i><b>19.5.2</b> Comparing others pairs of groups</a></li>
<li class="chapter" data-level="19.5.3" data-path="nominal-predictors.html"><a href="nominal-predictors.html#contrasts-comparing-groups-of-groups"><i class="fa fa-check"></i><b>19.5.3</b> Contrasts: Comparing “groups of groups”</a></li>
</ul></li>
<li class="chapter" data-level="19.6" data-path="nominal-predictors.html"><a href="nominal-predictors.html#more-variations"><i class="fa fa-check"></i><b>19.6</b> More Variations</a></li>
<li class="chapter" data-level="19.7" data-path="nominal-predictors.html"><a href="nominal-predictors.html#ch19-exercises"><i class="fa fa-check"></i><b>19.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html"><i class="fa fa-check"></i><b>20</b> Multiple Nominal Predictors</a><ul>
<li class="chapter" data-level="20.1" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#crop-yield-by-till-method-and-fertilizer"><i class="fa fa-check"></i><b>20.1</b> Crop Yield by Till Method and Fertilizer</a><ul>
<li class="chapter" data-level="20.1.1" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#what-does-sigma-represent"><i class="fa fa-check"></i><b>20.1.1</b> What does <span class="math inline">\(\sigma\)</span> represent?</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#split-plot-design"><i class="fa fa-check"></i><b>20.2</b> Split Plot Design</a></li>
<li class="chapter" data-level="20.3" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#which-model-should-we-use"><i class="fa fa-check"></i><b>20.3</b> Which model should we use?</a><ul>
<li class="chapter" data-level="20.3.1" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#modeling-choices"><i class="fa fa-check"></i><b>20.3.1</b> Modeling Choices</a></li>
<li class="chapter" data-level="20.3.2" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#measuring-a-model-prediction-error"><i class="fa fa-check"></i><b>20.3.2</b> Measuring a Model – Prediction Error</a></li>
<li class="chapter" data-level="20.3.3" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#out-of-sample-prediction-error"><i class="fa fa-check"></i><b>20.3.3</b> Out-of-sample prediction error</a></li>
<li class="chapter" data-level="20.3.4" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#approximating-out-of-sample-prediction-error"><i class="fa fa-check"></i><b>20.3.4</b> Approximating out-of-sample prediction error</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#using-loo"><i class="fa fa-check"></i><b>20.4</b> Using loo</a></li>
<li class="chapter" data-level="20.5" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#overfitting-example"><i class="fa fa-check"></i><b>20.5</b> Overfitting Example</a><ul>
<li class="chapter" data-level="20.5.1" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#brains-data"><i class="fa fa-check"></i><b>20.5.1</b> Brains Data</a></li>
<li class="chapter" data-level="20.5.2" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#measuring-fit-with-r2"><i class="fa fa-check"></i><b>20.5.2</b> Measuring fit with <span class="math inline">\(r^2\)</span></a></li>
<li class="chapter" data-level="20.5.3" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#leave-one-out-analysis"><i class="fa fa-check"></i><b>20.5.3</b> Leave One Out Analysis</a></li>
</ul></li>
<li class="chapter" data-level="20.6" data-path="multiple-nominal-predictors.html"><a href="multiple-nominal-predictors.html#ch20-exercses"><i class="fa fa-check"></i><b>20.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="dichotymous-response.html"><a href="dichotymous-response.html"><i class="fa fa-check"></i><b>21</b> Dichotymous Response</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">(Re)Doing Bayesain Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-nominal-predictors" class="section level1">
<h1><span class="header-section-number">20</span> Multiple Nominal Predictors</h1>
<div id="crop-yield-by-till-method-and-fertilizer" class="section level2">
<h2><span class="header-section-number">20.1</span> Crop Yield by Till Method and Fertilizer</h2>
<p>The data in <code>CalvinBayes::SplitPlotAgri</code> are from an agricultural study in which
different tilling methods and different fertilizers were used and the crop yield (in
bushels per acre) was subsequently measured.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_point</span>(Yield <span class="op">~</span><span class="st"> </span>Fert <span class="op">|</span><span class="st"> </span><span class="er">~</span><span class="st"> </span>Till, <span class="dt">data =</span> SplitPlotAgri, <span class="dt">alpha =</span> <span class="fl">0.4</span>, <span class="dt">size =</span> <span class="dv">4</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch20-splitplot-plot-1.png" width="576" /></p>
<p>Here are two models. See if you can figure out what they are.
(How can you use R to check if you are correct?)</p>
<ul>
<li>What parameters does each model have?</li>
<li>Write a formula that describes the model. Be sure to clarify what the variables
mean.</li>
<li>How would you use each model to estimate the mean yield when using ridge tilling
and deep fertilizer? (Imagine that you already have the posterior distribution in
hand.)</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">fert1_brm &lt;-
<span class="st">  </span><span class="kw">brm</span>(Yield <span class="op">~</span><span class="st"> </span>Till <span class="op">+</span><span class="st"> </span>Fert, <span class="dt">data =</span> SplitPlotAgri)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>

<pre class="sourceCode r"><code class="sourceCode r">fert2_brm &lt;-<span class="st">     </span>
<span class="st">  </span><span class="kw">brm</span>(Yield <span class="op">~</span><span class="st"> </span>Till <span class="op">*</span><span class="st"> </span>Fert, <span class="dt">data =</span> SplitPlotAgri)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## recompiling to avoid crashing R session</code></pre>
<pre><code>## Start sampling</code></pre>

<p>In each of these models, the response (yield) is normally distributed
around a mean value that depends on the type of fertilizer and tilling method
used:</p>
<p><span class="math display">\[\begin{align*}
Y_i &amp;\sim \mu_i + {\sf Norm}(0, \sigma) \\
Y_i &amp;\sim  {\ sf Norm}(\mu_i, \sigma)
\end{align*}\]</span></p>
<p>In model 1, the two nominal predictors are converted into indicator variables:</p>
<p><span class="math display">\[\begin{align*}
x_1 &amp;= [\![ \mathrm{Till} = \mathrm{Moldbrd} ]\!] \\
x_2 &amp;= [\![ \mathrm{Till} = \mathrm{Ridge} ]\!] \\
x_3 &amp;= [\![ \mathrm{Fert} = \mathrm{Deep} ]\!] \\
x_4 &amp;= [\![ \mathrm{Fert} = \mathrm{Surface} ]\!] \\
\end{align*}\]</span></p>
<p>So the model becomes (omitting the subscripted <span class="math inline">\(i\)</span>):</p>
<p><span class="math display">\[\begin{align*}
\mu &amp;=  \beta0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 
\\
&amp;=
\beta_0 +
\beta_1 [\![ \mathrm{Till} = \mathrm{Moldbrd} ]\!] +
\beta_2 [\![ \mathrm{Till} = \mathrm{Ridge} ]\!] 
\beta_3 [\![ \mathrm{Fert} = \mathrm{Deep} ]\!] +
\beta_4 [\![ \mathrm{Fert} = \mathrm{Surface} ]\!] +
\end{align*}\]</span></p>
<p>We can visualize this in a tabular form as</p>
<table>
<colgroup>
<col width="29%" />
<col width="20%" />
<col width="16%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Chisel</th>
<th>Moldbrd</th>
<th>Ridge</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Broad</td>
<td><span class="math inline">\(\beta_0\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_1\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_2\)</span></td>
</tr>
<tr class="even">
<td>Deep</td>
<td><span class="math inline">\(\beta_0 + \beta_3\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_1 + \beta_3\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_2 + \beta_3\)</span></td>
</tr>
<tr class="odd">
<td>Surface</td>
<td><span class="math inline">\(\beta_0 + \beta_4\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_1 + \beta_4\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_2 + \beta_4\)</span></td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r">fert1_brm  </code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Yield ~ Till + Fert 
##    Data: SplitPlotAgri (Number of observations: 99) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept     120.34      2.92   114.71   126.08       3919 1.00
## TillMoldbrd    16.11      3.16     9.78    22.22       4368 1.00
## TillRidge      15.75      3.13     9.63    22.00       4408 1.00
## FertDeep       15.79      3.25     9.58    22.32       3877 1.00
## FertSurface    12.66      3.23     6.37    18.92       4352 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma    13.11      0.96    11.38    15.17       4394 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Note that this model implies that the difference in yield between using
two fertilizers is the same for each of the three tilling methods and the
difference due to tilling methods is the same for each of the three fertilizers.
This may not be a reasonable assumption. Perhaps some fertilizers work better
with certain tilling methods than with others. Model 2 allows for this.</p>
<p>The interaction (<code>Till * Fert</code>) creates additional new variables of the form
<span class="math inline">\(x_i x_j\)</span> where <span class="math inline">\(i = 1\)</span> or <span class="math inline">\(2\)</span> and <span class="math inline">\(j = 3\)</span> or <span class="math inline">\(4\)</span>.
For example,
<span class="math display">\[\begin{align*}
x_1 x_3 &amp;=  
    [\![ \mathrm{Till} = \mathrm{Moldbrd} ]\!] \cdot
    [\![ \mathrm{Fert} = \mathrm{Deep}    ]\!] \\
    &amp; = 
    [\![ \mathrm{Till} = \mathrm{Moldbrd} \mathrm{\ and \ }
         \mathrm{Fert} = \mathrm{Deep} ]\!] 
\end{align*}\]</span></p>
<p>If we let <span class="math inline">\(\beta_{i:j}\)</span> be the coefficient on <span class="math inline">\(x_i x_j\)</span>, then our table for
<span class="math inline">\(\mu\)</span> becomes</p>
<table>
<colgroup>
<col width="29%" />
<col width="20%" />
<col width="16%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Chisel</th>
<th>Moldbrd</th>
<th>Ridge</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Broad</td>
<td><span class="math inline">\(\beta_0\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_1\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_2\)</span></td>
</tr>
<tr class="even">
<td>Deep</td>
<td><span class="math inline">\(\beta_0 + \beta_3\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_1 + \beta_3 + \beta_{1:3}\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_2 + \beta_3 + \beta_{2:3}\)</span></td>
</tr>
<tr class="odd">
<td>Surface</td>
<td><span class="math inline">\(\beta_0 + \beta_4\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_1 + \beta_4 + \beta_{1:4}\)</span></td>
<td><span class="math inline">\(\beta_0 + \beta_2 + \beta_4 + \beta_{2:4}\)</span></td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r">fert2_brm  </code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Yield ~ Till * Fert 
##    Data: SplitPlotAgri (Number of observations: 99) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                         Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept                 124.41      3.64   117.16   131.52       1805 1.00
## TillMoldbrd                16.34      5.39     5.49    26.95       1930 1.00
## TillRidge                   3.58      5.31    -6.77    13.69       1953 1.00
## FertDeep                    9.36      5.12    -0.52    19.30       1799 1.00
## FertSurface                 7.02      5.14    -3.14    16.84       2051 1.00
## TillMoldbrd:FertDeep        0.85      7.60   -14.19    15.72       1892 1.00
## TillRidge:FertDeep         18.15      7.50     3.61    32.73       2010 1.00
## TillMoldbrd:FertSurface    -1.65      7.73   -16.86    13.63       2092 1.00
## TillRidge:FertSurface      18.22      7.54     3.63    32.96       1948 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma    12.68      0.93    10.97    14.62       2988 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>In this model, there is are no dependencies among the various group means and the
interaction parameters (<span class="math inline">\(\beta_{i:j}\)</span>) are a measure of how much this mattered.
(If they are close to 0, then this will be very much like the additive model.)</p>
<p>As before, we can opt to fit the model without an intercept. This produces a
different parameterization of the same model.</p>
<pre class="sourceCode r"><code class="sourceCode r">fert2a_brm &lt;-<span class="st">   </span>
<span class="st">  </span><span class="kw">brm</span>(Yield <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Till <span class="op">*</span><span class="st"> </span>Fert, <span class="dt">data =</span> SplitPlotAgri)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">fert2a_brm</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Yield ~ 0 + Till * Fert 
##    Data: SplitPlotAgri (Number of observations: 99) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                         Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## TillChisel                124.30      3.63   116.91   131.25       2135 1.00
## TillMoldbrd               140.69      4.08   132.48   148.83       3586 1.00
## TillRidge                 127.91      3.89   120.12   135.76       3592 1.00
## FertDeep                    9.49      5.15    -0.27    19.36       2215 1.00
## FertSurface                 7.05      5.04    -2.96    17.07       2094 1.00
## TillMoldbrd:FertDeep        0.72      7.54   -13.92    15.97       2274 1.00
## TillRidge:FertDeep         18.00      7.57     2.91    33.12       2481 1.00
## TillMoldbrd:FertSurface    -1.75      7.66   -16.69    13.25       2497 1.00
## TillRidge:FertSurface      18.29      7.56     3.52    33.26       2357 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma    12.70      0.96    11.02    14.74       3748 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<div id="what-does-sigma-represent" class="section level3">
<h3><span class="header-section-number">20.1.1</span> What does <span class="math inline">\(\sigma\)</span> represent?</h3>
<p>In each of these models, <span class="math inline">\(\sigma\)</span> is the standard deviation of yield for
all plots (not just those in our data) with a given combination of fertilizer
and tilling method. These models specify that the standard deviation is the same
in each of these groups (but we could modify that assumption and estimate
separate standard deviations in each group if we wanted). The estimate of <span class="math inline">\(\sigma\)</span>
is a bit smaller for the model with interaction because the added flexibility
of the model allows us to estimate the means more flexibly.</p>

</div>
</div>
<div id="split-plot-design" class="section level2">
<h2><span class="header-section-number">20.2</span> Split Plot Design</h2>
<p>There is a bit more to our story. The study used 33 different fields. Each field
was divided into 3 sections and a different fertilizer was applied to each of the
three sections. (Which fertilizer was used on which section was determined at random.)
This is called a “split-plot design” (even if it is applied to things that are not
fields of crops).</p>
<p>It would have been possible to divide each field into 9 sub-plots and use all
combinations of tilling and fertilizer, but that’s not how this study was done.
The tilling method was the same for the entire field – likely because it was
much more efficient to plow the fields this way.</p>
<p>The plot below indicates that different fields appear to have different baseline
yields since the dots associated with one field tend to be near the top or bottom
of each of the fertilizer clusters. We can add an additional variable to our
model to handle this situation.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_point</span>(Yield <span class="op">~</span><span class="st"> </span>Fert <span class="op">|</span><span class="st"> </span><span class="er">~</span><span class="st"> </span>Till, <span class="dt">data =</span> SplitPlotAgri, <span class="dt">alpha =</span> <span class="fl">0.4</span>, <span class="dt">size =</span> <span class="dv">4</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_line</span>(<span class="dt">group =</span> <span class="op">~</span>Field)</code></pre>
<p><img src="Redoing_files/figure-html/ch20-splitplot-plot-with-fields-1.png" width="576" /></p>
<pre class="sourceCode r"><code class="sourceCode r">fert3_brm &lt;-
<span class="st">  </span><span class="co"># the use of factor() is important here because the field ids are numbers</span>
<span class="st">  </span><span class="co"># factor converts this into a factor (ie, a nominal variable)</span>
<span class="st">  </span><span class="kw">brm</span>(Yield <span class="op">~</span><span class="st"> </span>Till <span class="op">*</span><span class="st"> </span>Fert <span class="op">+</span><span class="st"> </span><span class="kw">factor</span>(Field), <span class="dt">data =</span> SplitPlotAgri)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## recompiling to avoid crashing R session</code></pre>
<pre><code>## Start sampling</code></pre>
<pre><code>## Warning: There were 3999 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">fert3_brm</code></pre>
<pre><code>## Warning: The model has not converged (some Rhats are &gt; 1.1). Do not analyse the results! 
## We recommend running more iterations and/or setting stronger priors.</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Yield ~ Till * Fert + factor(Field) 
##    Data: SplitPlotAgri (Number of observations: 99) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                         Estimate Est.Error  l-95% CI u-95% CI Eff.Sample Rhat
## Intercept                 118.99      4.53    110.76   127.55          3 2.22
## TillMoldbrd               136.86   7252.75 -15054.17 11917.89          2 5.44
## TillRidge                 828.01   5320.89  -9049.31  9955.85          3 3.66
## FertDeep                    9.37      2.23      5.28    13.93         26 1.14
## FertSurface                 6.84      2.41      2.04    11.50         11 1.25
## factorField2               14.68      5.70      3.76    24.68          4 1.68
## factorField3               17.62      5.23      6.40    26.65          6 1.51
## factorField4                5.85      5.10     -3.66    16.34          9 1.33
## factorField5               16.39      5.07      5.70    25.33          4 1.61
## factorField6               -1.29      5.27    -11.90     7.74          6 1.47
## factorField7               -7.73      5.35    -17.63     2.99          9 1.55
## factorField8               -9.51      5.23    -18.75     0.37          7 1.37
## factorField9               -0.09      4.62     -9.50     9.01          9 1.35
## factorField10              17.14      4.92      7.80    27.02          5 1.53
## factorField11              14.47      5.42      3.85    24.99         11 1.38
## factorField12              -1.82      5.70    -13.58     8.55          5 1.51
## factorField13            -111.31   7254.43 -11890.83 15083.34          2 5.44
## factorField14            -106.52   7254.44 -11883.23 15091.71          2 5.44
## factorField15            -125.17   7254.41 -11902.10 15070.68          2 5.44
## factorField16             -93.91   7254.40 -11873.31 15102.90          2 5.44
## factorField17            -117.99   7254.51 -11895.74 15076.08          2 5.44
## factorField18            -125.60   7254.47 -11899.57 15070.78          2 5.44
## factorField19            -115.92   7254.49 -11893.10 15077.36          2 5.44
## factorField20            -107.70   7254.43 -11880.38 15087.11          2 5.44
## factorField21            -129.63   7254.43 -11906.67 15065.57          2 5.44
## factorField22            -119.58   7254.46 -11896.83 15077.46          2 5.44
## factorField23            -829.25   5320.12  -9957.87  9041.24          3 3.66
## factorField24            -796.29   5320.26  -9924.95  9070.10          3 3.66
## factorField25            -827.91   5320.23  -9956.23  9034.72          3 3.66
## factorField26            -835.91   5320.19  -9967.37  9032.79          3 3.66
## factorField27            -818.54   5320.23  -9947.83  9051.39          3 3.66
## factorField28            -819.45   5320.23  -9947.22  9049.76          3 3.66
## factorField29            -831.15   5320.22  -9958.66  9034.40          3 3.66
## factorField30            -828.15   5320.24  -9954.33  9038.14          3 3.66
## factorField31            -801.45   5320.24  -9935.83  9069.36          3 3.66
## factorField32            -817.45   5320.35  -9952.94  9051.22          3 3.66
## factorField33            -799.54   5320.22  -9929.25  9069.07          3 3.66
## TillMoldbrd:FertDeep        1.29      3.33     -5.63     7.62         32 1.15
## TillRidge:FertDeep         17.63      3.56     10.70    24.88         54 1.06
## TillMoldbrd:FertSurface    -1.15      3.48     -7.92     5.82         26 1.18
## TillRidge:FertSurface      17.86      3.38     11.28    24.54         21 1.17
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma     5.80      0.57     4.75     7.04         45 1.05
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>That’s a lot of output. And the model is performing badly. Fortunately,
we don’t really want this model anyway.
We now have a an adjustment for each field, and there
were 33 fields. But we are not really interested in predicting
the yield <em>for a given field</em>. Our primary interest is in which fertilizers
and tilling methods work well. We hope our results apply generally to all fields.
So field field plays a different role in this study.
We are only comparing 3 fertilizers and 3 tilling methods,
but there are many more fields than the 33 in our study. They are intended to
be representative of all fields (and their variable quality for producing large
yields).</p>
<p>If we think that field quality might be described by a normal distribution (or
some other distribution), we might be more interested in the parameters of that
distribution than in the specific estimates for the particular fields in this
study. The kind of model we want for this is called a <strong>hierarchical</strong> or <strong>multi-level</strong> model, and <code>brm()</code> makes it easy to describe such a model.</p>
<p>Here’s a way to think about such a model</p>
<ul>
<li>Each field has a baseline productivity.</li>
<li>The baseline productivities are normal with some mean and standard
deviation that tell us about the distribution of productivity among
fields. Our 33 fields should helps us estimate this distribution.</li>
<li>That baseline productivity can be adjusted up or down depending on the
tilling method and fertilizer used.</li>
</ul>
<p>In <code>brm()</code> lingo, the effect of field is to adjust the intercept, so we can write
it like this:</p>
<pre class="sourceCode r"><code class="sourceCode r">fert4_brm &lt;-
<span class="st">  </span><span class="kw">brm</span>(Yield <span class="op">~</span><span class="st"> </span>Till <span class="op">*</span><span class="st"> </span>Fert <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Field), <span class="dt">data =</span> SplitPlotAgri)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<p>We can see in the output below that the variability from plot to plot is
estimated by a standard deviation of roughly 8 to 15. Individual field
estimates are hidden in this report, but you can see them if you type
<code>stanfit(fert_brm)</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">fert4_brm</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Yield ~ Till * Fert + (1 | Field) 
##    Data: SplitPlotAgri (Number of observations: 99) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~Field (Number of levels: 33) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)    11.70      1.73     8.82    15.49        777 1.00
## 
## Population-Level Effects: 
##                         Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept                 124.09      3.75   116.87   131.76        764 1.00
## TillMoldbrd                16.77      5.73     4.92    28.11        817 1.00
## TillRidge                   3.85      5.47    -7.03    14.83        853 1.00
## FertDeep                    9.55      2.26     5.20    14.06       2154 1.00
## FertSurface                 7.22      2.30     2.61    11.67       1975 1.00
## TillMoldbrd:FertDeep        0.68      3.44    -5.98     7.46       2254 1.00
## TillRidge:FertDeep         17.90      3.33    11.43    24.28       2162 1.00
## TillMoldbrd:FertSurface    -2.04      3.43    -8.69     4.75       2136 1.00
## TillRidge:FertSurface      17.98      3.29    11.39    24.24       2167 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma     5.69      0.53     4.75     6.84       1743 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>The three groupings of the parameters shows are</p>
<ul>
<li><p>group-level effects</p>
<p>This is where we find the standard deviation associated with
<code>Field</code>. We are interested in the fields as a group, not as individual
fields.</p></li>
<li><p>population-level effects</p>
<p>The parameters for <code>Till</code> and <code>Fert</code> go here.</p></li>
<li><p>family specific</p>
<p>This is where we find parameters associated with the “noise” of the model,
in this case the standard deviation of the normal distribution.
If we used a t-distribution, we would find <code>nu</code> and <code>sigma</code> here.</p></li>
</ul>
</div>
<div id="which-model-should-we-use" class="section level2">
<h2><span class="header-section-number">20.3</span> Which model should we use?</h2>
<div id="modeling-choices" class="section level3">
<h3><span class="header-section-number">20.3.1</span> Modeling Choices</h3>
<p>Now that we are able to create more and more kinds of models, model selection
is going to become a bigger issue.
Here are just some of the choices we now have when constructing a model.</p>
<ol style="list-style-type: decimal">
<li><p>What variables?</p>
<p>If our primary goal is to study the association between the
response and certain predictor variables, we need to include
those variables in the model.</p>
<p>But additional variables can be helpful if they explain some of the
variation in the response variable in a way that makes it easier
to see the association with the variables of interest.</p>
<p>Since we have information about fields, and it seems plausible that
productivity varies by field, we prefer models include <code>Field</code>.
Similarly, even if we were only intersted in one of fertilizer or tilling
method, it may be useful to include both.</p>
<p>We might wish for some additional variables for our study of crop yields.
Perhaps knowing additional information about the fields (soil type,
hilly or flat? water shed or water basin? previous years crop, etc.).
Any of these things might help explain the variation from field
to field.</p>
<p>But adding too many variables can actually make this worse!</p>
<ul>
<li><p>If variables are correlated in our data (colinearity of predictors),
including both will usually make our posterior distributions
for the associated parameters much wider.</p></li>
<li><p>Additional variables can lead to over-fitting. Additional variables
will always make our model fit the current data set better, but eventually
we will begin fitting the idiosyncracies of our particular data set
rather than patterns that are likely to extend to new observations.</p></li>
</ul></li>
<li><p>Interaction terms?</p>
<p>When we use more than one predictor, we need to decide whether to include
interaction terms. Interaction is important to include if we are open to
to the possibility that the “effect” of one variable on the response
may depend on the value of some other variable.</p></li>
<li><p>Noise distribution?</p>
<p>Normal distributions are traditional and relatively easy to interpret.
T distributions are more robust against unusual observations or
heavy tailed distributions. Both of these are symmetric distributions.
If we expect or see evidience of a lack of symmetry, we may need to
use transformations of the data or other families of distributions.</p></li>
<li><p>What priors?</p>
<p>Bayesian inference adds another layer: prior selection. For most of our
models we have used “weakly informed priors”. These priors avoid parameter
values that are impossible (negative values for standard deviations, for
example) and provide crude order-of-magnitude guidance. They can also
serve a mild regularizing effect (shrinking parameter estimates toward 0,
for example, to counter against over fitting). If more information is
available, it is possible to use more informed priors.</p>
<p>Choice of prior matters more for small data sets than for large data sets.
This makes sense both mathematically and intuitively. Intiuitively,
if we don’t have much new data, it won’t change our beliefs much from what
they were before. But if we have a lot of data, we will come to roughly
the same conclusion no matter what we believed before.</p></li>
<li><p>Multi-level?</p>
<p>Are the values of nominal variables in our data exhaustive of the
possibilities (or of our interests)? Or are they just representative
of a larger set of possible values?
In the latter case, a multi-level model may be called for.</p>
<p>To clarify this, it can be good to imagine expanding the data set. If you were
to collect additional data, would the variables take on new values?
In our crop yield study, adding more data would require using new fields,
but we could still use the same three fertilizers and same three tilling
methods. Furthermore, the three tilling methods selected are not likely
representative of some distribution of tilling methods the way the fields
studied might be representative of many fields in a given region.</p></li>
</ol>
<p>These are only some of the questions we need to answer when constructing a model.
But how do we decide? Part of the decision is based on things we know or believe
in advance. Our model may be designed to reflect a theory about how data
are generated or may be informed by other studies that have been done in similar
situations. But there are also ways to investigate and compare models.</p>
</div>
<div id="measuring-a-model-prediction-error" class="section level3">
<h3><span class="header-section-number">20.3.2</span> Measuring a Model – Prediction Error</h3>
<div id="prediction-vs.observation" class="section level4">
<h4><span class="header-section-number">20.3.2.1</span> Prediction vs. Observation</h4>
<p>One way to measure how well a model is working is to compare the predictions
the model makes for the response variable <span class="math inline">\(\hat y_i\)</span> to the observed
response values in the data <span class="math inline">\(y_i\)</span>. To simplify things, we would like to convert
these <span class="math inline">\(n\)</span> predictions and <span class="math inline">\(n\)</span> observations into a single number.</p>
<p>If you have taken a statistics course before, you may have done this using
<strong>Sum of Squared Errors</strong> (SSE) or <strong>Mean Squared Error</strong> (MSE).</p>
<p><span class="math display">\[\begin{align*}
SSE &amp; = \sum_{i = 1}^n (y_i - \hat y_i)^2 \\
MSE &amp; = \frac{1}{n} SSE = \frac{1}{n} \sum_{i = 1}^n (y_i - \hat y_i)^2
\end{align*}\]</span></p>
<p>If you are familiar with <span class="math inline">\(r^2\)</span>, it is related to MSE:</p>
<p><span class="math display">\[\begin{align*}
SSE &amp;= \sum_{i = 1}^n (y_i - \hat y_i)^2 \\
SST &amp;= \sum_{i = 1}^n (y_i - \overline{y})^2 \\
r^2 &amp;= 1 - \frac{SSE}{SST}
\end{align*}\]</span></p>
<p>We are working with Bayesian models, so <span class="math inline">\(SSE\)</span>, <span class="math inline">\(MSE\)</span> and <span class="math inline">\(r^2\)</span> have posterior
distributions, since they depend on (the posterior distribution of) <span class="math inline">\(\theta\)</span>.
Each posterior value of <span class="math inline">\(\theta\)</span> leads to a value of <span class="math inline">\(\hat y_i = E(y_i \mid \theta)\)</span>
and that in turn leads to a values of <span class="math inline">\(SSE\)</span>, <span class="math inline">\(MSE\)</span>, and <span class="math inline">\(r^2\)</span>.</p>
<p>Putting that all together to highlight the dependence on <span class="math inline">\(\theta\)</span>, we get</p>
<p><span class="math display">\[MSE = \frac{1}{n} \sum_{i = 1}^n (y_i - E(y_i \mid \theta))^2\]</span></p>
<p>The intuition behind all three quantities is that model fit can be measured
by how close the model prediction <span class="math inline">\(\hat y_i\)</span> is to the obsered resopnse <span class="math inline">\(y_i\)</span>.
<span class="math inline">\(MSE\)</span> adjusts for sample size to make it easier to compare values
across data sets of different sizes. <span class="math inline">\(r^2\)</span> makes a further normalization to
put things on a 0-1 scale. (1 is a perfect fit. 0 means the model always gives
the same prediction, so it isn’t doing anything useful.)</p>
</div>
<div id="log-predictive-density" class="section level4">
<h4><span class="header-section-number">20.3.2.2</span> (Log) predictive density</h4>
<p>Another option is to compute <strong>log predictive density</strong> (lpd):</p>
<p><span class="math display">\[\mathrm{lpd}(\theta; y) = \log p(y \mid \theta)\]</span></p>
<p>Once again, <span class="math inline">\(y\)</span> is fixed, so this is a function of <span class="math inline">\(\theta\)</span>.
In fact, it is just the log likelihood function.
For a given value of <span class="math inline">\(\theta\)</span>,
lpd measures (on a log scale) the probability of observing the data.
A larger value indicates a better fit.
Once again, because lpd is a function of <span class="math inline">\(\theta\)</span>,
it also has a posterior distribution.</p>
<p>Assuming that the values of <span class="math inline">\(y\)</span> are independent given the parameters
(and the predictor values <span class="math inline">\(x\)</span>), this can be written as</p>
<p><span class="math display">\[
\mathrm{lpd}(\theta; y)
= \log p(y \mid \theta)
= \log \prod_{i = 1}^n p(y_i \mid \theta)
= \sum_{i = 1}^n \log p(y_i \mid \theta)
\]</span></p>
<p>In this case, we can compute the log posterior density pointwise and add.
In practice, this is often done even when independence does not hold. So
technically we are working with <strong>log pointwise posterior density</strong>:</p>
<p><span class="math display">\[
\mathrm{lppd}(\theta; y)
= \sum_{i = 1}^n \log p(y_i \mid \theta)
\]</span>
As with <span class="math inline">\(SSE\)</span>, <span class="math inline">\(MSE\)</span>, and <span class="math inline">\(r^2\)</span> this assigns a score to each <span class="math inline">\(i\)</span> and then sums over
those scores.</p>
<p>For linear models with normal noise and uniform priors,
lpd is proportional to <span class="math inline">\(MSE\)</span> (and to <span class="math inline">\(SSE\)</span>).<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
</div>
<div id="predictors" class="section level4">
<h4><span class="header-section-number">20.3.2.3</span> Predictors</h4>
<p>In the notation above, we have been hiding the role of predictors <span class="math inline">\(x\)</span> (and we will
continue to do so below).
A model with predictors makes different predictions depending on the
vaules of the predictors. In all our examples, <span class="math inline">\(x\)</span> will be fixed, but we could
include it in the notation if we wanted. For example,</p>
<p><span class="math display">\[
\mathrm{lpd}(\theta; y, x)
= \log p(y \mid \theta, x)
\]</span></p>
</div>
<div id="numbers-from-distributions" class="section level4">
<h4><span class="header-section-number">20.3.2.4</span> Numbers from distributions</h4>
<p>We can convert a measure <span class="math inline">\(\mathrm{lpd}(\theta; y)\)</span>, which depends on <span class="math inline">\(\theta\)</span>,
into a single number in several ways. We will illustrate below</p>
<ol style="list-style-type: decimal">
<li>We could replace <span class="math inline">\(\theta\)</span> with a particular number <span class="math inline">\(\hat \theta\)</span>.
(<span class="math inline">\(\hat \theta\)</span> might be the mean, median, or mode of the posterior distribution
or the mode of the likelihood function, for example).
If we do this we get the number</li>
</ol>
<p><span class="math display">\[
\mathrm{lpd}(\hat \theta; y) 
= \log p(y \mid \hat\theta) 
= \sum_{i = 1}^n \log p(y_i \mid \theta)
\]</span>
This is sometimes called a “plug-in” estimate since we are plugging in a single
number for <span class="math inline">\(\theta\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Instead of summarizing <span class="math inline">\(\theta\)</span> with a single number, we could summarize
<span class="math inline">\(p(y_i \mid \theta)\)</span> with a single number by averaging over the posterior
sample values <span class="math inline">\(p(y_i \mid \theta^s)\)</span>. (<span class="math inline">\(\theta^s\)</span> denotes the value of
<span class="math inline">\(\theta\)</span> in row <span class="math inline">\(s\)</span> of our <span class="math inline">\(S\)</span> posterior samples.)
If sum over <span class="math inline">\(i\)</span>, we get the <strong>log pointwise posterior density</strong> (lppd):</li>
</ol>
<p><span class="math display">\[\begin{align}
\mathrm{lppd}  
&amp;\approx 
\sum_{i = 1}^n \log \left( \frac{1}{S} \sum_{s = 1}^S  p(y_i \mid \theta^s)\right)
\end{align}\]</span></p>
<p>This is an approximation because our poseterior samples are only an approximation
to the true posterior distribution. But if the effective sample size of the
posterior is large, this approximation should be very good.</p>
<!-- 1. Posterior predictive checks. -->
<!-- If our model is reflective of the way the data were generated, then we should be -->
<!-- able to use it to generate new data that is similar to the actual data. Posterior -->
<!-- predictive checks of various sorts can be use to investigate. -->
<p>Unfortunately, both of these measures (<span class="math inline">\(MSE\)</span> and log predictive density)
have a problem.
They measure how well the model fits the data used to fit the model, but
we are more interested in how well the model might fit new data
(generated by the same random process that generated the current data).
This leads to <strong>overfitting</strong> and <strong>prefers larger, more complex models</strong>,
since the extra flexibility of these models makes it easier for them to
“fit the data”.</p>
<!-- * We aren't really interested in how well our model fits the current data, but how  -->
<!-- well it would fit other data (new data to be collected in the future or hypothetical -->
<!-- data that could have been collected). -->
<!-- * It may be better to incorporate elements of multiple models into a final -->
<!-- model than to simply choose one of them. -->
</div>
</div>
<div id="out-of-sample-prediction-error" class="section level3">
<h3><span class="header-section-number">20.3.3</span> Out-of-sample prediction error</h3>
<p>More interesting would be to measure how well the models would fit <strong>new data</strong>.
This is referred to as <strong>out-of-sample prediction</strong>, in contrast to
<strong>in-sample prediction</strong>.</p>
<p>So let’s consider how well our model predicts new data <span class="math inline">\(\tilde y\)</span> rather than
the observed data <span class="math inline">\(y\)</span>:</p>
<!-- \mathrm{E}_{\mathrm{post}}(\mathrm{lpd}) -->
<p><span class="math display">\[
\mathrm{lpd}(\theta; \tilde y) 
= \log p(\tilde y \mid \theta)
= \log \prod_{i = 1}^n p(\tilde y_i \mid \theta)
= \sum_{i = 1}^n \log p(\tilde y_i \mid \theta)
\]</span></p>
<p>which we can convert into a single number by plugging by posterior averaging:</p>
<!-- $$ -->
<!-- \mathrm{lpd}(\hat \theta; \tilde y) -->
<!-- = \log p(\tilde y \mid \hat \theta) -->
<!-- = \log \prod_{i = 1}^n p(\tilde y_i \mid \hat \theta) -->
<!-- = \sum_{i = 1}^n \log p(\tilde y_i \mid \hat \theta) -->
<!-- $$ -->
<!-- or by averaging over the posterior distribution -->
<!-- $$ -->
<!-- \log p_{\mathrm{post}}(\tilde y) -->
<!-- = -->
<!-- \log \mathrm{E}_{\mathrm{post}}(p(\tilde y \mid \theta)) -->
<!-- = -->
<!-- \sum_{i = 1}^n  -->
<!-- \log \left(\frac{1}{S} \sum_{s = 1}^S p(\tilde y_i \mid \theta_i)\right) -->
<!-- $$ -->
<p>And since <span class="math inline">\(\tilde y\)</span> is not fixed (like <span class="math inline">\(y\)</span> was), we take an additional
step and compute the expected value (average) of this quantity over the
distribution of <span class="math inline">\(\tilde y_i\)</span> to get the <strong>expected log (pointwise) predictive density</strong> for a new response <span class="math inline">\(\tilde y_i\)</span>:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p><span class="math display">\[
\mathrm{elppd} 
=
\mathrm{E}\left(\sum_{i = 1}^n \log p_{\mathrm{post}}(\tilde y_i)\right)
\approx
\sum_{i = 1}^n \mathrm{E}\left(\log 
\frac{1}{S} \sum_{s = 1}^S p(\tilde y_i \mid \theta^s))\right)
\]</span></p>
<p>This expected value is taken over the true distribution of <span class="math inline">\(\tilde y_i\)</span> (which is a
problem, stay tuned.)</p>
</div>
<div id="approximating-out-of-sample-prediction-error" class="section level3">
<h3><span class="header-section-number">20.3.4</span> Approximating out-of-sample prediction error</h3>
<p>What we would ideally want (elppd), we cannot compute
since it requires us to know the distribution of out-of-sample data
(<span class="math inline">\(\tilde y_i\)</span>).
This leads us to the following impossible set of goals for our ideal
measure of model (predictive) performance (borrowed from <span class="citation">(Gelman, Hwang, and Vehtari <a href="#ref-Gelman:2014">2014</a>)</span>):</p>
<ul>
<li>an <strong>unbaised</strong> and <strong>accurate</strong> measure</li>
<li>of <strong>out-of-sample prediction</strong> error (elppd)</li>
<li>that will be valid over a <strong>general</strong> class of models,</li>
<li>and that <strong>requires minimal computation</strong> beyond that need to fit the model
in the first place.</li>
</ul>
<p>Here are three approaches to solving this problem</p>
<ol style="list-style-type: decimal">
<li><p>Use within-sample predictive accuracy.</p>
<p>But this isn’t ideal since it overestimates performace of the model
(and more so for more complicated models).</p></li>
<li><p>Adjust within-sample predictive accuracy.</p>
<p>Within-sample predictive accuracy will over-estimate out-of-sample predictive
accuracy. If we knew (or could estimate) by how much, we could adjust
by that amount to eliminate (or reduce) the bias. Quantities like
AIC (Aikeke’s information criterion),
DIC (deviance information criterion),
and WAIC (widely applicable information criterion)
take the approach of substracting something from lppd that
depends on the complexity of the model.</p></li>
<li><p>Use cross-validation</p>
<p>The main idea here is to use some of the data to fit the model and the rest
of the data to evaluate prediction error. This is a poor person’s version of
“out-of-sample”. We will focus on <strong>leave one out</strong> (LOO) cross validation
where we fit the model <span class="math inline">\(n\)</span> times, each time leaving out one row of the data
and using the resulting model to predict the removed row.
If we really needed to recompute the model <span class="math inline">\(n\)</span> times,
this would be too computationally expensive for large data sets and complex
models. But there are (more) efficient
approximations to LOO-cv that make it doable. They are based on the idea
that the posterior distribution using <span class="math inline">\(y(-i)\)</span> (all but row <span class="math inline">\(i\)</span> of the data)
should usually be similar to the posterior distribution using <span class="math inline">\(y\)</span> (all of the data).
So we can recycle the work done to compute our original posterior.
The result is only an approximation, and it doesn’t always work well,
so sometimes we have to recreate the posterior from scratch, at least for
some of the rows.</p></li>
</ol>
<p>The formulas for <strong>estimated out-of-sample predictive density</strong></p>
<p><span class="math display">\[\begin{align*}
\widehat{\mathrm{elppd}}_{\mathrm{AIC}}
  &amp;= \mathrm{lpd}(\hat\theta_{\mathrm{mle}}, y) - p_{\mathrm{AIC}} \\
\widehat{\mathrm{elppd}}_{\mathrm{DIC}} 
  &amp;= \mathrm{lpd}(\hat\theta_{\mathrm{Bayes}}, y) - p_{\mathrm{DIC}} \\
\widehat{\mathrm{elppd}}_{\mathrm{WAIC}} 
  &amp;= \mathrm{lppd} - p_{\mathrm{WAIC}} \\
\widehat{\mathrm{elppd}}_{\mathrm{LOO}} 
  &amp;= \sum_{i=1}^n \log p_{\mathrm{post}(-i)}(y_i) 
  \approx \sum_{i=1}^n \log \left( \frac{1}{S} \sum_{s = 1}^S p(y_i \mid \theta^{is})\right)
\end{align*}\]</span></p>
<p>and the associated <strong>effictive number of parameters</strong>:</p>
<p><span class="math display">\[\begin{align*}
p_{\mathrm{AIC}}  &amp;= \mbox{number of parameters in the model}\\
p_{\mathrm{DIC}}  &amp;= 2 \mathrm{var}_{\mathrm{post}}(\log p(y \mid \theta)) \\
p_{\mathrm{WAIC}} &amp;= 2 \mathrm{var}_{\mathrm{post}}(\sum_{i = 1}^n \log p(y_i \mid \theta)) \\
p_{\mathrm{LOO}}  &amp;= \hat{\mathrm{llpd}} - \hat{\mathrm{llpd}}_{\mathrm{LOO}} \\
\end{align*}\]</span></p>
<p>Notes</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\theta^{is}\)</span> is the value of <span class="math inline">\(\theta\)</span> in row <span class="math inline">\(s\)</span> of the posterior distribution
<em>when row <span class="math inline">\(i\)</span> has been removed from the data</em>. What makes LOO practical is
that this can be approximated without refitting the model <span class="math inline">\(n\)</span> times.</p></li>
<li><p>AIC and DIC differ from WAIC and LOO in that they use a point estimate
for <span class="math inline">\(\theta\)</span> (the maximum likelihood estimate for AIC and the
mode of the posterior distribution for DIC) rather than using the
full posterior distribution.</p></li>
<li><p>AIC penalizes a model 1 for each parameter. This is correct for linear
models with normal noise and uniform priors, but is not correct in general.
You can think of DIC and WAIC as estimating the effective number of
parameters by looking at how much variation there is in <span class="math inline">\(\log(p(y_i \mid \theta))\)</span>.
The more this quantity changes with changes in <span class="math inline">\(\theta\)</span>, the more flexible
the model is (and the more it should be penalized).</p></li>
<li><p>LOO doesn’t work by adusting for an estimated number of parameters;
it attempts to estimate elppd directly.
But we can reverse engineer things to get an estimated number of parameters
by taking the difference between
the (estimated) within-sample and out-of-sample predictive density.</p></li>
<li><p>LOO and WAIC are assymptotically equivalent (that is they give more
and more similar values as the sample size increases), but LOO typically
performs a bit better on small data sets, so the authors of the loo package
recommend LOO over WAIC as the go-to measure for comparing models.</p></li>
<li><p>Historically, information criteria have been expressed on the “devaiance scale”.
To convert from log predictive density scale to deviance scale, we multiply by -2.
On the deviance scale, smaller is better.
On the log predictive density scale, larger is better
(but the values are usually negative.) The <code>waic()</code> and <code>loo()</code> functions
compute both values.</p></li>
<li><p>The output from <code>loo()</code> and <code>waic()</code> labels things elpd rather than elppd.</p></li>
</ol>
<!-- The problem is that we have used all of our data to create our -->
<!-- model.  What to do? -->
<!--     a. Training and Testing -->
<!--     We could split our data into two portions. The first part (training data) -->
<!--     would be used to fit the model and the second (test data) would be used  -->
<!--     to measure how well the model performs. Models that are overfitting may do -->
<!--     well on the training data but do poorly with test data. Other models may -->
<!--     not do as well with the training data, but might do better with the test data. -->
<!--     We should prefer the latter sort of model. -->
<!--     The downside is that we have lost a portion of our data for fitting purposes. -->
<!--     b. Cross validation. -->
<!--     Cross validation is a bit like Train/Test repeated many times. Each time we fit -->
<!--     the model to a portion of the data and see how it performs on the rest  -->
<!--     of the data. LOO (leave one out) cross validation does this by leaving out -->
<!--     one row of the data and seeing how well a model fit to the remaining rows  -->
<!--     predicts it.  If we have $n$ rows of data, we can create $n$ leave-one-out  -->
<!--     models. -->
<!--     One downside of cross validation is that it can be computationally expensive. -->
<!--     (That's a lot of bayesian models to fit.) To speed things up, approximations -->
<!--     are used. The loo R package provides tools for computing approximate LOO -->
<!--     cross-validation on Bayesian models. The method employed goes by the name -->
<!--     Pareto smoothed importance-sampling leave-one-out cross-validation  -->
<!--     (PSIS-LOO). -->
<!--     c. Information criteria. -->
<!--     AIC (Aikeke's Information Criterion),  -->
<!--     DIC (Deviance Information Criterion),  -->
<!--     and WAIC (Widely Applicable Information Criterion) -->
<!--     are successively more complicated (and better) ways of estimating -->
<!--     out-of-sample prediction. As the names suggest, they are all based  -->
<!--     on a concept called information. The loo R package also provides -->
<!--     methods for computing WAIC (but it recommends using LOO). -->
<!-- ## Measuring fit -->
<!-- ### $R^2$ -->
<!-- Using $R^2$ alone as a measure of fit has the problem that $R^2$ increases -->
<!-- as we add complexity to the model, which pushes us toward overfitting. -->
<!-- ```{r} -->
<!-- Brains.R2 <- -->
<!--   data_frame( -->
<!--     degree = 0:6, -->
<!--     R2 = sapply( list(m7, m1, m2, m3, m4, m5, m6), mosaic::rsquared) -->
<!--   ) -->
<!-- Brains.R2 -->
<!-- gf_point(R2 ~ degree, data = Brains.R2) %>% -->
<!--   gf_line(R2 ~ degree, data = Brains.R2, alpha = 0.5) %>% -->
<!--   gf_labs(x = "degree of polynomial", y = expression(R^2)) -->
<!-- ``` -->
<!-- There are ways to adjust $R^2$ to reduce this problem, but we are going to introduce  -->
<!-- other methods of measuring fit. -->
<!-- ```{r} -->
<!-- gf_point(R2 ~ degree, color = ~ factor(degree), data = Brains.R2) %>% -->
<!--   gf_line(R2 ~ degree, data = Brains.R2, alpha = 0.5) %>% -->
<!--   gf_segment(R2 + 1 ~ degree + 7,  color = ~ factor(degree),  -->
<!--              data = Brains.R2, alpha = 0.3) %>% -->
<!--   gf_labs(x = "degree of polynomial", y = expression(R^2)) -->
<!-- ``` -->
<!-- ### Weather Prediction Accuracy -->
<!-- Consider the predictions of two weather people over the same set of 10 days. -->
<!-- Which one did a better job of predicting?  How should we measure this? -->
<!--  * **First Weather Person:** -->
<!--     ![](images/weather1.png) -->
<!--  * **Second Weather Person:** -->
<!--     ![](images/weather2.png) -->
<!-- Last time we discussed some ways to compare which weather person makes the best predictions. -->
<!-- Here is one more: Given each weather person's "model" as a means of generating data, which  -->
<!-- one makes the observed weather most likely?  Now weather person 1 wins handily: -->
<!-- ```{r} -->
<!-- # WP #1 -->
<!-- 1^3 * 0.4^7 -->
<!-- # WP #2 -- no chance! -->
<!-- 0^3 * 1^7 -->
<!-- ``` -->
<!-- This has two advantages for us: -->
<!--  1. This is just the likelihood, an important part of our Bayesian modeling system. -->
<!--  2. It is based on joint probability rather than average probability.  Weather person 2 is taking unfair advantage of average probability by making predictions we know are "impossible". -->
<!-- ## Shannon Entropy and related notions -->
<!-- Now let's take a bit of a detour on the road to another method of assessing the predictive -->
<!-- accuracy of a model.  The route will look something like this: -->
<!-- $$ -->
<!-- \mbox{Information} \to \mbox{(Shannon) Entropy} \to \mbox{Divergence} \to \mbox{Deviance} \to \mbox{Inforation Criteria (DIC and WAIC)} -->
<!-- $$ -->
<!-- DIC (Deviance Information Criterion) and WAIC (Widely Applicable Information Criterion) are  -->
<!-- where we are heading.  For now, you can think of them as improvements to (adjusted) $R^2$ -->
<!-- that will work better for Bayesian models. -->
<!-- ### Information -->
<!-- Let's begin by considering the amount of  -->
<!-- information we gain when we observe some random process.   -->
<!-- Suppose that the event we observed has probability $p$. -->
<!-- Let $I(p)$ be the amount of information we gain from observing  -->
<!-- this outcome.  $I(p)$ depends on $p$ but not on the outcome itself, -->
<!-- and should satisfy the following properties. -->
<!--  1. $I(1)$ = 0.   -->
<!--     Since the outcome was certain, we didn't learn anything by observing. -->
<!--  2. $I(0)$ is undefined. -->
<!--     We won't observe an outcome with probability 0. -->
<!--  3. $I()$ is a decreasing function of $p$.   -->
<!--     The more unusual the event, the more information we obtain when it occurs. -->
<!--     In particular, $I(p) \ge 0$ for all $p \in (0, 1]$. -->
<!--  4. $I(p_1 p_2) = I(p_1) + I(p_2)$.   -->
<!--     This is motivated by independent events.  If we observe two independent  -->
<!--     events $A_1$ and $A_2$ with probabilities $p_1$ and $p_2$, we can consider this as a single event with -->
<!--     probability $p_1 p_2$. -->
<!-- The function $I()$ should remind you of a function you have seen before. -->
<!-- Logarithms satisfy these properties 1, 2, and 4, but logarithms are increaseing -->
<!-- functions.  We get the function we want if we define -->
<!-- $$ -->
<!-- I(p) = - \log(p) = \log(1/p) -->
<!-- $$ -->
<!-- We can choose any base we like: 2, $e$, and $10$ are common choices.   -->
<!-- Our text chooses natural logarithms.   -->
<!-- In can be shown that negative logarithms are the only -->
<!-- functions that have our desired properties. -->
<!-- ### Entropy -->
<!-- Now consider a random process $X$ with $n$ outcomes having probabilities  -->
<!-- $\mathbf{p} = p_1, p_2, \dots, p_n$. That is, -->
<!-- $$ -->
<!-- P(X = x_i) = p_i, -->
<!-- $$ -->
<!-- The amount of information for each outcome depends on $p_i$. -->
<!-- The **Shannon entropy** (denoted $H$) is the average (usually called "expected")  -->
<!-- amount of information gained from each observation of the random process: -->
<!-- $$ -->
<!-- H(X) = H(\mathrm{p}) = \mathrm{expected \ information} =  \sum p_i \cdot I(p_i) = - \sum p_i \log(p_i) -->
<!-- $$ -->
<!-- Note that  -->
<!--  * $H(X) \ge 0$ since $p_i \ge 0$ and $I(p_i) = - \log(p_i) \ge 0$.   -->
<!--  * Outcomes with probability 0 must be removed from the list  -->
<!--  (alternatively, we can treat $0 \log(0)$ as $0$ for the purposes of entropy.  -->
<!--  Note: $\lim_{q \to \infty} q \log(q) = 0$, so this is a continuous extension.) -->
<!--  * $H(X)$, like $I(p_i)$ depends only on the probabilities, not on the outcomes themselves. -->
<!--  * $H$ is a **continuous** function. -->
<!--  * Among all distributions with a fixed number of outcomes, $H$ is **maximized**  -->
<!--      when all outcomes are equally likely (for a fixed number of outcomes) -->
<!--  * among equiprobable distributions $H$ **increases as the number of outcomes increases**. -->
<!--  * $H$ is **additive** in the following sense: if $X$ and $Y$ are independent, then -->
<!--  $H(\langle X, Y\rangle) = H(X) + H(Y)$. -->
<!-- $H$ can be thought of as a measure of **uncertainty**. -->
<!-- Uncertainty decreases as we make observations. -->
<!--  * Consider a random variable that takes on only one value (all the time).   -->
<!--  There is nothing uncertain, and $H(X) = 1 \cdot \log(1) = 0$. -->
<!-- ```{r, chunk6.9a} -->
<!-- p <- 1 -->
<!-- - sum(p * log(p)) -->
<!-- ``` -->
<!--  * A random coin toss has entropy 1 if we use base 2 logarithms.  (In this case the unit is called a **shannon**  -->
<!--  or a bit of uncertainty.) -->
<!-- Applied to a 50-50 coin we get: -->
<!-- ```{r, chunk6.9b} -->
<!-- p <- c(0.5, 0.5) -->
<!-- # one shannon of uncertainty -->
<!-- - sum(p * log2(p)) -->
<!-- ``` -->
<!--  * It is more common in statistics to use natural logarithms. -->
<!--  In that case, the unit for entropy is called a **nat**,  -->
<!--  and  the entropy of a fair coin toss is -->
<!-- ```{r, chunk6.9c} -->
<!-- p <- c(0.5, 0.5) -->
<!-- # uncertainty of a fair coin in nats -->
<!-- - sum(p * log(p)) -->
<!-- ``` -->
<!-- We can write a little function to compute entropy for cases where there are only  -->
<!-- a finite number of outcomes. -->
<!-- ```{r} -->
<!-- H <- function(p, base = exp(1)) { -->
<!--   - sum(p * log(p, base = base)) -->
<!-- } -->
<!-- # in nats -->
<!-- H(c(0.5, 0.5)) -->
<!-- H(c(0.3, 0.7)) -->
<!-- # in shannons -->
<!-- H(c(0.5, 0.5), base = 2) -->
<!-- H(c(0.3, 0.7), base = 2) -->
<!-- ``` -->
<!-- ### Decrease in Entropy = Gained Information -->
<!-- Decreases in this uncertainty are gained information.   -->
<!-- #### R code 6.9 -->
<!-- Applied to a 30-70 coin we get: -->
<!-- ```{r, chunk6.9} -->
<!-- p <- c(0.3, 0.7) -->
<!-- - sum(p * log(p)) -->
<!-- ``` -->
<!-- ### Divergence -->
<!-- Kullback-Leibler divergence compares two distributions and asks "if we are  -->
<!-- anticipating \mathrm{q}, but get \mathrm{p}, how much more surprised will -->
<!-- we be than if we had been expecting \mathrm{p} in the first place?" -->
<!-- Here's the definition -->
<!-- $$ -->
<!-- D_{KL}(\mathrm{p}, \mathrm{q}) =  -->
<!-- \mathrm{expected\ difference\ in\ ``surprise"}  -->
<!-- = \sum p_i \left( I(q_i) - I(p_i) \right) -->
<!-- = \sum p_i I(q_i) - \sum p_i I(p_i)  -->
<!-- $$ -->
<!-- This looks like the difference between two entropies.  It alsmost is. -->
<!-- The first one is actually a **cross entropy** where we use probabilities -->
<!-- from one distribution and information from the other.  We denote this -->
<!-- $$ -->
<!-- H(\mathbf{p}, \mathbf{q}) = \sum p_i I(q_i) = - \sum p_i \log(q_i) -->
<!-- $$ -->
<!-- Note that $H(\mathbf{p}) = H(\mathbf{p}, \mathbf{p})$, so -->
<!-- \begin{align*} -->
<!-- D_{KL} &= H(\mathrm{p}, \mathrm{q}) - H(\mathrm{p}) -->
<!-- \\ -->
<!-- &= -->
<!-- \sum p_i \log(p_i) - \sum p_i \log(q_i) -->
<!-- \\ -->
<!-- &= \sum p_i (\log(p_i) - \log(q_i)). -->
<!-- \end{align*} -->
<!-- #### R code 6.10 -->
<!-- ```{r, chunk6.10} -->
<!-- # fit model with lm -->
<!-- m1 <- lm(brain_size ~ body_mass, data = Brains) -->
<!-- # compute deviance by cheating -->
<!-- (-2) * logLik(m1) -->
<!-- ``` -->
<!-- #### R code 6.11 -->
<!-- ```{r, chunk6.11} -->
<!-- # standardize the body_mass before fitting -->
<!-- Brains <- -->
<!--   Brains %>% mutate(body_mass.s = zscore(body_mass)) -->
<!-- m8 <- map( -->
<!--   alist(brain_size ~ dnorm(mu, sigma), -->
<!--         mu <- a + b * body_mass.s), -->
<!--   data = Brains, -->
<!--   start = list( -->
<!--     a = mean(Brains$brain_size), -->
<!--     b = 0, -->
<!--     sigma = sd(Brains$brain_size) -->
<!--   ), -->
<!--   method = "Nelder-Mead" -->
<!-- ) -->
<!-- # extract MAP estimates -->
<!-- theta <- coef(m8); theta -->
<!-- # compute deviance -->
<!-- dev <- (-2) * sum(dnorm( -->
<!--   Brains$brain_size, -->
<!--   mean = theta[1] + theta[2] * Brains$body_mass.s, -->
<!--   sd = theta[3], -->
<!--   log = TRUE -->
<!-- )) -->
<!-- dev %>% setNames("dev")  # setNames just labels the out put -->
<!-- -2 * logLik(m8)        # for comparison -->
<!-- ``` -->
</div>
</div>
<div id="using-loo" class="section level2">
<h2><span class="header-section-number">20.4</span> Using loo</h2>
<p>The loo package provides functions for computing WAIC and LOO estimates of
epld (and their information criterion counterparts).
While the definitions are a bit involved,
using WAIC or LOO to compare models is relatively easy.
WAIC can be faster, but LOO performs better (according to the authors of
the loo package).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(loo)
<span class="kw">waic</span>(fert4_brm)</code></pre>
<pre><code>## 
## Computed from 4000 by 99 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -331.1  7.5
## p_waic        30.7  3.9
## waic         662.1 15.0</code></pre>
<pre><code>## Warning: 21 (21.2%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">loo</span>(fert4_brm)</code></pre>
<pre><code>## Warning: Found 8 observations with a pareto_k &gt; 0.7 in model &#39;fert4_brm&#39;. It is recommended to set
## &#39;reloo = TRUE&#39; in order to calculate the ELPD without the assumption that these observations are
## negligible. This will refit the model 8 times to compute the ELPDs for the problematic observations
## directly.</code></pre>
<pre><code>## 
## Computed from 4000 by 99 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -335.1  8.1
## p_loo        34.7  4.5
## looic       670.2 16.2
## ------
## Monte Carlo SE of elpd_loo is NA.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     64    64.6%   616       
##  (0.5, 0.7]   (ok)       27    27.3%   167       
##    (0.7, 1]   (bad)       8     8.1%   34        
##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;      
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<p>Sometimes the LOO-PSIS (Pareto-smoothed importance sampling) approximation
method doesn’t work well and <code>loo()</code> recommends refitting some of the models
from scratch. This is based on the shape parameter (k) of the Pareto distribution
used to smooth the tails of the posterior.
Let’s allow <code>loo()</code> to run from scratch the models it thinks need it.
(This is still much faster than refitting a model for each row of the
data since we only start from scratch a small number of times. And we don’t
need to recompile the model, since that doesn’t change; we just need to
generate posterior samples using a different data set.)
If there are quite a number of these, <code>loo()</code> will suggest k-fold cross-validation
instead of leave-one-out cross-validation. These leaves out multiple rows of
data from each refit. Since there are fewer models this way, it can exchange speed
for accuracy.</p>
<pre class="sourceCode r"><code class="sourceCode r">fert4_loo &lt;-<span class="st"> </span><span class="kw">loo</span>(fert4_brm, <span class="dt">reloo =</span> <span class="ot">TRUE</span>) <span class="co"># refit as necessary</span></code></pre>
<pre><code>## 8 problematic observation(s) found.
## The model will be refit 8 times.</code></pre>
<pre><code>## 
## Fitting model 1 out of 8 (leaving out observation 15)</code></pre>
<pre><code>## 
## Fitting model 2 out of 8 (leaving out observation 19)</code></pre>
<pre><code>## 
## Fitting model 3 out of 8 (leaving out observation 31)</code></pre>
<pre><code>## 
## Fitting model 4 out of 8 (leaving out observation 57)</code></pre>
<pre><code>## 
## Fitting model 5 out of 8 (leaving out observation 77)</code></pre>
<pre><code>## 
## Fitting model 6 out of 8 (leaving out observation 82)</code></pre>
<pre><code>## 
## Fitting model 7 out of 8 (leaving out observation 83)</code></pre>
<pre><code>## 
## Fitting model 8 out of 8 (leaving out observation 87)</code></pre>
<pre><code>## Start sampling
## Start sampling
## Start sampling
## Start sampling
## Start sampling
## Start sampling
## Start sampling
## Start sampling</code></pre>
<p>In this case, things didn’t change that much when refitting the six “bad” models.</p>
<pre class="sourceCode r"><code class="sourceCode r">fert4_loo</code></pre>
<pre><code>## 
## Computed from 4000 by 99 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -335.3  8.2
## p_loo        34.9  4.6
## looic       670.6 16.4
## ------
## Monte Carlo SE of elpd_loo is 0.3.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     72    72.7%   34        
##  (0.5, 0.7]   (ok)       27    27.3%   167       
##    (0.7, 1]   (bad)       0     0.0%   &lt;NA&gt;      
##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;      
## 
## All Pareto k estimates are ok (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fert4_loo)</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo4-reprise-1.png" width="576" /></p>
<pre class="sourceCode r"><code class="sourceCode r">fert4a_loo &lt;-<span class="st"> </span><span class="kw">loo</span>(fert4_brm)</code></pre>
<pre><code>## Warning: Found 8 observations with a pareto_k &gt; 0.7 in model &#39;fert4_brm&#39;. It is recommended to set
## &#39;reloo = TRUE&#39; in order to calculate the ELPD without the assumption that these observations are
## negligible. This will refit the model 8 times to compute the ELPDs for the problematic observations
## directly.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fert4a_loo)</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo4-reprise-2.png" width="576" /></p>
<p>If we have multiple models, we can use <code>loo::compare()</code> to compare them based on
WAIC or LOO. Before doing that, let’s add one more model to our list.</p>
<pre class="sourceCode r"><code class="sourceCode r">fert5_brm &lt;-
<span class="st">  </span><span class="kw">brm</span>(Yield <span class="op">~</span><span class="st"> </span>Till <span class="op">+</span><span class="st"> </span>Fert <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Field), <span class="dt">data =</span> SplitPlotAgri)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## recompiling to avoid crashing R session</code></pre>
<pre><code>## Start sampling</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(loo)
<span class="kw">compare</span>(
  <span class="kw">waic</span>(fert1_brm), 
  <span class="kw">waic</span>(fert2_brm),
  <span class="kw">waic</span>(fert4_brm),
  <span class="kw">waic</span>(fert5_brm)
)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">fert1_loo &lt;-<span class="st"> </span><span class="kw">loo</span>(fert1_brm) 
fert2_loo &lt;-<span class="st"> </span><span class="kw">loo</span>(fert2_brm) 
fert5_loo &lt;-<span class="st"> </span><span class="kw">loo</span>(fert5_brm) </code></pre>
<pre><code>## Warning: Found 1 observations with a pareto_k &gt; 0.7 in model &#39;fert5_brm&#39;. It is recommended to set
## &#39;reloo = TRUE&#39; in order to calculate the ELPD without the assumption that these observations are
## negligible. This will refit the model 1 times to compute the ELPDs for the problematic observations
## directly.</code></pre>
<p>Now we can compare our four models using LOO:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compare</span>(fert1_loo, fert2_loo, fert4_loo, fert5_loo)</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">elpd_diff</th>
<th align="right">se_diff</th>
<th align="right">elpd_loo</th>
<th align="right">se_elpd_loo</th>
<th align="right">p_loo</th>
<th align="right">se_p_loo</th>
<th align="right">looic</th>
<th align="right">se_looic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>fert4_loo</td>
<td align="right">0.00</td>
<td align="right">0.000</td>
<td align="right">-335.3</td>
<td align="right">8.222</td>
<td align="right">34.933</td>
<td align="right">4.5779</td>
<td align="right">670.6</td>
<td align="right">16.44</td>
</tr>
<tr class="even">
<td>fert5_loo</td>
<td align="right">-23.75</td>
<td align="right">6.290</td>
<td align="right">-359.0</td>
<td align="right">7.618</td>
<td align="right">29.254</td>
<td align="right">3.7416</td>
<td align="right">718.1</td>
<td align="right">15.24</td>
</tr>
<tr class="odd">
<td>fert2_loo</td>
<td align="right">-61.26</td>
<td align="right">7.818</td>
<td align="right">-396.5</td>
<td align="right">5.940</td>
<td align="right">9.258</td>
<td align="right">1.0882</td>
<td align="right">793.1</td>
<td align="right">11.88</td>
</tr>
<tr class="even">
<td>fert1_loo</td>
<td align="right">-62.37</td>
<td align="right">8.145</td>
<td align="right">-397.7</td>
<td align="right">5.598</td>
<td align="right">5.460</td>
<td align="right">0.6145</td>
<td align="right">795.3</td>
<td align="right">11.20</td>
</tr>
</tbody>
</table>
<p>Important things to remember:</p>
<ol style="list-style-type: decimal">
<li><p>Estimated elpd and information criteria are not
meaningful on their own, they are only useful for <strong>comparisons</strong>.</p></li>
<li><p>Comparisons can only be made among models that are fit using the <strong>same data</strong>
since the computed values depend on both the model and the data.</p></li>
<li><p>All of these methods are approximate. <code>loo()</code> and <code>waic()</code> provide
standard errors as well as estimates.
Use those to help determine whether differences between models
are meaningful or not.</p></li>
<li><p><code>p_loo</code> (effective number of parameters) is also an interesting measure.
If this estimate does not seem to correspond to roughly the number of free
parameters in your model, that is usually a sign that something is wrong. (Perhaps
the model is mis-specified.) Keep in mind that multi-level models or models
with strong priors place some restrictions on the parameters. This can lead
to an effective number of parameters that is smaller than the actual number
of parameters.</p></li>
<li><p>This is only one aspect of how a model is performing. There may be good
reasons to prefer a model with lower (estimated) log predictive density.
Posterior predictive checks, theory, interpretability, etc. can all be part
of deciding which models are better. But these methods can help us avoid
selecting models that only look good because they are overfitting.</p>
<p>Note: Sometimes the best solution is to create a new model that combines
elements from models tried along the way.</p></li>
<li><p>Beware of “model hacking.” If you try enough models, you might stumble across
something. But it might not be meaningful. Choose models with some thought, don’t
just keep trying models in hopes that one of them will produce something interesting.</p></li>
</ol>
</div>
<div id="overfitting-example" class="section level2">
<h2><span class="header-section-number">20.5</span> Overfitting Example</h2>
<div id="brains-data" class="section level3">
<h3><span class="header-section-number">20.5.1</span> Brains Data</h3>
<p>This small data set giving the brain volume (cc) and body mass (kg) for several
species. It is used to illustrate a very bad idea – improving the “fit” by
increasing the degree of the polynomial used to model the relationship
between brain size and body mass.</p>
<pre class="sourceCode r"><code class="sourceCode r">Brains &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">data.frame</span>(
    <span class="dt">species =</span>  <span class="kw">c</span>(<span class="st">&quot;afarensis&quot;</span>, <span class="st">&quot;africanus&quot;</span>, <span class="st">&quot;habilis&quot;</span>, <span class="st">&quot;boisei&quot;</span>,
                 <span class="st">&quot;rudolfensis&quot;</span>, <span class="st">&quot;ergaster&quot;</span>, <span class="st">&quot;sapiens&quot;</span>),
    <span class="dt">brain_size =</span> <span class="kw">c</span>(<span class="dv">438</span>, <span class="dv">452</span>, <span class="dv">612</span>, <span class="dv">521</span>, <span class="dv">752</span>, <span class="dv">871</span>, <span class="dv">1350</span>),
    <span class="dt">body_mass =</span>  <span class="kw">c</span>(<span class="fl">37.0</span>, <span class="fl">35.5</span>, <span class="fl">34.5</span>, <span class="fl">41.5</span>, <span class="fl">55.5</span>, <span class="fl">61.0</span>, <span class="fl">53.5</span>)
  )
<span class="kw">gf_point</span>(brain_size <span class="op">~</span><span class="st"> </span>body_mass, <span class="dt">data =</span> Brains, 
         <span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>, <span class="dt">verbose =</span> <span class="ot">TRUE</span>)  <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_text</span>(brain_size <span class="op">~</span><span class="st"> </span>body_mass, <span class="dt">label =</span> <span class="op">~</span>species, <span class="dt">alpha =</span> <span class="fl">0.8</span>, 
          <span class="dt">color =</span> <span class="st">&quot;navy&quot;</span>, <span class="dt">size =</span> <span class="dv">3</span>, <span class="dt">angle =</span> <span class="dv">30</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch20-brains-1.png" width="576" /></p>
<p>To speed things up for this illustration, the model fits are frequentist
using <code>lm()</code>, but we could fit the same models using bayesian methods
and <code>brm()</code>. The model being fit below is</p>
<p><span class="math display">\[\begin{align*}
 \mbox{brain_size} &amp; \sim \mathrm{Norm}(\mu, \sigma) \\
 \mu  &amp; \sim a + b \cdot \mbox{body_mass} 
\end{align*}\]</span></p>
<p>There are no priors because <code>lm()</code> isn’t using a Bayesian approach.<br />
We’re using <code>lm()</code> here because it is faster to fit,
but the same principle would be illustrated if we used
a Bayesian linear model instead.</p>
<pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(brain_size <span class="op">~</span><span class="st"> </span>body_mass, <span class="dt">data =</span> Brains)</code></pre>
<p>(Note: <code>\lm()</code> fits the parameters using “maximum likelihood”. You can
think of this as using uniform priors on the coefficients,
which means that the posterior is proportional to the likelihood,
and maximum likelihood estimates are the same as the <em>maximum a posteriori</em>
(mode of the posterior distribution) estimates.
The estimate for <span class="math inline">\(\sigma\)</span> that <code>\lm()</code> uses is modified to make it
an <em>unbiased estimator</em>.)</p>
</div>
<div id="measuring-fit-with-r2" class="section level3">
<h3><span class="header-section-number">20.5.2</span> Measuring fit with <span class="math inline">\(r^2\)</span></h3>
<p><span class="math inline">\(r^2\)</span> can be defined several equivalent ways.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">var</span>(<span class="kw">resid</span>(m1)) <span class="op">/</span><span class="st"> </span><span class="kw">var</span>(Brains<span class="op">$</span>brain_size)</code></pre>
<pre><code>## [1] 0.4902</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sum</span>((Brains<span class="op">$</span>brain_size <span class="op">-</span><span class="st"> </span><span class="kw">fitted</span>(m1))<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sum</span>((Brains<span class="op">$</span>brain_size <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(Brains<span class="op">$</span>brain_size))<span class="op">^</span><span class="dv">2</span>)</code></pre>
<pre><code>## [1] 0.4902</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rsquared</span>(m1)  <span class="co"># rsquared is in the mosaic package</span></code></pre>
<pre><code>## [1] 0.4902</code></pre>
<p>In a Bayesian setting we would have a distribution of <span class="math inline">\(r^2\)</span> values, each computed
using a different row from the posterior sampling.</p>
<p>Now let’s consider a model that uses a quadratic relationship.</p>
<pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(brain_size <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(body_mass,<span class="dv">2</span>), <span class="dt">data =</span> Brains)
<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">var</span>(<span class="kw">resid</span>(m2)) <span class="op">/</span><span class="st"> </span><span class="kw">var</span>(Brains<span class="op">$</span>brain_size)</code></pre>
<pre><code>## [1] 0.536</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rsquared</span>(m2)</code></pre>
<pre><code>## [1] 0.536</code></pre>
<p>We can use any degree polynomial in the same way.</p>
<pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(brain_size <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(body_mass, <span class="dv">1</span>), <span class="dt">data =</span> Brains)
m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(brain_size <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(body_mass, <span class="dv">2</span>), <span class="dt">data =</span> Brains)
m3 &lt;-<span class="st"> </span><span class="kw">lm</span>(brain_size <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(body_mass, <span class="dv">3</span>), <span class="dt">data =</span> Brains)
m4 &lt;-<span class="st"> </span><span class="kw">lm</span>(brain_size <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(body_mass, <span class="dv">4</span>), <span class="dt">data =</span> Brains)
m5 &lt;-<span class="st"> </span><span class="kw">lm</span>(brain_size <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(body_mass, <span class="dv">5</span>), <span class="dt">data =</span> Brains)
m6 &lt;-<span class="st"> </span><span class="kw">lm</span>(brain_size <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(body_mass, <span class="dv">6</span>), <span class="dt">data =</span> Brains)</code></pre>
<p><code>poly(body_mass, k)</code> creates a degree <span class="math inline">\(k\)</span> polynomial in <span class="math inline">\(k\)</span> (but parameterized in
a special way that makes some kinds of statistical analysis easier – we
aren’t concerned with the particular parameterization here, just the overall
model fit).</p>
<p>And finally, here is a degree 0 polynomial (a constant).</p>
<pre class="sourceCode r"><code class="sourceCode r">m7 &lt;-<span class="st"> </span><span class="kw">lm</span>(brain_size <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> Brains)</code></pre>
</div>
<div id="leave-one-out-analysis" class="section level3">
<h3><span class="header-section-number">20.5.3</span> Leave One Out Analysis</h3>
<p>Here’s how you remove one row from a data set.</p>
<pre class="sourceCode r"><code class="sourceCode r">Brains.new &lt;-<span class="st"> </span>Brains[<span class="op">-</span><span class="dv">2</span>, ]</code></pre>
<p>One simple version of cross-validation is to fit the model several times,
but each time leaving out one observation (hence the name “leave one out”).
We can compare these models to each other to see how stable/volitile
the moel fits are and to see how well the “odd one out” is predicted
from the remaining observations.</p>
<pre class="sourceCode r"><code class="sourceCode r">leave_one_out &lt;-
<span class="st">  </span><span class="cf">function</span>(<span class="dt">index =</span> <span class="dv">1</span>, <span class="dt">degree =</span> <span class="dv">1</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="ot">NA</span>)) {
    pf &lt;-<span class="st"> </span><span class="kw">parent.frame</span>(<span class="dv">2</span>)
    <span class="cf">for</span>(i <span class="cf">in</span> index) {
      BrainsLOO &lt;-
<span class="st">        </span>Brains <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">out =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Brains) <span class="op">%in%</span><span class="st"> </span>i) 
      <span class="cf">for</span>(d <span class="cf">in</span> degree) {
        p &lt;-
<span class="st">          </span><span class="kw">gf_point</span>(
            brain_size <span class="op">~</span><span class="st"> </span>body_mass, <span class="dt">data =</span> BrainsLOO, <span class="dt">color =</span> <span class="op">~</span>out) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">          </span><span class="kw">gf_smooth</span>(
            <span class="dt">se =</span> <span class="ot">TRUE</span>, <span class="dt">fullrange =</span> <span class="ot">TRUE</span>,
            brain_size <span class="op">~</span><span class="st"> </span>body_mass, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, d),
            <span class="dt">data =</span> BrainsLOO[<span class="op">-</span>i, ], <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">          </span><span class="kw">gf_labs</span>(<span class="dt">title =</span> <span class="kw">paste</span>(<span class="st">&quot;removed:&quot;</span>, i, <span class="st">&quot; ;  degree =&quot;</span>, d)) <span class="op">%&gt;%</span>
<span class="st">          </span><span class="kw">gf_lims</span>(<span class="dt">y =</span> ylim) 
        <span class="kw">print</span>(p)
      }
    }
  }</code></pre>
<p>The simple linear model changes only slightly when we remove each data point
(although the model’s uncertainty decreases quite a bit when we remove the
data point that is least like the others).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">leave_one_out</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Brains), <span class="dt">degree =</span> <span class="dv">1</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2200</span>, <span class="dv">4000</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo-example-1-1.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-1-2.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-1-3.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-1-4.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-1-5.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-1-6.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-1-7.png" width="576" />
Cubic models and their uncertainties change more – they are more sensitive to the data.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">leave_one_out</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Brains), <span class="dt">degree =</span> <span class="dv">3</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2200</span>, <span class="dv">4000</span>))</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo-example-3-1.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-3-2.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-3-3.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-3-4.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-3-5.png" width="576" /></p>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_smooth).</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo-example-3-6.png" width="576" /><img src="Redoing_files/figure-html/ch20-loo-example-3-7.png" width="576" /></p>
<p>With a 5th degree polynomial (6 coefficients), the fit to the six data points
is “perfect”, but highly volitile.
The model has no uncertainty, but it is overfitting and overconfident.<br />
The fit to the omitted point might not be very reliable.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">leave_one_out</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Brains), <span class="dt">degree =</span> <span class="dv">5</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2200</span>, <span class="dv">4000</span>))</code></pre>
<pre><code>## Warning in qt((1 - level)/2, df): NaNs produced</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo-example-5-1.png" width="576" /></p>
<pre><code>## Warning in qt((1 - level)/2, df): NaNs produced</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo-example-5-2.png" width="576" /></p>
<pre><code>## Warning in qt((1 - level)/2, df): NaNs produced</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo-example-5-3.png" width="576" /></p>
<pre><code>## Warning in qt((1 - level)/2, df): NaNs produced</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo-example-5-4.png" width="576" /></p>
<pre><code>## Warning in qt((1 - level)/2, df): NaNs produced</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo-example-5-5.png" width="576" /></p>
<pre><code>## Warning in qt((1 - level)/2, df): NaNs produced</code></pre>
<pre><code>## Warning: Removed 10 rows containing missing values (geom_smooth).</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo-example-5-6.png" width="576" /></p>
<pre><code>## Warning in qt((1 - level)/2, df): NaNs produced</code></pre>
<p><img src="Redoing_files/figure-html/ch20-loo-example-5-7.png" width="576" /></p>
</div>
</div>
<div id="ch20-exercses" class="section level2">
<h2><span class="header-section-number">20.6</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>The <code>CalvinBayes::Seaweed</code> data set (adapted from <span class="citation">(Qian and Shen <a href="#ref-Qian:2007">2007</a>)</span>)
records how quickly seaweed regenerates when in the presence of different types
of grazers. Data were collected from eight different tidal areas of the Oregon
coast. We want to predict the amount of seaweed from the two predictors: grazer
type and tidal zone. The tidal zones are simply labeled A–H. The grazer type was
more involved, with six levels: No grazers (None), small fish only (f), small
and large fish (fF), limpets only (L), limpets and small fish (Lf), limpets and
small fish and large fish (LfF). We would like to know the effects of the
different types of grazers, and we would also like to know about the different
zones.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Create a plot that puts <code>SeaweedAmt</code> on the y-axis, <code>Grazer</code> on the
x-axis, and uses <code>Zone</code> for faceting. Use <code>gf_jitter()</code> to avoid overplotting.
Set <code>height</code> and <code>width</code> to appropriate values so the plot is still easily
interpretable.</p></li>
<li><p>Fit a model with both predictors and their interaction assuming
homogeneous variances in each group. Why would it not be a good idea
to fit a model with heterogenous variances?</p></li>
<li><p>What is the effect of small fish across all the zones? Answer
this question by setting up the following three contrasts:
none versus small fish only;
limpets only versus limpets and small fish;
the average of none and limpets only versus
the average of small fish only and limpets with small fish.
Discuss the results.</p></li>
<li><p>What is the effect of limpets?
There are several contrasts that can address this question,
but be sure to include a contrast that compares all of the locations
with limpets to all of the locations without limpets.</p></li>
<li><p>Set up a contrast to compare Zone A with Zone D. Briefly discuss the
result.</p></li>
<li><p>Does the effect of limpets depend on whether the location is in zone A or D?
Use an appropriate contrast to find out.</p></li>
</ol></li>
</ol>
<!-- Exercise 19.3. [Purpose: Use the heterogeneous-variance model to examine differences of scales across groups.]  -->
<!-- Note: added WAIC and LOO part and moved to this chapter. -->
<ol start="2" style="list-style-type: decimal">
<li><p>This problem investigates the synthetic data set <code>CalvinBayes::NonhomogVar</code>.</p>
<p><img src="Redoing_files/figure-html/ch20-prob2-plot-1.png" width="576" /></p>
<ol style="list-style-type: lower-alpha">
<li>From the plot, it is pretty clear that that variance is not the same
across the groups. Fit two models.</li>
</ol>
<pre class="sourceCode r"><code class="sourceCode r">model1_brm &lt;-<span class="st"> </span><span class="kw">brm</span>(Y <span class="op">~</span><span class="st"> </span>Group, <span class="dt">data =</span> NonhomogVar)
model2_brm &lt;-<span class="st"> </span><span class="kw">brm</span>(<span class="kw">bf</span>(Y <span class="op">~</span><span class="st"> </span>Group, sigma <span class="op">~</span><span class="st"> </span>Group), <span class="dt">data =</span> NonhomogVar)</code></pre>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>What is the difference between these two models? (That is, what does
<code>sigma ~ Group</code> do?)</p></li>
<li><p>Describe the priors for each model. (Use <code>prior_summary()</code> if you are
not sure.)</p></li>
<li><p>Create the following plots:</p>
<ol style="list-style-type: lower-roman">
<li><p>A plot showing the posterior distributions of
<span class="math inline">\(\sigma\)</span> from model 1 and each of the <span class="math inline">\(\sigma_j\)</span>’s from model 2.
(Stack multiple calls to <code>gf_dens()</code> on a single plot. Use
<code>color</code> or <code>linetype</code> or <code>size</code> or some combination of these to make
them distinguishable.
Note: <code>color = ~&quot;sigmaA&quot;</code>, etc will give you a nice legend.
This works for <code>linetype</code> and <code>size</code> as well.)</p></li>
<li><p>A plot showing the posterior distribution of <span class="math inline">\(\sigma_A - \sigma_B\)</span>
in model 2.</p></li>
<li><p>A plot showing the posterior distribution of <span class="math inline">\(\sigma_B - \sigma_C\)</span>
in model 2.</p></li>
<li><p>A plot showing the posterior distribution of
<span class="math inline">\(\frac{\sigma_A + \sigma_D}{2} - \frac{\sigma_B + \sigma_C}{2}\)</span> in model 2.</p></li>
</ol></li>
<li><p>Use each model to answer the following:</p>
<ol style="list-style-type: lower-roman">
<li>Are the means of groups A and B different?</li>
<li>Are the means of groups B and C different?</li>
</ol>
<p>Explain why the models agree or disagree.</p></li>
<li><p>The original plot suggests that model 2 should be preferred over model 1.
Compare the models using WAIC and LOOIC. Are these measures able to
detect that model 2 is better than model 1?</p></li>
</ol></li>
<li><p>Create a model like <code>fert4_brm</code> but use <code>family = student()</code>. This will
add a parameter to your model. According to WAIC and LOO, which model should
your prefer? (Note: the answer can be neither.)</p></li>
</ol>
<!-- 4. **What is lp__?** You may have noticed that Stan models include  -->
<!-- a posterior column labeled `lp__`. This is $\log(y \mid \theta)$, the log -->
<!-- likelihood "up to a constant". (Recall the algorithms used by Stan and JAGS -->
<!-- only need to know the likelihood up to a constant multiple or that log likelihood -->
<!-- up to an additive constant. Stan builds in some efficiency by not repeated  -->
<!-- computing things that only adjuste the log likelihood by an additive constant.) -->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Gelman:2014">
<p>Gelman, Andrew, Jessica Hwang, and Aki Vehtari. 2014. “Understanding Predictive Information Criteria for Bayesian Models.” <em>Statistics and Computing</em> 24 (6). Hingham, MA, USA: Kluwer Academic Publishers: 997–1016. <a href="https://doi.org/10.1007/s11222-013-9416-2">https://doi.org/10.1007/s11222-013-9416-2</a>.</p>
</div>
<div id="ref-Qian:2007">
<p>Qian, Song S., and Zehao Shen. 2007. “ECOLOGICAL Applications of Multilevel Analysis of Variance.” <em>Ecology</em> 88 (10): 2489–95. <a href="https://doi.org/10.1890/06-2041.1">https://doi.org/10.1890/06-2041.1</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>In this case, the posterior and the likelihood are the same, and
the noise distribution is normal.
The proportionality can be confirmed by observing that the “interesting part”
of <span class="math inline">\(\log p(y_i \mid \theta)\)</span> is <span class="math inline">\((y_i - \hat y_i)^2\)</span>.<a href="multiple-nominal-predictors.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Technically, we should be computing the average using an integral
instead of averaging over our posterior samples. But since this is a quantity
we can’t compute anyway, I’ve expressed this in terms of an average over
our posterior samples.<a href="multiple-nominal-predictors.html#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="nominal-predictors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dichotymous-response.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Redoing.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
