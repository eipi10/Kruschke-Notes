<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>2 Credibility, Models, and Parameters | (Re)Doing Bayesain Data Analysis</title>
  <meta name="description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="2 Credibility, Models, and Parameters | (Re)Doing Bayesain Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Credibility, Models, and Parameters | (Re)Doing Bayesain Data Analysis" />
  
  <meta name="twitter:description" content="Code, exercises and discussion to accompany a course taught from Kruschke’s Doing Bayesian Data Analysis (2ed)" />
  

<meta name="author" content="R Pruim">


<meta name="date" content="2019-02-23">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="some-useful-bits-of-r.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">(Re)Doing Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What’s in These Notes</a></li>
<li class="part"><span><b>I The Basics: Models, Probability, Bayes, and R</b></span></li>
<li class="chapter" data-level="2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html"><i class="fa fa-check"></i><b>2</b> Credibility, Models, and Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#the-steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.1</b> The Steps of Bayesian Data Analysis</a><ul>
<li class="chapter" data-level="2.1.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-code"><i class="fa fa-check"></i><b>2.1.1</b> R code</a></li>
<li class="chapter" data-level="2.1.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#r-packages"><i class="fa fa-check"></i><b>2.1.2</b> R packages</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-1-which-coin-is-it"><i class="fa fa-check"></i><b>2.2</b> Example 1: Which coin is it?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#freedom-of-choice"><i class="fa fa-check"></i><b>2.2.1</b> Freedom of choice</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#distributions"><i class="fa fa-check"></i><b>2.3</b> Distributions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#beta-distributions"><i class="fa fa-check"></i><b>2.3.1</b> Beta distributions</a></li>
<li class="chapter" data-level="2.3.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#normal-distributions"><i class="fa fa-check"></i><b>2.3.2</b> Normal distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#example-2-height-vs-weight"><i class="fa fa-check"></i><b>2.4</b> Example 2: Height vs Weight</a><ul>
<li class="chapter" data-level="2.4.1" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#data"><i class="fa fa-check"></i><b>2.4.1</b> Data</a></li>
<li class="chapter" data-level="2.4.2" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#describing-a-model-for-the-relationship-between-height-and-weight"><i class="fa fa-check"></i><b>2.4.2</b> Describing a model for the relationship between height and weight</a></li>
<li class="chapter" data-level="2.4.3" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#prior"><i class="fa fa-check"></i><b>2.4.3</b> Prior</a></li>
<li class="chapter" data-level="2.4.4" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior"><i class="fa fa-check"></i><b>2.4.4</b> Posterior</a></li>
<li class="chapter" data-level="2.4.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#posterior-predictive-check"><i class="fa fa-check"></i><b>2.4.5</b> Posterior Predictive Check</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#where-do-we-go-from-here"><i class="fa fa-check"></i><b>2.5</b> Where do we go from here?</a></li>
<li class="chapter" data-level="2.6" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#ch02-exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="credibility-models-and-parameters.html"><a href="credibility-models-and-parameters.html#footnotes"><i class="fa fa-check"></i><b>2.7</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html"><i class="fa fa-check"></i><b>3</b> Some Useful Bits of R</a><ul>
<li class="chapter" data-level="3.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#style-guide"><i class="fa fa-check"></i><b>3.1</b> You Gotta Have Style</a><ul>
<li class="chapter" data-level="3.1.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#an-additional-note-about-homework"><i class="fa fa-check"></i><b>3.1.1</b> An additional note about homework</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors-lists-and-data-frames"><i class="fa fa-check"></i><b>3.2</b> Vectors, Lists, and Data Frames</a><ul>
<li class="chapter" data-level="3.2.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#vectors"><i class="fa fa-check"></i><b>3.2.1</b> Vectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#lists"><i class="fa fa-check"></i><b>3.2.2</b> Lists</a></li>
<li class="chapter" data-level="3.2.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#data-frames-for-rectangular-data"><i class="fa fa-check"></i><b>3.2.3</b> Data frames for rectangular data</a></li>
<li class="chapter" data-level="3.2.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#other-types-of-data"><i class="fa fa-check"></i><b>3.2.4</b> Other types of data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#plotting-with-ggformula"><i class="fa fa-check"></i><b>3.3</b> Plotting with ggformula</a></li>
<li class="chapter" data-level="3.4" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#creating-data-with-expand.grid"><i class="fa fa-check"></i><b>3.4</b> Creating data with expand.grid()</a></li>
<li class="chapter" data-level="3.5" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#transforming-and-summarizing-data-dplyr-and-tidyr"><i class="fa fa-check"></i><b>3.5</b> Transforming and summarizing data dplyr and tidyr</a></li>
<li class="chapter" data-level="3.6" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#writing-functions"><i class="fa fa-check"></i><b>3.6</b> Writing Functions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#why-write-functions"><i class="fa fa-check"></i><b>3.6.1</b> Why write functions?</a></li>
<li class="chapter" data-level="3.6.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#function-parts"><i class="fa fa-check"></i><b>3.6.2</b> Function parts</a></li>
<li class="chapter" data-level="3.6.3" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#the-function-function-has-its-function"><i class="fa fa-check"></i><b>3.6.3</b> The function() function has its function</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#some-common-error-messages"><i class="fa fa-check"></i><b>3.7</b> Some common error messages</a><ul>
<li class="chapter" data-level="3.7.1" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#object-not-found"><i class="fa fa-check"></i><b>3.7.1</b> object not found</a></li>
<li class="chapter" data-level="3.7.2" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#any-message-mentioning-yaml"><i class="fa fa-check"></i><b>3.7.2</b> Any message mentioning yaml</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#ch03-exercises"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="some-useful-bits-of-r.html"><a href="some-useful-bits-of-r.html#footnotes-1"><i class="fa fa-check"></i><b>3.9</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#some-terminology"><i class="fa fa-check"></i><b>4.1</b> Some terminology</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>4.2</b> Distributions in R</a><ul>
<li class="chapter" data-level="4.2.1" data-path="probability.html"><a href="probability.html#example-normal-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Example: Normal distributions</a></li>
<li class="chapter" data-level="4.2.2" data-path="probability.html"><a href="probability.html#simulating-running-proportions"><i class="fa fa-check"></i><b>4.2.2</b> Simulating running proportions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#joint-marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>4.3</b> Joint, marginal, and conditional distributions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="probability.html"><a href="probability.html#example-hair-and-eye-color"><i class="fa fa-check"></i><b>4.3.1</b> Example: Hair and eye color</a></li>
<li class="chapter" data-level="4.3.2" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>4.3.2</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#ch04-exercises"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#footnotes-2"><i class="fa fa-check"></i><b>4.5</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html"><i class="fa fa-check"></i><b>5</b> Bayes’ Rule and the Grid Method</a><ul>
<li class="chapter" data-level="5.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#the-big-bayesian-idea"><i class="fa fa-check"></i><b>5.1</b> The Big Bayesian Idea</a><ul>
<li class="chapter" data-level="5.1.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#likelihood"><i class="fa fa-check"></i><b>5.1.1</b> Likelihood</a></li>
<li class="chapter" data-level="5.1.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#when-bayes-is-easy"><i class="fa fa-check"></i><b>5.1.2</b> When Bayes is easy</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#estimating-the-bias-in-a-coin-using-the-grid-method"><i class="fa fa-check"></i><b>5.2</b> Estimating the bias in a coin using the Grid Method</a><ul>
<li class="chapter" data-level="5.2.1" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#creating-a-grid"><i class="fa fa-check"></i><b>5.2.1</b> Creating a Grid</a></li>
<li class="chapter" data-level="5.2.2" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#hdi-from-the-grid"><i class="fa fa-check"></i><b>5.2.2</b> HDI from the grid</a></li>
<li class="chapter" data-level="5.2.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#automating-the-grid"><i class="fa fa-check"></i><b>5.2.3</b> Automating the grid</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#working-on-the-log-scale"><i class="fa fa-check"></i><b>5.3</b> Working on the log scale</a></li>
<li class="chapter" data-level="5.4" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#discrete-params"><i class="fa fa-check"></i><b>5.4</b> Discrete Parameters</a></li>
<li class="chapter" data-level="5.5" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#ch05-exercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
<li class="chapter" data-level="5.6" data-path="bayes-rule-and-the-grid-method.html"><a href="bayes-rule-and-the-grid-method.html#footnotes-3"><i class="fa fa-check"></i><b>5.6</b> Footnotes</a></li>
</ul></li>
<li class="part"><span><b>II Inferring a Binomial Probability</b></span></li>
<li class="chapter" data-level="6" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><i class="fa fa-check"></i><b>6</b> Inferring a Binomial Probability via Exact Mathematical Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-distributions-1"><i class="fa fa-check"></i><b>6.1</b> Beta distributions</a></li>
<li class="chapter" data-level="6.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta-and-bayes"><i class="fa fa-check"></i><b>6.2</b> Beta and Bayes</a><ul>
<li class="chapter" data-level="6.2.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#the-bernoulli-likelihood-function"><i class="fa fa-check"></i><b>6.2.1</b> The Bernoulli likelihood function</a></li>
<li class="chapter" data-level="6.2.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#a-convenient-prior"><i class="fa fa-check"></i><b>6.2.2</b> A convenient prior</a></li>
<li class="chapter" data-level="6.2.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#pros-and-cons-of-conjugate-priors"><i class="fa fa-check"></i><b>6.2.3</b> Pros and Cons of conjugate priors</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#getting-to-know-the-beta-distributions"><i class="fa fa-check"></i><b>6.3</b> Getting to know the Beta distributions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#important-facts"><i class="fa fa-check"></i><b>6.3.1</b> Important facts</a></li>
<li class="chapter" data-level="6.3.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#alternative-parameterizations-of-beta-distributions"><i class="fa fa-check"></i><b>6.3.2</b> Alternative parameterizations of Beta distributions</a></li>
<li class="chapter" data-level="6.3.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#beta_params"><i class="fa fa-check"></i><b>6.3.3</b> beta_params()</a></li>
<li class="chapter" data-level="6.3.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#automating-bayesian-updates-for-a-proportion-beta-prior"><i class="fa fa-check"></i><b>6.3.4</b> Automating Bayesian updates for a proportion (beta prior)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#what-if-the-prior-isnt-a-beta-distribution"><i class="fa fa-check"></i><b>6.4</b> What if the prior isn’t a beta distribution?</a></li>
<li class="chapter" data-level="6.5" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#ch06-exercises"><i class="fa fa-check"></i><b>6.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html"><i class="fa fa-check"></i><b>7</b> Markov Chain Monte Carlo (MCMC)</a><ul>
<li class="chapter" data-level="7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#king-markov-and-adviser-metropolis"><i class="fa fa-check"></i><b>7.1</b> King Markov and Adviser Metropolis</a></li>
<li class="chapter" data-level="7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#quick-intro-to-markov-chains"><i class="fa fa-check"></i><b>7.2</b> Quick Intro to Markov Chains</a><ul>
<li class="chapter" data-level="7.2.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#more-info-please"><i class="fa fa-check"></i><b>7.2.1</b> More info, please</a></li>
<li class="chapter" data-level="7.2.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#definition"><i class="fa fa-check"></i><b>7.2.2</b> Definition</a></li>
<li class="chapter" data-level="7.2.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#time-homogeneous-markov-chains"><i class="fa fa-check"></i><b>7.2.3</b> Time-Homogeneous Markov Chains</a></li>
<li class="chapter" data-level="7.2.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#matrix-representation"><i class="fa fa-check"></i><b>7.2.4</b> Matrix representation</a></li>
<li class="chapter" data-level="7.2.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#regular-markov-chains"><i class="fa fa-check"></i><b>7.2.5</b> Regular Markov Chains</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#back-to-king-markov"><i class="fa fa-check"></i><b>7.3</b> Back to King Markov</a></li>
<li class="chapter" data-level="7.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#how-well-does-the-metropolis-algorithm-work"><i class="fa fa-check"></i><b>7.4</b> How well does the Metropolis Algorithm work?</a><ul>
<li class="chapter" data-level="7.4.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-to-any-island"><i class="fa fa-check"></i><b>7.4.1</b> Jumping to any island</a></li>
<li class="chapter" data-level="7.4.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#jumping-only-to-neighbor-islands"><i class="fa fa-check"></i><b>7.4.2</b> Jumping only to neighbor islands</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#markov-chains-and-posterior-sampling"><i class="fa fa-check"></i><b>7.5</b> Markov Chains and Posterior Sampling</a><ul>
<li class="chapter" data-level="7.5.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#example-1-estimating-a-proportion"><i class="fa fa-check"></i><b>7.5.1</b> Example 1: Estimating a proportion</a></li>
<li class="chapter" data-level="7.5.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#example-2-estimating-mean-and-variance"><i class="fa fa-check"></i><b>7.5.2</b> Example 2: Estimating mean and variance</a></li>
<li class="chapter" data-level="7.5.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#issues-with-metropolis-algorithm"><i class="fa fa-check"></i><b>7.5.3</b> Issues with Metropolis Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#two-coins"><i class="fa fa-check"></i><b>7.6</b> Two coins</a><ul>
<li class="chapter" data-level="7.6.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#the-model"><i class="fa fa-check"></i><b>7.6.1</b> The model</a></li>
<li class="chapter" data-level="7.6.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#exact-analysis"><i class="fa fa-check"></i><b>7.6.2</b> Exact analysis</a></li>
<li class="chapter" data-level="7.6.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#metropolis"><i class="fa fa-check"></i><b>7.6.3</b> Metropolis</a></li>
<li class="chapter" data-level="7.6.4" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#gibbs-sampling"><i class="fa fa-check"></i><b>7.6.4</b> Gibbs sampling</a></li>
<li class="chapter" data-level="7.6.5" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#advantages-and-disadvantages-of-gibbs-vs-metropolis"><i class="fa fa-check"></i><b>7.6.5</b> Advantages and Disadvantages of Gibbs vs Metropolis</a></li>
<li class="chapter" data-level="7.6.6" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#so-what-do-we-learn-about-the-coins"><i class="fa fa-check"></i><b>7.6.6</b> So what do we learn about the coins?</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#mcmc-posterior-sampling-big-picture"><i class="fa fa-check"></i><b>7.7</b> MCMC posterior sampling: Big picture</a><ul>
<li class="chapter" data-level="7.7.1" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#mcmc-markov-chain-monte-carlo"><i class="fa fa-check"></i><b>7.7.1</b> MCMC = Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="7.7.2" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#posterior-sampling-random-walk-through-the-posterior"><i class="fa fa-check"></i><b>7.7.2</b> Posterior sampling: Random walk through the posterior</a></li>
<li class="chapter" data-level="7.7.3" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#where-do-we-go-from-here-1"><i class="fa fa-check"></i><b>7.7.3</b> Where do we go from here?</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="markov-chain-monte-carlo-mcmc.html"><a href="markov-chain-monte-carlo-mcmc.html#ch07-exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html"><i class="fa fa-check"></i><b>8</b> JAGS – Just Another Gibbs Sampler</a><ul>
<li class="chapter" data-level="8.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#what-jags-is"><i class="fa fa-check"></i><b>8.1</b> What JAGS is</a><ul>
<li class="chapter" data-level="8.1.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#updating-c-and-clang"><i class="fa fa-check"></i><b>8.1.1</b> Updating C and CLANG</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-1-estimating-a-proportion-1"><i class="fa fa-check"></i><b>8.2</b> Example 1: estimating a proportion</a><ul>
<li class="chapter" data-level="8.2.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-1"><i class="fa fa-check"></i><b>8.2.1</b> The Model</a></li>
<li class="chapter" data-level="8.2.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#load-data"><i class="fa fa-check"></i><b>8.2.2</b> Load Data</a></li>
<li class="chapter" data-level="8.2.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#specify-the-model"><i class="fa fa-check"></i><b>8.2.3</b> Specify the model</a></li>
<li class="chapter" data-level="8.2.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#run-the-model"><i class="fa fa-check"></i><b>8.2.4</b> Run the model</a></li>
<li class="chapter" data-level="8.2.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-coda"><i class="fa fa-check"></i><b>8.2.5</b> Using coda</a></li>
<li class="chapter" data-level="8.2.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-bayesplot"><i class="fa fa-check"></i><b>8.2.6</b> Using bayesplot</a></li>
<li class="chapter" data-level="8.2.7" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#using-kruschkes-functions"><i class="fa fa-check"></i><b>8.2.7</b> Using Kruschke’s functions</a></li>
<li class="chapter" data-level="8.2.8" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#optional-arguments-to-jags"><i class="fa fa-check"></i><b>8.2.8</b> Optional arguments to jags()</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#example-2-comparing-two-proportions"><i class="fa fa-check"></i><b>8.3</b> Example 2: comparing two proportions</a><ul>
<li class="chapter" data-level="8.3.1" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-data"><i class="fa fa-check"></i><b>8.3.1</b> The data</a></li>
<li class="chapter" data-level="8.3.2" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#the-model-2"><i class="fa fa-check"></i><b>8.3.2</b> The model</a></li>
<li class="chapter" data-level="8.3.3" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#describing-the-model-to-jags"><i class="fa fa-check"></i><b>8.3.3</b> Describing the model to JAGS</a></li>
<li class="chapter" data-level="8.3.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#fitting-the-model"><i class="fa fa-check"></i><b>8.3.4</b> Fitting the model</a></li>
<li class="chapter" data-level="8.3.5" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#inspecting-the-results"><i class="fa fa-check"></i><b>8.3.5</b> Inspecting the results</a></li>
<li class="chapter" data-level="8.3.6" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#difference-in-proportions"><i class="fa fa-check"></i><b>8.3.6</b> Difference in proportions</a></li>
<li class="chapter" data-level="8.3.7" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#sampling-from-the-prior"><i class="fa fa-check"></i><b>8.3.7</b> Sampling from the prior</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="jags-just-another-gibbs-sampler.html"><a href="jags-just-another-gibbs-sampler.html#ch08-exercises"><i class="fa fa-check"></i><b>8.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="heierarchical-models.html"><a href="heierarchical-models.html"><i class="fa fa-check"></i><b>9</b> Heierarchical Models</a><ul>
<li class="chapter" data-level="9.1" data-path="heierarchical-models.html"><a href="heierarchical-models.html#one-coin-from-one-mint"><i class="fa fa-check"></i><b>9.1</b> One coin from one mint</a></li>
<li class="chapter" data-level="9.2" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-one-mint"><i class="fa fa-check"></i><b>9.2</b> Multiple coins from one mint</a></li>
<li class="chapter" data-level="9.3" data-path="heierarchical-models.html"><a href="heierarchical-models.html#multiple-coins-from-multiple-mints"><i class="fa fa-check"></i><b>9.3</b> Multiple coins from multiple mints</a></li>
<li class="chapter" data-level="9.4" data-path="heierarchical-models.html"><a href="heierarchical-models.html#ch09-exercises"><i class="fa fa-check"></i><b>9.4</b> Exerciess</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">(Re)Doing Bayesain Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="credibility-models-and-parameters" class="section level1">
<h1><span class="header-section-number">2</span> Credibility, Models, and Parameters</h1>
<div id="the-steps-of-bayesian-data-analysis" class="section level2">
<h2><span class="header-section-number">2.1</span> The Steps of Bayesian Data Analysis</h2>
<p>In general, Bayesian analysis of data follows these steps:</p>
<ol style="list-style-type: decimal">
<li><p>Identify the <strong>data</strong> relevant to the research questions.</p>
<p>What are the measurement scales of the data?
Which data variables are to be predicted, and which data variables are
supposed to act as predictors?</p></li>
<li><p>Define a <strong>descriptive model for the</strong> relevant <strong>data</strong>. The mathematical form and its
parameters should be meaningful and appropriate to the theoretical purposes of the
analysis.</p></li>
<li><p>Specify a <strong>prior distribution</strong> on the parameters. The prior must pass muster with the
audience of the analysis, such as skeptical scientists.</p></li>
<li><p>Use Bayesian inference to <strong>re-allocate credibility across parameter values</strong>. Interpret
the posterior distribution with respect to theoretically meaningful issues (assuming
that the model is a reasonable description of the data; see next step).</p></li>
<li><p>Check that the posterior predictions mimic the data with reasonable
accuracy (i.e., conduct a “<strong>posterior predictive check</strong>”).
If not, then consider a different descriptive model.</p></li>
</ol>
<p>In this chapter we will focus on two examples so we can get an overview of
what Bayesian data analysis looks like. In subsequent chapters we will fill
in lots of the missing details.</p>
<div id="r-code" class="section level3">
<h3><span class="header-section-number">2.1.1</span> R code</h3>
<p>Some of the R code used in this chapter has been hidden, and some of it is visible.
In any case the point of this chapter is not to understand the details of the R
code. It is there mainly for those of you who are curious, or because you might
come back and look at this chapter later in the semester.</p>
<p>For those of you new to R, we will be learning it as we go along. For those of you
who have used R before, some of this will be familiar to you, but other things
likely will not be familiar.</p>
</div>
<div id="r-packages" class="section level3">
<h3><span class="header-section-number">2.1.2</span> R packages</h3>
<p>We will make use of a number of R packages as we go along. Here is the code
used to load the packages used in this chapter. If you try to mimic the
code on your own machine, you will need to use these packages.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggformula)       <span class="co"># for creating plots</span>
<span class="kw">theme_set</span>(<span class="kw">theme_bw</span>())    <span class="co"># change the default graphics settings</span>
<span class="kw">library</span>(dplyr)           <span class="co"># for data wrangling</span>
<span class="kw">library</span>(mosaic)          <span class="co"># includes the previous 2 (and some other stuff)</span>
<span class="kw">library</span>(CalvinBayes)     <span class="co"># includes BernGrid() </span>
<span class="kw">library</span>(brms)            <span class="co"># used to fit the model in the second exmample, </span>
                         <span class="co"># but hidden from view here</span></code></pre>
</div>
</div>
<div id="example-1-which-coin-is-it" class="section level2">
<h2><span class="header-section-number">2.2</span> Example 1: Which coin is it?</h2>
<p>As first simple illustration of the big ideas of Bayesian inference, let’s
consider a situation where we have a coin that is known to result in heads
in either 0, 20, 40, 60, 80, or 100% of tosses. But we don’t know which.
Our plan is to gather data by flipping the coin and recording the results.
If we let <span class="math inline">\(\theta\)</span> be the true probability of tossing a head,
we can refer to these 5 possibilities as
<span class="math inline">\(\theta = 0\)</span>, <span class="math inline">\(\theta = 0.2\)</span>, <span class="math inline">\(\theta = 0.4\)</span>, <span class="math inline">\(\theta = 0.6\)</span>,
<span class="math inline">\(\theta = 0.8\)</span>, and <span class="math inline">\(\theta = 1\)</span>.</p>
<p>Before collecting our data, if have no other information, we will consider
each coin to be equally credible. We could represent that as follows.</p>
<p><img src="Redoing_files/figure-html/ch02-coin-cred-01-1.png" width="65%" /></p>
<p>Now suppose we toss the coin and obtain a head. What does that do to our
credibilities? Clearly <span class="math inline">\(\theta = 0\)</span> is no longer possible. So the
credibility of that option becomes 0. The other credibilities are adjusted
as well. We will see later just how, but the following should be intuitive:</p>
<ul>
<li>the options with larger values of <span class="math inline">\(\theta\)</span> should increase in credibility
more than those with lower values of <span class="math inline">\(\theta\)</span>.</li>
<li>the total credibility of all options should remain 1 (100%).</li>
</ul>
<p>In fact, the adjusted credibility after one head toss looks like this:</p>
<p><img src="Redoing_files/figure-html/ch02-coins-cred-02-1.png" width="65%" /></p>
<p>This updating of credibility of possible values of <span class="math inline">\(\theta\)</span> is the key idea
in Bayesian inference. Bayesians don’t call these distributions of
credibility “before” and “after”, however. Instead they use the longer words
“prior” and “posterior”, which mean the same thing.</p>
<p><img src="Redoing_files/figure-html/ch02-coins-cred-03-1.png" width="65%" /></p>
<p>Now suppose we toss the coin again and get another head. Once again we can
update the credibility, and once again, the larger values of <span class="math inline">\(\theta\)</span> will
see their credibility increase while the smaller values of <span class="math inline">\(\theta\)</span> will
see their credibility decrease.</p>
<p><img src="Redoing_files/figure-html/ch02-coins-cred-04-1.png" width="65%" /></p>
<p>Time for a third toss. This time we obtain a tail. Now the credibility
of <span class="math inline">\(\theta = 1\)</span> drops to 0, and the relative credibilities of the
smaller values of <span class="math inline">\(\theta\)</span> will increase and of the larger values of <span class="math inline">\(\theta\)</span>
will decrease.</p>
<p><img src="Redoing_files/figure-html/ch02-coins-cred-05-1.png" width="65%" />
Finally, we flip one more tail.</p>
<p><img src="Redoing_files/figure-html/ch02-coins-cred-06-1.png" width="65%" /></p>
<p>As expected, the posterior is now symmetric with the two central values
of <span class="math inline">\(\theta\)</span> having the larger credibility.</p>
<p>We can keep playing this game as long as we like. Each coin toss provides
a bit more information with which to update the posterior, which becomes our
new prior for subsequent data. The <code>BernGrid()</code> function in the <code>CalvinBayes</code>
package makes it easy to generate plots similar to the ones above. <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">BernGrid</span>(<span class="st">&quot;HHTTTHTTT&quot;</span>,                         <span class="co"># the data</span>
            <span class="dt">steps =</span> <span class="ot">TRUE</span>,                     <span class="co"># show each step</span>
            <span class="dt">p =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="dv">1</span>))  <span class="co"># possible probabilities</span></code></pre>
<pre><code>## Converting data to 1, 1, 0, 0, 0, 1, 0, 0, 0</code></pre>
<p><img src="Redoing_files/figure-html/ch02-coins-cred-07-1.png" width="65%" /></p>
<div id="freedom-of-choice" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Freedom of choice</h3>
<p>In practice, we are usually not given a small number of possible values
for the probability (of obtaining heads in our example, but it could
be any probability). Instead, the probability could
be any value between 0 and 1. But we can do Bayesian updating in
essentially the same way. Instead of a bar chart, we will use a
line graph (called a density plot) to show how the credibility depends
on the parameter value.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">BernGrid</span>(<span class="st">&quot;HHTTTHTTT&quot;</span>,                      <span class="co"># the data</span>
            <span class="dt">steps =</span> <span class="ot">TRUE</span>)                  <span class="co"># show each step</span></code></pre>
<pre><code>## Converting data to 1, 1, 0, 0, 0, 1, 0, 0, 0</code></pre>
<p><img src="Redoing_files/figure-html/ch02-coins-cred-08-1.png" width="65%" /></p>
</div>
</div>
<div id="distributions" class="section level2">
<h2><span class="header-section-number">2.3</span> Distributions</h2>
<p>The (prior and posterior) distributions in the previous plots were
calculated numerically using a Bayesian update rule that we will soon learn.
Density functions have the properties that
* they are never negative, and
* the total area under the curve is 1.
Where the density curve is taller, values are more likely. So in the last
posterior credibility above, we see that values near 1/3 are the most credible
while values below 0.015 or above 0.065 are not very credible. In particular,
we still can’t discount the possibility that we are dealing with a fair coin
since 0.5 lies well within the most credible central portion of the plot.</p>
<p>We will also encounter densities with names like “normal”, “beta”, and “t”.
The <code>gf_dist()</code> function from <code>ggformula</code> can be used to plot distributions.
We just need to provide R’s version of the name for the family and any
required parameter values.</p>
<div id="beta-distributions" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Beta distributions</h3>
<p>The curves in our coins example above look a lot like beta distributions.
In fact, we will eventually learn that they are beta distributions, and that
each new observed coin toss increases either <code>shape1</code> or <code>shape2</code> by 1.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="dv">1</span>, <span class="dt">shape2 =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;gray50&quot;</span>) <span class="op">%&gt;%</span>
<span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">%&gt;%</span>
<span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="dv">3</span>, <span class="dt">shape2 =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;orange&quot;</span>) <span class="op">%&gt;%</span>
<span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="dv">3</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;forestgreen&quot;</span>) <span class="op">%&gt;%</span>
<span class="kw">gf_dist</span>(<span class="st">&quot;beta&quot;</span>, <span class="dt">shape1 =</span> <span class="dv">3</span>, <span class="dt">shape2 =</span> <span class="dv">3</span>, <span class="dt">color =</span> <span class="st">&quot;navy&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch02-beta-01-1.png" width="65%" /></p>
</div>
<div id="normal-distributions" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Normal distributions</h3>
<p>Another important family of distributions is the normal family.
These are bell-shaped, symmetric distributions centered at the mean (<span class="math inline">\(\mu\)</span>).
A second parameter, the standard deviation (<span class="math inline">\(\sigma\)</span>) quantifies how spread
out the distribution is.</p>
<p>To plot a normal distribution with mean 10 and standard deviation 1 or 2,
we use</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dist</span>(<span class="st">&quot;norm&quot;</span>, <span class="dt">mean =</span> <span class="dv">10</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">%&gt;%</span>
<span class="kw">gf_dist</span>(<span class="st">&quot;norm&quot;</span>, <span class="dt">mean =</span> <span class="dv">10</span>, <span class="dt">sd =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch02-norm-01-1.png" width="65%" /></p>
<p>The red curve is “twice as spread out” as the blue one.</p>
<p>We can also draw random samples from distributions. Random samples will not
exactly follow the shape of the distribution they were drawn from, so it
takes some experience to get calibrated to know when things are “close enough”
to consider a proposed distribution to be believable, and when they are
“different enough” to be skeptical. Generating some random data and
comparing to the theoretical distribution can help us calibrate.</p>
<p>In the example below, we generate 25 random samples of size 100 and compare
their (density) histograms to the theoretical Norm(10, 2) distribution. <code>expand.grid()</code>
produces a data frame with two columns containing every combination of
the numbers 1 through 100 with the numbers 1 through 25, for a total of
2500 rows.
<code>mutate()</code> is used to add a new variable to the data frame.</p>
<pre class="sourceCode r"><code class="sourceCode r">Rdata &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">expand.grid</span>(
    <span class="dt">rep =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>,
    <span class="dt">sample =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">25</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dv">2500</span>, <span class="dt">mean =</span> <span class="dv">10</span>, <span class="dt">sd =</span> <span class="dv">2</span>)
  )
<span class="kw">head</span>(Rdata)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">rep</th>
<th align="right">sample</th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">11.171</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1</td>
<td align="right">11.419</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">9.781</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">9.093</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">1</td>
<td align="right">11.212</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">1</td>
<td align="right">6.364</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_dhistogram</span>( <span class="op">~</span><span class="st"> </span>x <span class="op">|</span><span class="st"> </span>sample, <span class="dt">data =</span> Rdata, 
              <span class="dt">color =</span> <span class="st">&quot;gray30&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_dist</span>(<span class="st">&quot;norm&quot;</span>, <span class="dt">mean =</span> <span class="dv">10</span>, <span class="dt">sd =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch02-norm-calibration-plot-1.png" width="65%" /></p>
<p>We will see many other uses of these functions. See the next chapter for
in introduction to R functions that will be useful.</p>
</div>
</div>
<div id="example-2-height-vs-weight" class="section level2">
<h2><span class="header-section-number">2.4</span> Example 2: Height vs Weight</h2>
<p>The coins example above is overly simple compared to typical applications.
Before getting to the nuts and bolts of doing Bayesian data analysis,
let’s look at a somewhat more realistic example. Suppose we want to model the
relationship between weight and height in 40-year-old Americans.</p>
<!-- *Note: The notation in this example in Kruschke's book is not as sharp as I would prefer.* -->
<div id="data" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Data</h3>
<p>Here’s a scatter plot of some data from the NHANES study that we will use
for this example. (Note: this is not the same data set used in the book.
The data here come from the <code>NHANES::NHANES</code> data set.)</p>
<p><img src="Redoing_files/figure-html/ch02-nhanes-02-1.png" width="65%" /></p>
</div>
<div id="describing-a-model-for-the-relationship-between-height-and-weight" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Describing a model for the relationship between height and weight</h3>
<p>A plausible model is that weight is linearly related to height.
We will make this model a bit more precise by defining the
<strong>model parameters</strong> and distributions involved.</p>
<p>Typically statisticians use Greek letters to represent parameters. This model
has three parameters (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma\)</span>) and makes two claims</p>
<ol style="list-style-type: decimal">
<li>The <strong>average weight</strong> of people with height <span class="math inline">\(x\)</span> is <span class="math inline">\(\beta_0 + \beta_1 x\)</span>
(some linear function of <span class="math inline">\(x\)</span>). We can express this as</li>
</ol>
<p><span class="math display">\[
\mu_{Y|x} = E(Y \mid x) =  \beta_0 + \beta_1 x 
\]</span>
or
<span class="math display">\[
\mu_{\mbox{weight}|\mbox{height}} = E(\mbox{weight} \mid \mbox{height}) =  
    \beta_0 + \beta_1 \cdot \mbox{height}
\]</span>
The <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span> notation is useful for general formulas;
for specific problems (especially in R code), it
is usually better to use descriptive names for the variables.
The capital <span class="math inline">\(Y\)</span> indicates that it has a distribution. <span class="math inline">\(x\)</span> is lower
case because we are imagining a specific value there. So for each
value of <span class="math inline">\(x\)</span>, the there is a distribution of <span class="math inline">\(Y\)</span>’s.</p>
<ol start="2" style="list-style-type: decimal">
<li>But <strong>not everyone is average</strong>.
The model used here assumes that the distributions
of the heights of people with a given weight are symmetrically distributed
around the average weight for that height and that the distribution is
normal (bell-shaped). The parameter <span class="math inline">\(\sigma\)</span> is called the standard deviation
and measures the amount of variability. If <span class="math inline">\(\sigma\)</span> is small, then most people’s
weights are very close to the average for their height. If <span class="math inline">\(\sigma\)</span> is larger, then
there is more variability in weights for people who have the same height.
We express this as
<span class="math display">\[\begin{align}
y \mid x \sim {\sf Norm}(\mu_{y|x}, \sigma)
\end{align}\]</span>
Notice the <span class="math inline">\(\sim\)</span> in this expression. It is read “is distributed as” and
describes the distribution (shape) of some quantity.</li>
</ol>
<p>Putting this all together, and being a little bit sloppy we might write it
this way:</p>
<p><span class="math display">\[\begin{align}
    Y &amp;\sim {\sf Norm}(\mu, \sigma) \\
    \mu &amp; \sim \beta_0 + \beta_1 x
\end{align}\]</span></p>
<p>In this style the dependence of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span> is implicit (via <span class="math inline">\(\mu\)</span>’s dependence on
<span class="math inline">\(x\)</span>) and we save writing <span class="math inline">\(\mid x\)</span> in a few places.</p>
</div>
<div id="prior" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Prior</h3>
<p>A prior distribution describes what is known/believed about the parameters before
we use the information from our data. This could be informed by previous data,
or it may be a fairly uninformative prior that considers many values of the
parameter to be credible. For this example, we use very flat broad priors
(centered at 0 for the <span class="math inline">\(\beta\)</span>’s and extending from 0 to a very large
number of <span class="math inline">\(\sigma\)</span>. (We know that <span class="math inline">\(\sigma &gt; 0\)</span>, so our prior should reflect
that knowledge.)</p>
</div>
<div id="posterior" class="section level3">
<h3><span class="header-section-number">2.4.4</span> Posterior</h3>
<p>The posterior distribution is calculated by combining the information about the
model (via the <strong>likelihood function</strong>) with the prior. The posterior will
provide updated distributions for <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma\)</span>. These
distributions will be narrow if our data give a strong evidence about their
values and wider if even after considering the data, there is still considerable
uncertainty about the parameter values.</p>
<p>For now we won’t worry about how the posterior distribution is computed, but
we can inspect it visually. (It is called <code>Post</code> in the R code below.)
For example, if we are primarily interested in
the slope (how much heavier are people on average for each inch they are taller?),
we can plot the posterior distribution of <span class="math inline">\(\beta_1\)</span> or calculate its mean,
or the region containing the central 95% of the distribution.
Such a region is called a <strong>highest density interval</strong> (HDI)
(sometimes called the highest posterior density interval (HPDI), to emphasize
that we are looking at a posterior distribution, but an HDI can be computed for
other distributions as well).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_density</span>( <span class="op">~</span><span class="st"> </span>b_height, <span class="dt">data =</span> Post, <span class="dt">alpha =</span> <span class="fl">0.5</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch02-nhanes-post-01-1.png" width="65%" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="op">~</span><span class="st"> </span>b_height, <span class="dt">data =</span> Post)</code></pre>
<pre><code>## [1] 7.223</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hdi</span>(Post, <span class="dt">pars =</span> <span class="st">&quot;b_height&quot;</span>)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">par</th>
<th align="right">lo</th>
<th align="right">hi</th>
<th align="right">prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">b_height</td>
<td align="right">5.551</td>
<td align="right">9.045</td>
<td align="right">0.95</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_areas</span>(<span class="kw">as.mcmc</span>(Post), <span class="dt">pars =</span> <span class="st">&quot;b_height&quot;</span>, <span class="dt">prob =</span> <span class="fl">0.95</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch02-nhanes-post-01-2.png" width="65%" /></p>
<p>Although we don’t get a very precise estimate of <span class="math inline">\(\beta_1\)</span> from this model/data
combination, we can be quite confident that taller people are indeed
heavier (on average), somewhere between 5 and 10 pounds heavier per inch taller.</p>
<p>Another interesting plot shows lines overlaid on the scatter plot.
Each line represents a plausible (according to the posterior distribution)
combination of slope and intercept. 100 such lines are included in the plot
below.</p>
<p><img src="Redoing_files/figure-html/ch02-nhanes-lines-01-1.png" width="65%" /><img src="Redoing_files/figure-html/ch02-nhanes-lines-01-2.png" width="65%" /></p>
</div>
<div id="posterior-predictive-check" class="section level3">
<h3><span class="header-section-number">2.4.5</span> Posterior Predictive Check</h3>
<p>Notice that only a few of the dots are covered by the blue lines. That’s because
the blue lines represent plausible <em>average</em> weights. But the model takes
into account that some people may be quite a bit heavier or lighter than average.
A posterior predictive check is a way of checking that the data look like
they could have been plausibly generated by our model.</p>
<p>We can generate a simulated weight for a given height
by randomly selecting values of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\sigma\)</span>
so that the more credible values are more likely to be selected,
and using the normal distribution to generate a difference between
an individual weight and the average weight (as determined by the
parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
For example, here are the first two rows of our posterior distribution:</p>
<pre class="sourceCode r"><code class="sourceCode r">Post <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>(<span class="dv">2</span>)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">b_Intercept</th>
<th align="right">b_height</th>
<th align="right">sigma</th>
<th align="right">lp__</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">-217.0</td>
<td align="right">6.028</td>
<td align="right">40.11</td>
<td align="right">-784.5</td>
</tr>
<tr class="even">
<td align="right">-422.1</td>
<td align="right">9.030</td>
<td align="right">48.28</td>
<td align="right">-786.5</td>
</tr>
</tbody>
</table>
<p>To simulate a weight for a height of 65 inches based on the fist row,
we could take a random
draw from a <span class="math inline">\({\sf Norm}(-217 + 6.028 \cdot 65, 40.11)\)</span> distribution.
We can do a similar thing for the second row.</p>
<pre class="sourceCode r"><code class="sourceCode r">Post <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>(<span class="dv">2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred_y =</span> <span class="kw">rnorm</span>(<span class="dv">2</span>, <span class="dt">mean =</span> b_Intercept <span class="op">+</span><span class="st"> </span>b_height <span class="op">*</span><span class="st"> </span><span class="dv">65</span>, <span class="dt">sd =</span> sigma))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">b_Intercept</th>
<th align="right">b_height</th>
<th align="right">sigma</th>
<th align="right">lp__</th>
<th align="right">pred_y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">-217.0</td>
<td align="right">6.028</td>
<td align="right">40.11</td>
<td align="right">-784.5</td>
<td align="right">185.3</td>
</tr>
<tr class="even">
<td align="right">-422.1</td>
<td align="right">9.030</td>
<td align="right">48.28</td>
<td align="right">-786.5</td>
<td align="right">219.4</td>
</tr>
</tbody>
</table>
<p>Those values are quite different. This is because credible values of
<span class="math inline">\(\sigma\)</span> are quite large – an indication that individuals will vary quite
substantially from the average weight for their height.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gf_density</span>( <span class="op">~</span><span class="st"> </span>sigma, <span class="dt">data =</span> Post)</code></pre>
<p><img src="Redoing_files/figure-html/ch02-nhanes-post-05-1.png" width="65%" /></p>
<p>We should not be surprised to see some (~ 5%) of people who are 85 pounds above
or below the average weight for their height.</p>
<p>If we do this many times for several height values and plot the central
95% of the weights, we get a plot that looks like this:</p>
<pre class="sourceCode r"><code class="sourceCode r">PPC &lt;-
<span class="st">  </span><span class="kw">expand.grid</span>(
    <span class="dt">height =</span> <span class="kw">seq</span>(<span class="dv">56</span>, <span class="dv">76</span>, <span class="dt">by =</span> <span class="dv">1</span>),
    <span class="dt">rep =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Post)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">b_Intercept =</span> Post<span class="op">$</span>b_Intercept[rep],
    <span class="dt">b_height =</span> Post<span class="op">$</span>b_height[rep],
    <span class="dt">sigma =</span> Post<span class="op">$</span>sigma[rep],
    <span class="dt">weight =</span> b_Intercept <span class="op">+</span><span class="st"> </span>b_height <span class="op">*</span><span class="st"> </span>height <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="kw">n</span>(), <span class="dv">0</span>, sigma)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(height) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(
    <span class="dt">mean =</span> <span class="kw">mean</span>(weight),
    <span class="dt">lo =</span> <span class="kw">quantile</span>(weight, <span class="dt">prob =</span> <span class="fl">0.025</span>),
    <span class="dt">hi =</span> <span class="kw">quantile</span>(weight, <span class="dt">prob =</span> <span class="fl">0.975</span>)
  )

<span class="kw">gf_point</span>(weight <span class="op">~</span><span class="st"> </span>height, <span class="dt">data =</span> NHANES40, <span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gf_pointrange</span>(mean <span class="op">+</span><span class="st"> </span>lo <span class="op">+</span><span class="st"> </span>hi <span class="op">~</span><span class="st"> </span>height, <span class="dt">data =</span> PPC, <span class="dt">alpha =</span> <span class="fl">0.7</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>)</code></pre>
<p><img src="Redoing_files/figure-html/ch02-nhanes-post-06-1.png" width="65%" /></p>
<p>Now we see that indeed, most (but not all) of the data points fall
within a range that the model believes is credible. If this were not the
case, it would be evidence that our model is not well aligned with the data
and might lead us to explore other models.</p>
<p>Notice that by taking many different credible values of the parameters
(including <span class="math inline">\(\sigma\)</span>), we are
taking into account both our uncertainty about the parameter values and
the variability that the model describes in the population (even for given
parameter values).</p>
</div>
</div>
<div id="where-do-we-go-from-here" class="section level2">
<h2><span class="header-section-number">2.5</span> Where do we go from here?</h2>
<p>Now that we have seen an overview of Bayesian inference at work, you probably have
lots of questions. Of time we will improve our answers to each of them.</p>
<ol style="list-style-type: decimal">
<li><p>How do we create models?</p>
<p>One of the nice things about Bayesian inference is that it is so flexible.
That allows us to create all sorts of models. We will begin with models
of a proportion (and how that proportion might depend on other variables)
because these are the simplest to understand. Then we will move on other
important examples. (The back half of our book is a smorgasbord of example
situations.)</p></li>
<li><p>How do we select priors?</p>
<p>We will begin with fairly “uninformative” priors that say very little, and
we will experiment with different priors to see what affect the choice of prior
has on our analysis. Gradually we will learn more about prior selection.</p></li>
<li><p>How do we update the prior based on data to get the posterior?</p>
<p>Here we will learn several approaches, most of them computational. (There
are only a limited number of examples where the prior can be computed
analytically.) We will start with computational methods that are simple
to implement and relatively easy to understand, but are too inefficient
to use on large or complex problems. Eventually we will learn how to use
two important algorithms (JAGS and Stan) to describe and fit Bayesian models.</p></li>
<li><p>How do we tell whether the algorithm that generated the posterior worked well?</p>
<p>The computainal algorithms that compute posterior distributions can fail. No
one algorithm works best on every problem, and sometimes we need to describe
our model differently to help the computer. We will learn some diagnostics
to help us detect when there may be problems with our computations.</p></li>
<li><p>What can we do with the posterior once we have it?</p>
<p>After all the work of building a model, selectig a prior, fitting the model
to obtain a posterior, and convincing ourselves that no disasters have happened
along the way, what can we do with the posterior? We will use it both to
diagnose the model itself and to see what the model has to say.</p></li>
</ol>
</div>
<div id="ch02-exercises" class="section level2">
<h2><span class="header-section-number">2.6</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li>Consider Figure 2.6 on page 29 of <em>DBDA2E</em>.
Two of the data points fall above the vertical bars.
Does this mean that the model does not describe the data well?
Briefly explain your answer.</li>
</ol>
<!-- The next several are similar to Exercise 5.4. 
[Purpose: To gain intuition about Bayesian updating by using BernGrid().]  -->
<ol start="2" style="list-style-type: decimal">
<li><p>Run the following examples in R. Compare the plots produced
and comment the big idea(s) illustrated by this comparison.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(CalvinBayes)
<span class="kw">BernGrid</span>(<span class="st">&quot;H&quot;</span>, <span class="dt">resolution =</span> <span class="dv">4</span>,  <span class="dt">prior =</span> triangle<span class="op">::</span>dtriangle)
<span class="kw">BernGrid</span>(<span class="st">&quot;H&quot;</span>, <span class="dt">resolution =</span> <span class="dv">10</span>, <span class="dt">prior =</span> triangle<span class="op">::</span>dtriangle)
<span class="kw">BernGrid</span>(<span class="st">&quot;H&quot;</span>, <span class="dt">prior =</span> <span class="dv">1</span>, <span class="dt">resolution =</span> <span class="dv">100</span>, <span class="dt">geom =</span> geom_col)
<span class="kw">BernGrid</span>(<span class="st">&quot;H&quot;</span>, <span class="dt">resolution =</span> <span class="dv">100</span>,
         <span class="dt">prior =</span> <span class="cf">function</span>(p) <span class="kw">abs</span>(p <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.48</span>, <span class="dt">geom =</span> geom_col) </code></pre></li>
<li><p>Run the following examples in R. Compare the plots produced
and comment the big idea(s) illustrated by this comparison.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(CalvinBayes)
<span class="kw">BernGrid</span>(<span class="st">&quot;TTHT&quot;</span>, <span class="dt">prior =</span> triangle<span class="op">::</span>dtriangle)
<span class="kw">BernGrid</span>(<span class="st">&quot;TTHT&quot;</span>,
         <span class="dt">prior =</span> <span class="cf">function</span>(x) triangle<span class="op">::</span><span class="kw">dtriangle</span>(x)<span class="op">^</span><span class="fl">0.1</span>)
<span class="kw">BernGrid</span>(<span class="st">&quot;TTHT&quot;</span>,
         <span class="dt">prior =</span> <span class="cf">function</span>(x) triangle<span class="op">::</span><span class="kw">dtriangle</span>(x)<span class="op">^</span><span class="dv">10</span>)</code></pre></li>
<li><p>Run the following examples in R. Compare the plots produced
and comment the big idea(s) illustrated by this comparison.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(CalvinBayes)
dfoo &lt;-<span class="st"> </span><span class="cf">function</span>(p) {
  <span class="fl">0.02</span> <span class="op">*</span><span class="st"> </span><span class="kw">dunif</span>(p) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="fl">0.49</span> <span class="op">*</span><span class="st"> </span>triangle<span class="op">::</span><span class="kw">dtriangle</span>(p, <span class="fl">0.1</span>, <span class="fl">0.2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="fl">0.49</span> <span class="op">*</span><span class="st"> </span>triangle<span class="op">::</span><span class="kw">dtriangle</span>(p, <span class="fl">0.8</span>, <span class="fl">0.9</span>)
}
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">13</span>), <span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">14</span>)), <span class="dt">prior =</span> triangle<span class="op">::</span>dtriangle)
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">13</span>), <span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">14</span>)), <span class="dt">resolution =</span> <span class="dv">1000</span>, <span class="dt">prior =</span> dfoo)</code></pre></li>
<li><p>Run the following examples in R. Compare the plots produced
and comment the big idea(s) illustrated by this comparison.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(CalvinBayes)
dfoo &lt;-<span class="st"> </span><span class="cf">function</span>(p) {
  <span class="fl">0.02</span> <span class="op">*</span><span class="st"> </span><span class="kw">dunif</span>(p) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="fl">0.49</span> <span class="op">*</span><span class="st"> </span>triangle<span class="op">::</span><span class="kw">dtriangle</span>(p, <span class="fl">0.1</span>, <span class="fl">0.2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="fl">0.49</span> <span class="op">*</span><span class="st"> </span>triangle<span class="op">::</span><span class="kw">dtriangle</span>(p, <span class="fl">0.8</span>, <span class="fl">0.9</span>)
}
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">3</span>)), <span class="dt">prior =</span> dfoo)
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">10</span>)),  <span class="dt">prior =</span> dfoo)
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">30</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">30</span>)),  <span class="dt">prior =</span> dfoo)
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">100</span>)), <span class="dt">prior =</span> dfoo)</code></pre></li>
<li><p>Run the following examples in R and compare them to the ones
in the previous exercise. What do you observe?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(CalvinBayes)
dfoo &lt;-<span class="st"> </span><span class="cf">function</span>(p) {
  <span class="fl">0.02</span> <span class="op">*</span><span class="st"> </span><span class="kw">dunif</span>(p) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="fl">0.49</span> <span class="op">*</span><span class="st"> </span>triangle<span class="op">::</span><span class="kw">dtriangle</span>(p, <span class="fl">0.1</span>, <span class="fl">0.2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="fl">0.49</span> <span class="op">*</span><span class="st"> </span>triangle<span class="op">::</span><span class="kw">dtriangle</span>(p, <span class="fl">0.8</span>, <span class="fl">0.9</span>)
}
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">4</span>)), <span class="dt">prior =</span> dfoo)
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">3</span>)), <span class="dt">prior =</span> dfoo)
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">11</span>)),  <span class="dt">prior =</span> dfoo)
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">11</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">10</span>)),  <span class="dt">prior =</span> dfoo)
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">30</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">31</span>)),  <span class="dt">prior =</span> dfoo)
<span class="kw">BernGrid</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">31</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">30</span>)),  <span class="dt">prior =</span> dfoo)</code></pre></li>
</ol>
</div>
<div id="footnotes" class="section level2">
<h2><span class="header-section-number">2.7</span> Footnotes</h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>This is a bit of a trick that R2jags uses. The function created is never
run. The code is inspected and taken as the description of the model.
If you were to run the funtion, all it would do is create R formulas.<a href="credibility-models-and-parameters.html#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="some-useful-bits-of-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Redoing.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
