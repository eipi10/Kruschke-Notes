
# Bayes' Rule and the Grid Method

```{r, setup5, include = FALSE}
library(dplyr)
library(tidyr)
library(ggformula)
theme_set(theme_bw())
```


## Estimating the bias in a coin using the Grid Method

### Creating a Grid
```{r}
library(purrr)

x <- 1; n <- 4
CoinsGrid <- 
  expand.grid(
    theta = seq(0, 1, by = 0.001)
  ) %>%
  mutate(
    prior = pmin(theta, 1 - theta),  # higher if farther from edges
    prior = prior / sum(prior),      # normalize
    likelihood = map_dbl(theta, ~ dbinom(x = x, size = n, .x)),
    posterior = prior * likelihood,  
    posterior = posterior / sum(posterior)   # normalize
  )
```

### Plots from the grid

```{r}
gf_line(prior ~ theta, data = CoinsGrid)
gf_line(likelihood ~ theta, data = CoinsGrid)
gf_line(posterior ~ theta, data = CoinsGrid)
    
gf_area(prior ~ theta, data = CoinsGrid, alpha = 0.5)
gf_area(likelihood ~ theta, data = CoinsGrid, alpha = 0.5)
gf_area(posterior ~ theta, data = CoinsGrid, alpha = 0.5)
```

### HDI from the grid

Let's write a function to compute the Highest Density Interval (of the posterior
for theta) based on our grid.  Since different grids may use
different names for the parameter(s) and for the posterior, we'll write 
our function in a way that will let us specify those names if we need
to, but use `posterior` and `theta` by default.  And for good measure,
we'll calculate the posterior mode as well.

The basic idea (after standardizing the grid) is to sort the grid by
the posterior.  The mode will be at the end of the list, and the 
"bottom 95%" will be the HDI (or some other percent if we choose a 
different `level`).

This method works as long as the posterior is unimodal, increasing to the mode
from either side.

```{r}
HDI <- function(formula = posterior ~ theta, grid, level = 0.95) {
  # Create a standardized version of the grid
  model.frame(formula, data = grid) %>%    # turn formula into data frame
    setNames(c("posterior", "theta")) %>%     # standardrize names
    mutate(
      posterior = posterior / sum(posterior)  # normalize posterior
    ) %>%
    arrange(posterior) %>%                    # sort by posterior
    mutate(
      cum_posterior = cumsum(posterior)       # cumulative posterior
    ) %>% 
    filter(                                
      cum_posterior >= 1 - level,             # keep highest cum_posterior
    ) %>%
    summarise(                                # summarise what's left
      lo = min(theta),
      hi = max(theta),
      height = min(posterior),
      level = level,
      mode_height = last(posterior),
      mode = last(theta),
    )
}      
```

```{r}
HDICoins <- HDI(posterior ~ theta, grid = CoinsGrid)
HDICoins
```

With this information in hand, we can add a representation of the 95%
HDI to our plot.

```{r}
gf_line(posterior ~ theta, data = CoinsGrid) %>% 
  gf_hline(yintercept = ~height, data = HDICoins, 
           color = "red", alpha = 0.5) %>%
  gf_pointrangeh(height ~ mode + lo + hi, data = HDICoins, 
             color = "red", size = 1) %>%
  gf_labs(caption = "posterior mode and 95% HPI indicated in red")
```


### Automating the grid

Note: This function is a bit different from `CalvinBayes::BernGrid()`.

```{r}
MyBernGrid <- function(
  x, n,
  prior = dunif,
  res = 1001,
  ...) {
  
  Grid <- 
    expand.grid(
      theta = seq(0, 1, length.out = res)
      ) %>% 
    mutate(
      prior = prior(theta, ...),
      prior = prior / sum(prior),
      likelihood = dbinom(x, n, theta),
      likelihood = likelihood / sum(likelihood),
      posterior = prior * likelihood,
      posterior = posterior / sum(posterior)
    )
  
  H <- HDI(grid = Grid)
  
  gf_line(prior ~ theta, data = Grid, color = ~"prior", 
          size = 1.15, alpha = 0.8) %>% 
  gf_line(likelihood ~ theta, data = Grid, color = ~"likelihood", 
          size = 1.15, alpha = 0.7) %>% 
  gf_line(posterior ~ theta, data = Grid, color = ~"posterior",
          size = 1.15, alpha = 0.6) %>% 
    gf_pointrangeh( 
      height ~ mode + lo + hi, data = H,
      color = "red", size = 1) %>% 
    gf_labs(title = "Prior/Likelihood/Posterior", 
            subtitle = paste("Data: n =", n, ", x =", x)) %>%
    gf_refine(
      scale_color_manual(
        values = c(
          "prior" = "forestgreen", 
          "likelihood" = "blue", 
          "posterior" = "red"),
        breaks = c("prior", "likelihood", "posterior")
        )) %>%
    print()
  invisible(Grid)   # return the Grid, but don't show it
}
```

This function let's us quickly explore several scenarios and compare 
the results.  

* How does changing the prior affect the posterior?
* How does changing the data affect the posterior?

```{r}
library(triangle)
MyBernGrid(1, 4, prior = dtriangle, a = 0, b = 1, c = 0.5)
MyBernGrid(1, 4, prior = dunif)
MyBernGrid(10, 40, prior = dtriangle, a = 0, b = 1, c = 0.5)
MyBernGrid(10, 40, prior = dunif)
MyBernGrid(1, 4, prior = dtriangle, a = 0, b = 1, c = 0.8)
MyBernGrid(10, 40, prior = dtriangle, a = 0, b = 1, c = 0.8)
MyBernGrid(10, 40, prior = dbeta, shape1 = 25, shape2 = 12)
```

## Exercises


<!-- Exercise 5.1. [Purpose: Iterative application of Bayes’ rule, and seeing how posterior probabilities change with inclusion of more data.]  -->

1. Suppose we have a test with a 99% hit rate (sensitivity) and a 5% false 
alarm rate (95% specificity) just like in the example on page 103 of *DBDA2e*.
Now suppose that a random person is selected, has a first test that is positive,
then is retested and has a second test that is negative.  Taking into account
both tests, what is the probability that the person has the disease?

    Hint: We can use the the posterior after the first test as a prior for the 
    second test. Be sure to keep as many decimal digits as possible (use R and 
    don't round intermediate results).
    
    Note: In this problem we are assuming the the results of the two tests
    are independent, which might not be the case for some medical tests.
    
<!-- Exercise 5.2. 
[Purpose: Getting an intuition for the previous results by using “natural frequency” and “Markov” representations] -->

2. More testing.

    a. Suppose that the population consists of 100,000 people. Compute how many
people would be expected to fall into each cell of Table 5.4 on page 104 of 
*DBDA2e*. (To compute the expected number of people in a cell, 
just multiply the cell probability by the size of the population.) 

    You should find that out of 100,000 people, only 100 have the disease, 
    while 99,900 do not have the disease. 
    These marginal frequencies instantiate the prior
    probability that $p(\theta = \frown) = 0.001$. 
    Notice also the cell frequencies in
    the column $\theta = \frown$, which indicate that of 100 people 
    with the disease, 99 have a positive test result and 1 has a negative 
    test result. These cell
    frequencies instantiate the hit rate of 0.99. 
    Your job for this part of the exercise is to fill in the frequencies of 
    the remaining cells of the table.
    
    b. Take a good look at the frequencies in the table you just computed
    for the previous part. These are the so-called "natural frequencies" of the
    events, as opposed to the somewhat unintuitive expression in terms of
    conditional probabilities (Gigerenzer & Hoffrage, 1995). From the cell
    frequencies alone, determine the proportion of people who have the disease,
    given that their test result is positive. 
    
    Your answer should match the result from applying Bayes' rule to
    the probabilities.
    
    c. Now we'll consider a related representation of the probabilities in terms
    of natural frequencies, which is especially useful when we accumulate more
    data. This type of representation is called a "Markov" representation by
    Krauss, Martignon, and Hoffrage (1999). Suppose now we start with a
    population of $N = 10,000,000$ people. We expect 99.9% of them (i.e.,
    9,990,000) not to have the disease, and just 0.1% (i.e., 10,000) to have the
    disease. Now consider how many people we expect to test positive. Of the
    10,000 people who have the disease, 99%, (i.e., 9,900) will be expected to
    test positive. Of the 9,990,000 people who do not have the disease, 5%
    (i.e., 499,500) will be expected to test positive. Now consider re-testing
    everyone who has tested positive on the first test. How many of them are
    expected to show a negative result on the re-test? 

    d. What proportion of people who test positive at first and then negative on
    retest, actually have the disease? In other words, of the total number of
    people at the bottom of the diagram in the previous part (those are the
    people who tested positive then negative), what proportion of them are in
    the left branch of the tree? How does the result compare with your answer to
    Exercise 5.1?
    
<!-- Exercise 5.3. [Purpose: To see a hands-on example of data-order invariance.] -->

3. Consider again the disease and diagnostic test of the previous two exercises.

    a. Suppose that a person selected at random from the population gets the
    test and it comes back negative. Compute the probability that the person has
    the disease.
    
    b. The person then gets re-tested, and on the second test the result is
    positive. Compute the probability that the person has the disease. How does
    the result compare with your answer to Exercise 5.1?

 